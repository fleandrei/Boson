{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "csv_file=\"atlas-higgs-challenge-2014-v2.csv\"\n",
    "crossval_nbr=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Cust(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, train=True, crossval_nbr=5, transformation=None):\n",
    "        'Initialization'\n",
    "        self.all=np.array(pd.read_csv(csv_file))\n",
    "        #print(self.all[:,:-3])\n",
    "        self.data=torch.from_numpy(self.all[:,0:-3].astype('float64'))\n",
    "        self.train=train\n",
    "        \n",
    "        self.crossval_nbr=crossval_nbr\n",
    "        self.testpart_num=crossval_nbr-1\n",
    "        \n",
    "        self.nbrpoint=len(self.all)\n",
    "        #self.nbrtrain=int((1-1/crossval_nbr)*self.nbrpoint)\n",
    "        \n",
    "        self.Taille_section=int(self.nbrpoint/self.crossval_nbr)\n",
    "        self.debut_test=self.testpart_num * self.Taille_section\n",
    "        \n",
    "        if self.testpart_num==self.crossval_nbr-1:\n",
    "            self.fin_test=self.nbrpoint\n",
    "        else:\n",
    "            self.fin_test=self.debut_test + self.Taille_section\n",
    "        \n",
    "        self.nbrtest=self.fin_test - self.debut_test\n",
    "        self.nbrtrain=self.nbrpoint - self.nbrtest\n",
    "        #self.numtest=self.numpoint - self.numtrain\n",
    "        \n",
    "        self.transformation=transformation\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        if self.train:\n",
    "            return self.nbrtrain\n",
    "        else:\n",
    "            return self.fin_test - self.debut_test\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        if self.train:\n",
    "            if index>= self.debut_test:\n",
    "                index+=self.nbrtest\n",
    "                \n",
    "        else:\n",
    "            index+=self.debut_test\n",
    "        \n",
    "        #sample=torch.from_numpy(self.all[index][0:-3].astype('float64'))\n",
    "        sample=self.data[index]\n",
    "        sample.requires_grad=True\n",
    "        label=1 if self.all[index][-3]==\"s\" else 0\n",
    "        \n",
    "        if self.transformation :\n",
    "            sample=self.transformation(sample)\n",
    "\n",
    "        return sample.float(), float(label)\n",
    "    \n",
    "    def Set_Train(self, train):\n",
    "        self.train=train\n",
    "        \n",
    "    def Set_TestPart_Num(self,num):\n",
    "        self.testpart_num=num\n",
    "        \n",
    "        self.debut_test=self.testpart_num * self.Taille_section\n",
    "        \n",
    "        if self.testpart_num==self.crossval_nbr-1:\n",
    "            self.fin_test=self.nbrpoint\n",
    "        else:\n",
    "            self.fin_test=self.debut_test + self.Taille_section\n",
    "        \n",
    "        self.nbrtest=self.fin_test - self.debut_test\n",
    "        self.nbrtrain=self.nbrpoint - self.nbrtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=Dataset_Cust(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Loader=torch.utils.data.DataLoader(Dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMS(s,b):\n",
    "    assert s >= 0\n",
    "    assert b >= 0\n",
    "    bReg = 10.\n",
    "    return math.sqrt(2 * ((s + b + bReg) * \n",
    "                          math.log(1 + s / (b + bReg)) - s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron,self).__init__()\n",
    "        self.fc1=nn.Linear(30,1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))\n",
    "\n",
    "    \n",
    "class MLP_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_1,self).__init__()\n",
    "        self.fc1=nn.Linear(30,15)\n",
    "        self.fc2=nn.Linear(15,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return F.sigmoid(self.fc2(x))\n",
    "        \n",
    "class MLP_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_2,self).__init__()\n",
    "        self.fc1=nn.Linear(30,15)\n",
    "        self.fc2=nn.Linear(15,7)\n",
    "        self.fc3=nn.Linear(7,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return F.sigmoid(self.fc3(x))\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,10,5)\n",
    "        self.conv2=nn.Conv1d(10,20,5)\n",
    "        self.fc=nn.Linear(50*4*4,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=F.max_pool1d(F.relu(self.conv1(x)),2,2)\n",
    "        x=F.max_pool1d(F.relu(self.conv2(x)),2,2)\n",
    "        x=x.view(-1,50*4*4)\n",
    "        x=self.fc(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m)==nn.Conv1d:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Percep=Perceptron()\n",
    "Mlp1=MLP_1()\n",
    "Mlp2=MLP_2()\n",
    "Cnn=CNN()\n",
    "#print(Mlp1.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion= nn.BCELoss()\n",
    "optimizerPercep= optim.SGD(Percep.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizeMlp1= optim.SGD(Mlp1.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizeMlp2= optim.SGD(Mlp2.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizeCnn= optim.SGD(Cnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Model, Optimizer, Criterion, Data_Loader,Epoch=10, log_interval=100):\n",
    "    #Data_Loader.dataset.Set_TestPart_Num(4)\n",
    "    print(Data_Loader.dataset.debut_test)\n",
    "    predicted=np.empty(batch_size)\n",
    "    #print(type(Percep.fc1.weight[0][0].item()))\n",
    "    #print(type(Percep.fc1.bias[0].item()))\n",
    "    Model.apply(init_weights)\n",
    "    \n",
    "    Data_Loader.dataset.train=True\n",
    "    \n",
    "    for epoch in range(Epoch):\n",
    "        ams=0\n",
    "        TP=0 #True Positive\n",
    "        FP=0 # False Positive\n",
    "        S=0 #True Positive pondéré\n",
    "        B=0 # False Positive pondéré\n",
    "        Loss=0\n",
    "        correct=0\n",
    "    \n",
    "        fp=0\n",
    "        tp=0\n",
    "        total=0\n",
    "        running_loss=0\n",
    "        for batch_id, sample in enumerate(Data_Loader,0):\n",
    "            inputs, labels= sample\n",
    "            Optimizer.zero_grad()\n",
    "            \n",
    "            #print(inputs[0][0].requires_grad)\n",
    "            #print(labels.size(0))\n",
    "            #print(inputs[0][0])\n",
    "            \n",
    "            \n",
    "            outputs= Model(inputs[:,1:31])\n",
    "        \n",
    "             #print(outputs[0]+outputs[1])\n",
    "             # print(outputs.view(-1,32))\n",
    "            \n",
    "            #predicted= torch.tensor(1 if outputs.data[j] > 0.5 else 0 for j in range(len(outputs.data)))\n",
    "            total+= labels.size(0)\n",
    "            #correct+= (predicted== labels).sum().item()\n",
    "            for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "            \n",
    "        \n",
    "            #print(outputs)\n",
    "            #outputs=outputs.view(-1,10)\n",
    "            loss= Criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            Optimizer.step()\n",
    "        \n",
    "            running_loss+= loss.item()\n",
    "            Loss+=loss.item()\n",
    "            if batch_id%log_interval==0:\n",
    "                if tp+fp>0:\n",
    "                    print(\"epoch:{}, nombre batch:{} loss:{:.3f} precision={:.3f}%\".format(epoch+1, batch_id+1, running_loss/log_interval, 100*tp/(tp+fp))) \n",
    "                else:\n",
    "                    print(\"epoch:{}, nombre batch:{} loss:{:.3f} \".format(epoch+1, batch_id+1, running_loss/log_interval)) \n",
    "\n",
    "                total=0\n",
    "                tp=0\n",
    "                fp=0\n",
    "                running_loss=0\n",
    "        if TP+FP>0:\n",
    "            print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}  precision:{:.3f}%, AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ))\n",
    "        else:\n",
    "            print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}  , AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), AMS(S,B) ))\n",
    "        \n",
    "    print(\"Résultat Entrainement: {} bonnes classification sur {} données :loss moyen/batch={:.4f}, Accuracy={:.3f},  precision={:.3f}%, AMS={:.4f}\".format(correct,len(Data_Loader.dataset), Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset) ,100*TP/(TP+FP), AMS(S,B)))\n",
    "    return [Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [2 3]]\n",
      "[2.  2.5]\n"
     ]
    }
   ],
   "source": [
    "#Res=Train(Mlp2, optimizeMlp2, criterion, Data_Loader )\n",
    "#print(Res)\n",
    "a=np.arange(4).reshape(2,2)\n",
    "a[0]=[2,2]\n",
    "print(a)\n",
    "print(np.mean(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(Model, Criterion, Data_Loader):\n",
    "\n",
    "    correct= 0\n",
    "    total= 0\n",
    "    test_loss = 0\n",
    "    predicted=np.empty(batch_size)\n",
    "    TP=0\n",
    "    FP=0\n",
    "    S=0\n",
    "    B=0\n",
    "\n",
    "    Data_Loader.dataset.train=False\n",
    "    with torch.no_grad():\n",
    "        for sample in Data_Loader:\n",
    "            inputs, labels= sample\n",
    "            #images=images.view(-1,784)\n",
    "            outputs= Model(inputs[:,1:31])\n",
    "\n",
    "            total+= labels.size(0)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                    if outputs.data[i]>0.5:\n",
    "                        predicted[i]=1\n",
    "\n",
    "                        if labels[i]==1:\n",
    "                            TP+=1\n",
    "                            #tp+=1\n",
    "                            S+= inputs[i][31]\n",
    "                            correct+=1\n",
    "                        else:\n",
    "                            FP+=1\n",
    "                            #fp+=1\n",
    "                            B+=inputs[i][31]\n",
    "                    else:\n",
    "                        predicted[i]=0\n",
    "                        if labels[i]==0:\n",
    "                            correct+=1\n",
    "\n",
    "            test_loss+=Criterion(outputs,labels.float())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_loss/= len(Data_Loader.dataset)\n",
    "\n",
    "    print(\"Résultat test: {} bonnes classification sur {} données de test:loss moyen={:.8f} Accuracy={:.3f}%, Precision={:.3f}%, AMS={:.3f}\".format(correct,total, test_loss/total,100*correct/total, 100*TP/(TP+FP), AMS(S,B)))\n",
    "    \n",
    "    return [test_loss/total, 100*correct/total, 100*TP/(TP+FP), AMS(S,B)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0 :\n",
      "\n",
      "0\n",
      "epoch:1, nombre batch:1 loss:0.007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, nombre batch:101 loss:9.765 precision=46.895%\n",
      "epoch:1, nombre batch:201 loss:10.031 precision=45.302%\n",
      "epoch:1, nombre batch:301 loss:10.244 precision=45.740%\n",
      "epoch:1, nombre batch:401 loss:9.731 precision=46.369%\n",
      "epoch:1, nombre batch:501 loss:9.723 precision=47.101%\n",
      "epoch:1, nombre batch:601 loss:9.783 precision=46.889%\n",
      "epoch:1, nombre batch:701 loss:10.353 precision=44.246%\n",
      "epoch:1, nombre batch:801 loss:9.921 precision=48.977%\n",
      "epoch:1, nombre batch:901 loss:9.911 precision=48.741%\n",
      "epoch:1, nombre batch:1001 loss:9.619 precision=46.091%\n",
      "epoch:1, nombre batch:1101 loss:9.930 precision=46.871%\n",
      "epoch:1, nombre batch:1201 loss:10.096 precision=45.094%\n",
      "epoch:1, nombre batch:1301 loss:10.103 precision=42.715%\n",
      "epoch:1, nombre batch:1401 loss:9.671 precision=47.039%\n",
      "epoch:1, nombre batch:1501 loss:9.982 precision=45.870%\n",
      "epoch:1, nombre batch:1601 loss:10.184 precision=45.289%\n",
      "epoch:1, nombre batch:1701 loss:9.671 precision=48.376%\n",
      "epoch:1, nombre batch:1801 loss:10.174 precision=46.601%\n",
      "epoch:1, nombre batch:1901 loss:9.940 precision=46.393%\n",
      "epoch:1, nombre batch:2001 loss:9.990 precision=44.064%\n",
      "epoch:1, nombre batch:2101 loss:9.797 precision=46.361%\n",
      "epoch:1, nombre batch:2201 loss:10.016 precision=48.734%\n",
      "epoch:1, nombre batch:2301 loss:9.957 precision=46.452%\n",
      "epoch:1, nombre batch:2401 loss:9.990 precision=45.657%\n",
      "epoch:1, nombre batch:2501 loss:10.431 precision=43.573%\n",
      "epoch:1, nombre batch:2601 loss:9.950 precision=47.345%\n",
      "epoch:1, nombre batch:2701 loss:9.848 precision=45.778%\n",
      "epoch:1, nombre batch:2801 loss:9.930 precision=45.165%\n",
      "epoch:1, nombre batch:2901 loss:10.068 precision=46.804%\n",
      "epoch:1, nombre batch:3001 loss:9.852 precision=46.026%\n",
      "epoch:1, nombre batch:3101 loss:9.991 precision=48.636%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0c536d85c2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train {} :\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mData_Loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSet_TestPart_Num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPercep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizerPercep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test {} :\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTableau_Perceptron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPercep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0871e7a84ef2>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(Model, Optimizer, Criterion, Data_Loader, Epoch, log_interval)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                         \u001b[0mTP\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0mtp\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######TEST Perceptron\n",
    "Tableau_Perceptron=np.arange(crossval_nbr*4).reshape(crossval_nbr,4)\n",
    "for section in range(crossval_nbr):\n",
    "    print(\"Train {} :\\n\".format(section))\n",
    "    Data_Loader.dataset.Set_TestPart_Num(section)\n",
    "    Train(Percep, optimizerPercep, criterion, Data_Loader)\n",
    "    print(\"Test {} :\\n\".format(section))\n",
    "    Tableau_Perceptron[section]=Test(Percep, criterion, Data_Loader)\n",
    "\n",
    "Res_Perceptron=mean(Tableau_Perceptron,axis=0)\n",
    "\n",
    "print(\"tableau pour les {} étapes de validations:\\n\".format(crossval_nbr))\n",
    "print(Tableau_Perceptron)\n",
    "\n",
    "print(\"Moyennne des metrics:\\n\")\n",
    "print(Res_Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0 :\n",
      "\n",
      "0\n",
      "epoch:1, nombre batch:1 loss:0.007 \n",
      "epoch:1, nombre batch:101 loss:0.684 \n",
      "epoch:1, nombre batch:201 loss:0.668 \n",
      "epoch:1, nombre batch:301 loss:0.661 \n",
      "epoch:1, nombre batch:401 loss:0.653 \n",
      "epoch:1, nombre batch:501 loss:0.650 \n",
      "epoch:1, nombre batch:601 loss:0.647 \n",
      "epoch:1, nombre batch:701 loss:0.645 \n",
      "epoch:1, nombre batch:801 loss:0.649 \n",
      "epoch:1, nombre batch:901 loss:0.656 \n",
      "epoch:1, nombre batch:1001 loss:0.638 \n",
      "epoch:1, nombre batch:1101 loss:0.640 \n",
      "epoch:1, nombre batch:1201 loss:0.645 \n",
      "epoch:1, nombre batch:1301 loss:0.644 \n",
      "epoch:1, nombre batch:1401 loss:0.644 \n",
      "epoch:1, nombre batch:1501 loss:0.633 \n",
      "epoch:1, nombre batch:1601 loss:0.632 \n",
      "epoch:1, nombre batch:1701 loss:0.637 \n",
      "epoch:1, nombre batch:1801 loss:0.655 \n",
      "epoch:1, nombre batch:1901 loss:0.642 \n",
      "epoch:1, nombre batch:2001 loss:0.637 \n",
      "epoch:1, nombre batch:2101 loss:0.647 \n",
      "epoch:1, nombre batch:2201 loss:0.632 \n",
      "epoch:1, nombre batch:2301 loss:0.644 \n",
      "epoch:1, nombre batch:2401 loss:0.630 \n",
      "epoch:1, nombre batch:2501 loss:0.643 \n",
      "epoch:1, nombre batch:2601 loss:0.637 \n",
      "epoch:1, nombre batch:2701 loss:0.642 \n",
      "epoch:1, nombre batch:2801 loss:0.644 \n",
      "epoch:1, nombre batch:2901 loss:0.647 \n",
      "epoch:1, nombre batch:3001 loss:0.648 \n",
      "epoch:1, nombre batch:3101 loss:0.647 \n",
      "epoch:1, nombre batch:3201 loss:0.627 \n",
      "epoch:1, nombre batch:3301 loss:0.636 \n",
      "epoch:1, nombre batch:3401 loss:0.647 \n",
      "epoch:1, nombre batch:3501 loss:0.651 \n",
      "epoch:1, nombre batch:3601 loss:0.634 \n",
      "epoch:1, nombre batch:3701 loss:0.638 \n",
      "epoch:1, nombre batch:3801 loss:0.635 \n",
      "epoch:1, nombre batch:3901 loss:0.647 \n",
      "epoch:1, nombre batch:4001 loss:0.639 \n",
      "epoch:1, nombre batch:4101 loss:0.636 \n",
      "epoch:1, nombre batch:4201 loss:0.639 \n",
      "epoch:1, nombre batch:4301 loss:0.637 \n",
      "epoch:1, nombre batch:4401 loss:0.648 \n",
      "epoch:1, nombre batch:4501 loss:0.639 \n",
      "epoch:1, nombre batch:4601 loss:0.647 \n",
      "epoch:1, nombre batch:4701 loss:0.640 \n",
      "epoch:1, nombre batch:4801 loss:0.641 \n",
      "epoch:1, nombre batch:4901 loss:0.642 \n",
      "epoch:1, nombre batch:5001 loss:0.638 \n",
      "epoch:1, nombre batch:5101 loss:0.641 \n",
      "epoch:1, nombre batch:5201 loss:0.639 \n",
      "epoch:1, nombre batch:5301 loss:0.645 \n",
      "epoch:1, nombre batch:5401 loss:0.645 \n",
      "epoch:1, nombre batch:5501 loss:0.644 \n",
      "epoch:1, nombre batch:5601 loss:0.641 \n",
      "epoch:1, nombre batch:5701 loss:0.652 \n",
      "epoch:1, nombre batch:5801 loss:0.646 \n",
      "epoch:1, nombre batch:5901 loss:0.637 \n",
      "epoch:1, nombre batch:6001 loss:0.649 \n",
      "epoch:1, nombre batch:6101 loss:0.646 \n",
      "epoch:1, nombre batch:6201 loss:0.644 \n",
      "epoch:1, nombre batch:6301 loss:0.649 \n",
      "epoch:1, nombre batch:6401 loss:0.639 \n",
      "epoch:1, nombre batch:6501 loss:0.630 \n",
      "epoch:1, nombre batch:6601 loss:0.630 \n",
      "epoch:1, nombre batch:6701 loss:0.649 \n",
      "epoch:1, nombre batch:6801 loss:0.637 \n",
      "epoch:1, nombre batch:6901 loss:0.641 \n",
      "epoch:1, nombre batch:7001 loss:0.645 \n",
      "epoch:1, nombre batch:7101 loss:0.647 \n",
      "epoch:1, nombre batch:7201 loss:0.644 \n",
      "epoch:1, nombre batch:7301 loss:0.638 \n",
      "epoch:1, nombre batch:7401 loss:0.636 \n",
      "epoch:1, nombre batch:7501 loss:0.635 \n",
      "epoch:1, nombre batch:7601 loss:0.627 \n",
      "epoch:1, nombre batch:7701 loss:0.642 \n",
      "epoch:1, nombre batch:7801 loss:0.647 \n",
      "epoch:1, nombre batch:7901 loss:0.636 \n",
      "epoch:1, nombre batch:8001 loss:0.645 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-867283ec0576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train {} :\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mData_Loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSet_TestPart_Num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMlp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizeMlp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test {} :\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTableau_Mlp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMlp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0871e7a84ef2>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(Model, Optimizer, Criterion, Data_Loader, Epoch, log_interval)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_Loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-3f1a31dec64e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSet_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########Test MLP à une Couche Cachée\n",
    "Tableau_Mlp1=np.arange(crossval_nbr*4).reshape(crossval_nbr,4)\n",
    "for section in range(crossval_nbr):\n",
    "    print(\"Train {} :\\n\".format(section))\n",
    "    Data_Loader.dataset.Set_TestPart_Num(section)\n",
    "    Train(Mlp1, optimizeMlp1, criterion, Data_Loader)\n",
    "    print(\"Test {} :\\n\".format(section))\n",
    "    Tableau_Mlp1[section]=Test(Mlp1, criterion, Data_Loader)\n",
    "\n",
    "Res_Mlp1=mean(Tableau_Mlp1,axis=0)\n",
    "\n",
    "print(\"tableau pour les {} étapes de validations:\\n\".format(crossval_nbr))\n",
    "print(Tableau_Mlp1)\n",
    "\n",
    "print(\"Moyennne des metrics:\\n\")\n",
    "print(Res_Mlp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Test MLP à 2 couches cachées\n",
    "Tableau_Mlp2=np.arange(20).reshape(crossval_nbr,4)\n",
    "for section in range(crossval_nbr):\n",
    "    print(\"Train {} :\\n\".format(section))\n",
    "    Data_Loader.dataset.Set_TestPart_Num(section)\n",
    "    Train(Mlp2, optimizeMlp2, criterion, Data_Loader)\n",
    "    print(\"Test {} :\\n\".format(section))\n",
    "    Tableau_Mlp2[section]=Test(Mlp2, criterion, Data_Loader)\n",
    "\n",
    "Res_Mlp2=mean(Tableau_Mlp2,axis=0)\n",
    "\n",
    "print(\"tableau pour les {} étapes de validations:\\n\".format(crossval_nbr))\n",
    "print(Tableau_Mlp2)\n",
    "\n",
    "print(\"Moyennne des metrics:\\n\")\n",
    "print(Res_Mlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_Loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-89bebe60de6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrain_Loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(torch.tensor(1 if [1,0,0.5,0.4,0.7][i] > 0.5 else 0 for i in range(5)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(torch.tensor([1 if i==2 else 0 for i in range(7)]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Train_Loader' is not defined"
     ]
    }
   ],
   "source": [
    "Train_Loader.dataset.rate_train=0.9\n",
    "print(Train_Loader.dataset.rate_train)\n",
    "print(type(float(int(1))))\n",
    "#print(torch.tensor(1 if [1,0,0.5,0.4,0.7][i] > 0.5 else 0 for i in range(5)))\n",
    "#print(torch.tensor([1 if i==2 else 0 for i in range(7)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch:1, nombre batch:1 loss:0.174 precision=30.000%\n",
      "epoch:1, nombre batch:101 loss:10.202 precision=44.124%\n",
      "epoch:1, nombre batch:201 loss:10.029 precision=46.552%\n",
      "epoch:1, nombre batch:301 loss:9.835 precision=49.428%\n",
      "epoch:1, nombre batch:401 loss:10.137 precision=45.815%\n",
      "epoch:1, nombre batch:501 loss:9.999 precision=44.369%\n",
      "epoch:1, nombre batch:601 loss:9.844 precision=48.600%\n",
      "epoch:1, nombre batch:701 loss:9.582 precision=48.686%\n",
      "epoch:1, nombre batch:801 loss:9.783 precision=48.866%\n",
      "epoch:1, nombre batch:901 loss:10.405 precision=44.659%\n",
      "epoch:1, nombre batch:1001 loss:10.399 precision=44.766%\n",
      "epoch:1, nombre batch:1101 loss:9.908 precision=46.658%\n",
      "epoch:1, nombre batch:1201 loss:9.904 precision=45.125%\n",
      "epoch:1, nombre batch:1301 loss:9.973 precision=47.026%\n",
      "epoch:1, nombre batch:1401 loss:9.921 precision=47.362%\n",
      "epoch:1, nombre batch:1501 loss:10.327 precision=46.028%\n",
      "epoch:1, nombre batch:1601 loss:9.519 precision=49.718%\n",
      "epoch:1, nombre batch:1701 loss:10.344 precision=45.848%\n",
      "epoch:1, nombre batch:1801 loss:9.930 precision=46.087%\n",
      "epoch:1, nombre batch:1901 loss:9.844 precision=45.638%\n",
      "epoch:1, nombre batch:2001 loss:9.800 precision=46.556%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-113718b8d3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#outputs=outputs.view(-1,10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0moptimizerPercep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAIN Perceptron\n",
    "\n",
    "Epoch=10\n",
    "#Data_Loader.dataset.Set_TestPart_Num(4)\n",
    "print(Data_Loader.dataset.debut_test)\n",
    "log_interval=100\n",
    "predicted=np.empty(batch_size)\n",
    "#print(type(Percep.fc1.weight[0][0].item()))\n",
    "#print(type(Percep.fc1.bias[0].item()))\n",
    "for epoch in range(Epoch):\n",
    "    ams=0\n",
    "    TP=0 #True Positive\n",
    "    FP=0 # False Positive\n",
    "    S=0 #True Positive pondéré\n",
    "    B=0 # False Positive pondéré\n",
    "    Loss=0\n",
    "    correct=0\n",
    "    \n",
    "    fp=0\n",
    "    tp=0\n",
    "    total=0\n",
    "    running_loss=0\n",
    "    for batch_id, sample in enumerate(Data_Loader,0):\n",
    "            inputs, labels= sample\n",
    "            optimizerPercep.zero_grad()\n",
    "            \n",
    "            #print(inputs[0][0].requires_grad)\n",
    "            #print(labels.size(0))\n",
    "            #print(inputs[0][0])\n",
    "            \n",
    "            \n",
    "            outputs= Percep(inputs[:,1:31])\n",
    "            \n",
    "            #print(outputs[0]+outputs[1])\n",
    "           # print(outputs.view(-1,32))\n",
    "        \n",
    "            #predicted= torch.tensor(1 if outputs.data[j] > 0.5 else 0 for j in range(len(outputs.data)))\n",
    "            total+= labels.size(0)\n",
    "            #correct+= (predicted== labels).sum().item()\n",
    "            for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "            \n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "            \n",
    "        \n",
    "            #print(outputs)\n",
    "            #outputs=outputs.view(-1,10)\n",
    "            loss= criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizerPercep.step()\n",
    "        \n",
    "            running_loss+= loss.item()\n",
    "            Loss+=loss.item()\n",
    "            if batch_id%log_interval==0:\n",
    "                if tp+fp>0:\n",
    "                    print(\"epoch:{}, nombre batch:{} loss:{:.3f} precision={:.3f}%\".format(epoch+1, batch_id+1, running_loss/log_interval, 100*tp/(tp+fp))) \n",
    "                else:\n",
    "                    print(\"epoch:{}, nombre batch:{} loss:{:.3f} \".format(epoch+1, batch_id+1, running_loss/log_interval)) \n",
    "\n",
    "                total=0\n",
    "                tp=0\n",
    "                fp=0\n",
    "                running_loss=0\n",
    "    if TP+FP>0:\n",
    "        print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}  precision:{:.3f}%, AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ))\n",
    "    else:\n",
    "        print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}  , AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), AMS(S,B) ))\n",
    "        \n",
    "print(\"Résultat Entrainement: {} bonnes classification sur {} données :loss moyen/batch={:.4f}, Accuracy={:.3f},  precision={:.3f}%, AMS={:.4f}\".format(correct,len(Data_Loader.dataset), Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset) ,100*TP/(TP+FP), AMS(S,B)))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(Percep.state_dict(), 'Percep_weights.pth')\n",
    "Percep.load_state_dict(torch.load('Percep_weights.pth'))\n",
    "print(Percep.fc1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, nombre batch:1 loss:0.046 precision=50.000%\n",
      "epoch:1, nombre batch:101 loss:6.025 precision=63.537%\n",
      "epoch:1, nombre batch:201 loss:5.808 precision=59.711%\n",
      "epoch:1, nombre batch:301 loss:6.074 precision=66.087%\n",
      "epoch:1, nombre batch:401 loss:5.911 precision=60.561%\n",
      "epoch:1, nombre batch:501 loss:5.765 precision=63.265%\n",
      "epoch:1, nombre batch:601 loss:5.866 precision=61.475%\n",
      "epoch:1, nombre batch:701 loss:5.893 precision=63.261%\n",
      "epoch:1, nombre batch:801 loss:5.883 precision=54.140%\n",
      "epoch:1, nombre batch:901 loss:6.087 precision=62.635%\n",
      "epoch:1, nombre batch:1001 loss:5.889 precision=67.364%\n",
      "epoch:1, nombre batch:1101 loss:5.639 precision=60.893%\n",
      "epoch:1, nombre batch:1201 loss:6.032 precision=63.992%\n",
      "epoch:1, nombre batch:1301 loss:6.424 precision=65.485%\n",
      "epoch:1, nombre batch:1401 loss:5.948 precision=63.692%\n",
      "epoch:1, nombre batch:1501 loss:6.135 precision=65.217%\n",
      "epoch:1, nombre batch:1601 loss:6.028 precision=66.017%\n",
      "epoch:1, nombre batch:1701 loss:5.905 precision=61.553%\n",
      "epoch:1, nombre batch:1801 loss:6.346 precision=64.837%\n",
      "epoch:1, nombre batch:1901 loss:5.906 precision=61.448%\n",
      "epoch:1, nombre batch:2001 loss:6.209 precision=60.321%\n",
      "epoch:1, nombre batch:2101 loss:5.707 precision=65.432%\n",
      "epoch:1, nombre batch:2201 loss:6.342 precision=57.534%\n",
      "epoch:1, nombre batch:2301 loss:5.943 precision=59.315%\n",
      "epoch:1, nombre batch:2401 loss:6.212 precision=62.275%\n",
      "epoch:1, nombre batch:2501 loss:6.092 precision=63.738%\n",
      "epoch:1, nombre batch:2601 loss:5.962 precision=56.833%\n",
      "epoch:1, nombre batch:2701 loss:6.355 precision=58.704%\n",
      "epoch:1, nombre batch:2801 loss:5.498 precision=65.028%\n",
      "epoch:1, nombre batch:2901 loss:6.094 precision=64.612%\n",
      "epoch:1, nombre batch:3001 loss:5.812 precision=66.167%\n",
      "epoch:1, nombre batch:3101 loss:5.844 precision=65.548%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8dcef09d923c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#print(inputs[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMlp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b760c34a2039>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMLP_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAIN MLP 1 couche\n",
    "\n",
    "Epoch=10\n",
    "\n",
    "log_interval=100\n",
    "predicted=np.empty(batch_size)\n",
    "#print(type(Percep.fc1.weight[0][0].item()))\n",
    "#print(type(Percep.fc1.bias[0].item()))\n",
    "for epoch in range(Epoch):\n",
    "    ams=0\n",
    "    TP=0 #True Positive\n",
    "    FP=0 # False Positive\n",
    "    S=0 #True Positive pondéré\n",
    "    B=0 # False Positive pondéré\n",
    "    Loss=0\n",
    "    correct=0\n",
    "    \n",
    "    fp=0\n",
    "    tp=0\n",
    "    total=0\n",
    "    running_loss=0\n",
    "    for batch_id, sample in enumerate(Data_Loader,0):\n",
    "            inputs, labels= sample\n",
    "            optimizeMlp1.zero_grad()\n",
    "            \n",
    "            #print(inputs[0].requires_grad)\n",
    "            #print(labels.size(0))\n",
    "            \n",
    "            \n",
    "            #print(inputs[0])\n",
    "            outputs= Mlp1(inputs[:,1:31])\n",
    "            #print(outputs)\n",
    "            \n",
    "            #print(outputs[0]+outputs[1])\n",
    "           # print(outputs.view(-1,32))\n",
    "        \n",
    "            #predicted= torch.tensor(1 if outputs.data[j] > 0.5 else 0 for j in range(len(outputs.data)))\n",
    "            total+= labels.size(0)\n",
    "            #correct+= (predicted== labels).sum().item()\n",
    "            for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "            \n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "            \n",
    "        \n",
    "            #print(outputs)\n",
    "            #outputs=outputs.view(-1,10)\n",
    "            loss= criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizeMlp1.step()\n",
    "        \n",
    "            running_loss+= loss.item()\n",
    "            Loss+=loss.item()\n",
    "            if batch_id%log_interval==0:\n",
    "                if tp+fp>0:\n",
    "                    print(\"epoch:{}, nombre batch:{} loss:{:.3f} precision={:.3f}%\".format(epoch+1, batch_id+1, running_loss/log_interval, 100*tp/(tp+fp))) \n",
    "                total=0\n",
    "                tp=0\n",
    "                fp=0\n",
    "                running_loss=0\n",
    "    print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}%,  precision:{:.3f}%, AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ))\n",
    "   \n",
    "        \n",
    "print(\"Résultat Entrainement: {} bonnes classification sur {} données :loss moyen/batch={:.4f}, Accuracy={:.3f}%,  precision={:.3f}%, AMS={:.4f}\".format(correct,len(Data_Loader.dataset), Loss/(batch_id+1), 100*correct/len(Train_Loader.dataset) ,100*TP/(TP+FP), AMS(S,B)))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-63ee293b300f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(TP)\n",
    "print(FP)\n",
    "print(TP/(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, nombre batch:1 loss:0.046 precision=50.000%\n",
      "epoch:1, nombre batch:101 loss:14.304 precision=32.933%\n",
      "epoch:1, nombre batch:201 loss:13.472 precision=37.787%\n",
      "epoch:1, nombre batch:301 loss:13.849 precision=36.504%\n",
      "epoch:1, nombre batch:401 loss:13.346 precision=36.445%\n",
      "epoch:1, nombre batch:501 loss:13.675 precision=37.398%\n",
      "epoch:1, nombre batch:601 loss:13.911 precision=35.283%\n",
      "epoch:1, nombre batch:701 loss:13.431 precision=37.949%\n",
      "epoch:1, nombre batch:801 loss:13.442 precision=37.444%\n",
      "epoch:1, nombre batch:901 loss:13.772 precision=36.706%\n",
      "epoch:1, nombre batch:1001 loss:13.895 precision=35.990%\n",
      "epoch:1, nombre batch:1101 loss:14.226 precision=35.248%\n",
      "epoch:1, nombre batch:1201 loss:13.702 precision=36.889%\n",
      "epoch:1, nombre batch:1301 loss:13.944 precision=36.125%\n",
      "epoch:1, nombre batch:1401 loss:14.245 precision=34.044%\n",
      "epoch:1, nombre batch:1501 loss:13.963 precision=36.273%\n",
      "epoch:1, nombre batch:1601 loss:13.787 precision=35.985%\n",
      "epoch:1, nombre batch:1701 loss:14.125 precision=35.152%\n",
      "epoch:1, nombre batch:1801 loss:13.705 precision=36.629%\n",
      "epoch:1, nombre batch:1901 loss:14.045 precision=35.318%\n",
      "epoch:1, nombre batch:2001 loss:13.724 precision=36.768%\n",
      "epoch:1, nombre batch:2101 loss:13.735 precision=37.472%\n",
      "epoch:1, nombre batch:2201 loss:13.719 precision=36.897%\n",
      "epoch:1, nombre batch:2301 loss:13.776 precision=35.792%\n",
      "epoch:1, nombre batch:2401 loss:14.339 precision=34.991%\n",
      "epoch:1, nombre batch:2501 loss:13.981 precision=35.364%\n",
      "epoch:1, nombre batch:2601 loss:13.909 precision=35.204%\n",
      "epoch:1, nombre batch:2701 loss:13.886 precision=36.745%\n",
      "epoch:1, nombre batch:2801 loss:13.045 precision=38.798%\n",
      "epoch:1, nombre batch:2901 loss:13.769 precision=36.316%\n",
      "epoch:1, nombre batch:3001 loss:13.359 precision=38.726%\n",
      "epoch:1, nombre batch:3101 loss:13.601 precision=37.463%\n",
      "epoch:1, nombre batch:3201 loss:13.699 precision=37.491%\n",
      "epoch:1, nombre batch:3301 loss:13.756 precision=36.126%\n",
      "epoch:1, nombre batch:3401 loss:13.951 precision=35.874%\n",
      "epoch:1, nombre batch:3501 loss:13.784 precision=36.723%\n",
      "epoch:1, nombre batch:3601 loss:13.767 precision=36.125%\n",
      "epoch:1, nombre batch:3701 loss:13.861 precision=36.540%\n",
      "epoch:1, nombre batch:3801 loss:14.233 precision=35.004%\n",
      "epoch:1, nombre batch:3901 loss:14.109 precision=34.892%\n",
      "epoch:1, nombre batch:4001 loss:13.834 precision=36.485%\n",
      "epoch:1, nombre batch:4101 loss:13.682 precision=37.244%\n",
      "epoch:1, nombre batch:4201 loss:13.968 precision=35.498%\n",
      "epoch:1, nombre batch:4301 loss:14.069 precision=35.364%\n",
      "epoch:1, nombre batch:4401 loss:13.839 precision=36.218%\n",
      "epoch:1, nombre batch:4501 loss:13.710 precision=36.534%\n",
      "epoch:1, nombre batch:4601 loss:13.738 precision=35.535%\n",
      "epoch:1, nombre batch:4701 loss:13.592 precision=36.288%\n",
      "epoch:1, nombre batch:4801 loss:13.906 precision=36.035%\n",
      "epoch:1, nombre batch:4901 loss:14.122 precision=35.155%\n",
      "epoch:1, nombre batch:5001 loss:13.691 precision=35.979%\n",
      "epoch:1, nombre batch:5101 loss:13.962 precision=35.006%\n",
      "epoch:1, nombre batch:5201 loss:13.967 precision=36.310%\n",
      "epoch:1, nombre batch:5301 loss:13.852 precision=37.811%\n",
      "epoch:1, nombre batch:5401 loss:14.207 precision=35.147%\n",
      "epoch:1, nombre batch:5501 loss:13.881 precision=35.626%\n",
      "epoch:1, nombre batch:5601 loss:13.955 precision=35.881%\n",
      "epoch:1, nombre batch:5701 loss:13.958 precision=36.150%\n",
      "epoch:1, nombre batch:5801 loss:13.762 precision=35.788%\n",
      "epoch:1, nombre batch:5901 loss:14.279 precision=34.804%\n",
      "epoch:1, nombre batch:6001 loss:14.158 precision=35.074%\n",
      "epoch:1, nombre batch:6101 loss:13.991 precision=36.105%\n",
      "epoch:1, nombre batch:6201 loss:13.864 precision=36.577%\n",
      "epoch:1, nombre batch:6301 loss:14.145 precision=35.503%\n",
      "epoch:1, nombre batch:6401 loss:13.927 precision=35.704%\n",
      "epoch:1, nombre batch:6501 loss:13.756 precision=37.359%\n",
      "epoch:1, nombre batch:6601 loss:13.843 precision=37.285%\n",
      "epoch:1, nombre batch:6701 loss:14.299 precision=35.212%\n",
      "epoch:1, nombre batch:6801 loss:14.037 precision=35.843%\n",
      "epoch:1, nombre batch:6901 loss:14.461 precision=35.160%\n",
      "epoch:1, nombre batch:7001 loss:13.813 precision=35.372%\n",
      "epoch:1, nombre batch:7101 loss:14.166 precision=35.541%\n",
      "epoch:1, nombre batch:7201 loss:14.062 precision=36.681%\n",
      "epoch:1, nombre batch:7301 loss:13.507 precision=37.622%\n",
      "epoch:1, nombre batch:7401 loss:14.589 precision=33.545%\n",
      "epoch:1, nombre batch:7501 loss:14.029 precision=35.071%\n",
      "epoch:1, nombre batch:7601 loss:14.082 precision=34.594%\n",
      "epoch:1, nombre batch:7701 loss:14.061 precision=35.913%\n",
      "epoch:1, nombre batch:7801 loss:13.896 precision=34.915%\n",
      "epoch:1, nombre batch:7901 loss:13.829 precision=37.180%\n",
      "epoch:1, nombre batch:8001 loss:13.929 precision=36.000%\n",
      "epoch:1, nombre batch:8101 loss:13.759 precision=37.333%\n",
      "epoch:1, nombre batch:8201 loss:13.628 precision=37.004%\n",
      "epoch:1, nombre batch:8301 loss:14.142 precision=35.801%\n",
      "epoch:1, nombre batch:8401 loss:13.910 precision=36.370%\n",
      "epoch:1, nombre batch:8501 loss:13.828 precision=36.261%\n",
      "epoch:1, nombre batch:8601 loss:14.095 precision=35.307%\n",
      "epoch:1, nombre batch:8701 loss:14.004 precision=36.126%\n",
      "epoch:1, nombre batch:8801 loss:13.497 precision=37.189%\n",
      "epoch:1, nombre batch:8901 loss:13.937 precision=36.910%\n",
      "epoch:1, nombre batch:9001 loss:13.820 precision=36.584%\n",
      "epoch:1, nombre batch:9101 loss:14.002 precision=35.746%\n",
      "epoch:1, nombre batch:9201 loss:14.081 precision=36.370%\n",
      "epoch:1, nombre batch:9301 loss:13.929 precision=36.211%\n",
      "epoch:1, nombre batch:9401 loss:14.231 precision=33.975%\n",
      "epoch:1, nombre batch:9501 loss:13.853 precision=36.854%\n",
      "epoch:1, nombre batch:9601 loss:14.100 precision=35.484%\n",
      "epoch:1, nombre batch:9701 loss:14.156 precision=35.076%\n",
      "epoch:1, nombre batch:9801 loss:13.906 precision=36.043%\n",
      "epoch:1, nombre batch:9901 loss:14.133 precision=35.662%\n",
      "epoch:1, nombre batch:10001 loss:13.959 precision=36.881%\n",
      "epoch:1, nombre batch:10101 loss:14.382 precision=34.273%\n",
      "epoch:1, nombre batch:10201 loss:13.532 precision=37.768%\n",
      "epoch:1, nombre batch:10301 loss:13.545 precision=36.374%\n",
      "epoch:1, nombre batch:10401 loss:14.065 precision=35.356%\n",
      "epoch:1, nombre batch:10501 loss:14.050 precision=36.088%\n",
      "epoch:1, nombre batch:10601 loss:13.891 precision=37.182%\n",
      "epoch:1, nombre batch:10701 loss:13.751 precision=36.473%\n",
      "epoch:1, nombre batch:10801 loss:13.806 precision=35.417%\n",
      "epoch:1, nombre batch:10901 loss:14.242 precision=35.456%\n",
      "epoch:1, nombre batch:11001 loss:13.604 precision=37.777%\n",
      "epoch:1, nombre batch:11101 loss:13.941 precision=36.044%\n",
      "epoch:1, nombre batch:11201 loss:14.386 precision=35.033%\n",
      "epoch:1, nombre batch:11301 loss:13.645 precision=37.175%\n",
      "epoch:1, nombre batch:11401 loss:13.953 precision=35.622%\n",
      "epoch:1, nombre batch:11501 loss:14.319 precision=35.126%\n",
      "epoch:1, nombre batch:11601 loss:13.901 precision=36.064%\n",
      "epoch:1, nombre batch:11701 loss:13.986 precision=36.028%\n",
      "epoch:1, nombre batch:11801 loss:13.903 precision=36.252%\n",
      "epoch:1, nombre batch:11901 loss:13.522 precision=36.876%\n",
      "epoch:1, nombre batch:12001 loss:13.824 precision=35.690%\n",
      "epoch:1, nombre batch:12101 loss:13.997 precision=35.991%\n",
      "epoch:1, nombre batch:12201 loss:13.709 precision=36.659%\n",
      "epoch:1, nombre batch:12301 loss:13.981 precision=36.522%\n",
      "epoch:1, nombre batch:12401 loss:13.575 precision=37.402%\n",
      "epoch:1, nombre batch:12501 loss:13.800 precision=35.638%\n",
      "epoch:1, nombre batch:12601 loss:13.509 precision=37.787%\n",
      "epoch:1, nombre batch:12701 loss:14.305 precision=33.384%\n",
      "epoch:1, nombre batch:12801 loss:14.156 precision=35.746%\n",
      "epoch:1, nombre batch:12901 loss:13.495 precision=37.828%\n",
      "epoch:1, nombre batch:13001 loss:14.051 precision=35.650%\n",
      "epoch:1, nombre batch:13101 loss:13.876 precision=36.158%\n",
      "epoch:1, nombre batch:13201 loss:13.735 precision=36.781%\n",
      "epoch:1, nombre batch:13301 loss:13.574 precision=37.580%\n",
      "epoch:1, nombre batch:13401 loss:14.012 precision=34.896%\n",
      "epoch:1, nombre batch:13501 loss:14.233 precision=35.394%\n",
      "epoch:1, nombre batch:13601 loss:14.050 precision=35.975%\n",
      "epoch:1, nombre batch:13701 loss:13.935 precision=35.820%\n",
      "epoch:1, nombre batch:13801 loss:13.727 precision=36.415%\n",
      "epoch:1, nombre batch:13901 loss:14.009 precision=36.397%\n",
      "epoch:1, nombre batch:14001 loss:13.846 precision=35.847%\n",
      "epoch:1, nombre batch:14101 loss:14.017 precision=35.336%\n",
      "epoch:1, nombre batch:14201 loss:13.959 precision=36.822%\n",
      "epoch:1, nombre batch:14301 loss:13.581 precision=37.132%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, nombre batch:14401 loss:13.574 precision=36.911%\n",
      "epoch:1, nombre batch:14501 loss:13.547 precision=37.696%\n",
      "epoch:1, nombre batch:14601 loss:14.315 precision=35.030%\n",
      "epoch:1, nombre batch:14701 loss:14.329 precision=33.665%\n",
      "epoch:1, nombre batch:14801 loss:13.539 precision=37.766%\n",
      "epoch:1, nombre batch:14901 loss:14.022 precision=36.508%\n",
      "epoch:1, nombre batch:15001 loss:13.682 precision=36.439%\n",
      "epoch:1, nombre batch:15101 loss:13.966 precision=35.960%\n",
      "epoch:1, nombre batch:15201 loss:13.655 precision=36.761%\n",
      "epoch:1, nombre batch:15301 loss:13.520 precision=37.524%\n",
      "epoch:1, nombre batch:15401 loss:14.462 precision=34.254%\n",
      "epoch:1, nombre batch:15501 loss:14.195 precision=35.111%\n",
      "epoch:1, nombre batch:15601 loss:14.295 precision=35.424%\n",
      "epoch:1, nombre batch:15701 loss:13.480 precision=37.633%\n",
      "epoch:1, nombre batch:15801 loss:13.715 precision=36.513%\n",
      "epoch:1, nombre batch:15901 loss:14.344 precision=35.733%\n",
      "epoch:1, nombre batch:16001 loss:13.909 precision=35.586%\n",
      "epoch:1, nombre batch:16101 loss:14.403 precision=34.219%\n",
      "epoch:1, nombre batch:16201 loss:14.023 precision=36.440%\n",
      "epoch:1, nombre batch:16301 loss:13.589 precision=37.411%\n",
      "epoch:1, nombre batch:16401 loss:14.149 precision=34.495%\n",
      "epoch:1, nombre batch:16501 loss:13.963 precision=35.954%\n",
      "epoch:1, nombre batch:16601 loss:14.070 precision=35.887%\n",
      "epoch:1, nombre batch:16701 loss:13.814 precision=37.034%\n",
      "epoch:1, nombre batch:16801 loss:14.246 precision=35.762%\n",
      "epoch:1, nombre batch:16901 loss:13.636 precision=36.884%\n",
      "epoch:1, nombre batch:17001 loss:13.932 precision=37.006%\n",
      "epoch:1, nombre batch:17101 loss:13.578 precision=37.463%\n",
      "epoch:1, nombre batch:17201 loss:14.114 precision=35.556%\n",
      "epoch:1, nombre batch:17301 loss:14.020 precision=34.996%\n",
      "epoch:1, nombre batch:17401 loss:13.914 precision=36.564%\n",
      "epoch:1, nombre batch:17501 loss:13.904 precision=36.038%\n",
      "epoch:1, nombre batch:17601 loss:14.137 precision=35.717%\n",
      "epoch:1, nombre batch:17701 loss:13.875 precision=35.402%\n",
      "epoch:1, nombre batch:17801 loss:13.346 precision=38.604%\n",
      "epoch:1, nombre batch:17901 loss:14.228 precision=35.314%\n",
      "epoch:1, nombre batch:18001 loss:13.974 precision=36.269%\n",
      "epoch:1, nombre batch:18101 loss:14.557 precision=34.126%\n",
      "epoch:1, nombre batch:18201 loss:13.778 precision=36.793%\n",
      "epoch:1, nombre batch:18301 loss:14.204 precision=34.696%\n",
      "epoch:1, nombre batch:18401 loss:13.865 precision=35.846%\n",
      "epoch:1, nombre batch:18501 loss:13.783 precision=37.078%\n",
      "epoch:1, nombre batch:18601 loss:14.306 precision=34.059%\n",
      "epoch:1, nombre batch:18701 loss:13.727 precision=37.440%\n",
      "epoch:1, nombre batch:18801 loss:14.359 precision=35.589%\n",
      "epoch:1, nombre batch:18901 loss:13.653 precision=37.411%\n",
      "epoch:1, nombre batch:19001 loss:13.712 precision=37.899%\n",
      "epoch:1, nombre batch:19101 loss:13.745 precision=35.880%\n",
      "epoch:1, nombre batch:19201 loss:13.693 precision=37.523%\n",
      "epoch:1, nombre batch:19301 loss:14.203 precision=35.196%\n",
      "epoch:1, nombre batch:19401 loss:13.841 precision=37.103%\n",
      "epoch:1, nombre batch:19501 loss:13.813 precision=36.866%\n",
      "epoch:1, nombre batch:19601 loss:13.927 precision=36.162%\n",
      "epoch:1, nombre batch:19701 loss:14.279 precision=35.309%\n",
      "epoch:1, nombre batch:19801 loss:13.683 precision=37.020%\n",
      "epoch:1, nombre batch:19901 loss:13.575 precision=36.319%\n",
      "epoch:1, nombre batch:20001 loss:14.154 precision=34.932%\n",
      "epoch:1, nombre batch:20101 loss:13.495 precision=37.152%\n",
      "epoch:1, nombre batch:20201 loss:13.906 precision=36.942%\n",
      "epoch:1, nombre batch:20301 loss:13.994 precision=35.299%\n",
      "epoch:1, nombre batch:20401 loss:13.682 precision=37.453%\n",
      "epoch:1, nombre batch:20501 loss:14.267 precision=35.195%\n",
      "epoch:1, nombre batch:20601 loss:13.532 precision=38.920%\n",
      "epoch:1, nombre batch:20701 loss:14.070 precision=35.698%\n",
      "epoch:1, nombre batch:20801 loss:13.958 precision=36.015%\n",
      "epoch:1, nombre batch:20901 loss:13.733 precision=36.726%\n",
      "epoch:1, nombre batch:21001 loss:14.304 precision=35.035%\n",
      "epoch:1, nombre batch:21101 loss:14.180 precision=36.731%\n",
      "epoch:1, nombre batch:21201 loss:14.042 precision=36.225%\n",
      "epoch:1, nombre batch:21301 loss:13.974 precision=36.505%\n",
      "epoch:1, nombre batch:21401 loss:13.657 precision=37.363%\n",
      "epoch:1, nombre batch:21501 loss:13.894 precision=36.370%\n",
      "epoch:1, nombre batch:21601 loss:13.676 precision=37.430%\n",
      "epoch:1, nombre batch:21701 loss:13.447 precision=38.473%\n",
      "epoch:1, nombre batch:21801 loss:13.819 precision=36.738%\n",
      "epoch:1, nombre batch:21901 loss:14.230 precision=35.525%\n",
      "epoch:1, nombre batch:22001 loss:13.487 precision=37.140%\n",
      "epoch:1, nombre batch:22101 loss:14.315 precision=34.405%\n",
      "epoch:1, nombre batch:22201 loss:13.829 precision=36.721%\n",
      "epoch:1, nombre batch:22301 loss:13.772 precision=36.779%\n",
      "epoch:1, nombre batch:22401 loss:13.919 precision=36.225%\n",
      "epoch:1, nombre batch:22501 loss:13.605 precision=36.984%\n",
      "epoch:1, nombre batch:22601 loss:13.885 precision=36.425%\n",
      "epoch:1, nombre batch:22701 loss:14.018 precision=34.852%\n",
      "epoch:1, nombre batch:22801 loss:13.819 precision=37.380%\n",
      "epoch:1, nombre batch:22901 loss:13.942 precision=36.495%\n",
      "epoch:1, nombre batch:23001 loss:13.858 precision=37.601%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultat Epoch 1;  loss:13.904, Accuracy=42.717%,  precision:36.189%, AMS=0.9860\n",
      "epoch:2, nombre batch:1 loss:0.148 precision=34.483%\n",
      "epoch:2, nombre batch:101 loss:14.119 precision=36.106%\n",
      "epoch:2, nombre batch:201 loss:13.634 precision=36.305%\n",
      "epoch:2, nombre batch:301 loss:13.780 precision=37.638%\n",
      "epoch:2, nombre batch:401 loss:13.619 precision=37.893%\n",
      "epoch:2, nombre batch:501 loss:13.490 precision=38.410%\n",
      "epoch:2, nombre batch:601 loss:13.916 precision=35.845%\n",
      "epoch:2, nombre batch:701 loss:14.454 precision=34.755%\n",
      "epoch:2, nombre batch:801 loss:14.049 precision=36.201%\n",
      "epoch:2, nombre batch:901 loss:13.595 precision=37.528%\n",
      "epoch:2, nombre batch:1001 loss:14.076 precision=36.784%\n",
      "epoch:2, nombre batch:1101 loss:13.705 precision=37.425%\n",
      "epoch:2, nombre batch:1201 loss:13.858 precision=37.080%\n",
      "epoch:2, nombre batch:1301 loss:13.799 precision=35.413%\n",
      "epoch:2, nombre batch:1401 loss:14.253 precision=34.895%\n",
      "epoch:2, nombre batch:1501 loss:14.106 precision=36.016%\n",
      "epoch:2, nombre batch:1601 loss:14.160 precision=36.380%\n",
      "epoch:2, nombre batch:1701 loss:14.178 precision=36.347%\n",
      "epoch:2, nombre batch:1801 loss:13.724 precision=36.755%\n",
      "epoch:2, nombre batch:1901 loss:13.789 precision=37.016%\n",
      "epoch:2, nombre batch:2001 loss:14.129 precision=35.192%\n",
      "epoch:2, nombre batch:2101 loss:13.754 precision=36.718%\n",
      "epoch:2, nombre batch:2201 loss:13.822 precision=36.933%\n",
      "epoch:2, nombre batch:2301 loss:13.795 precision=36.924%\n",
      "epoch:2, nombre batch:2401 loss:13.929 precision=36.032%\n",
      "epoch:2, nombre batch:2501 loss:13.712 precision=36.378%\n",
      "epoch:2, nombre batch:2601 loss:14.237 precision=34.171%\n",
      "epoch:2, nombre batch:2701 loss:14.276 precision=34.823%\n",
      "epoch:2, nombre batch:2801 loss:13.428 precision=37.965%\n",
      "epoch:2, nombre batch:2901 loss:13.819 precision=36.535%\n",
      "epoch:2, nombre batch:3001 loss:13.989 precision=35.605%\n",
      "epoch:2, nombre batch:3101 loss:13.914 precision=36.453%\n",
      "epoch:2, nombre batch:3201 loss:13.823 precision=36.700%\n",
      "epoch:2, nombre batch:3301 loss:13.709 precision=36.914%\n",
      "epoch:2, nombre batch:3401 loss:13.653 precision=37.068%\n",
      "epoch:2, nombre batch:3501 loss:13.787 precision=37.058%\n",
      "epoch:2, nombre batch:3601 loss:13.540 precision=36.060%\n",
      "epoch:2, nombre batch:3701 loss:13.999 precision=35.914%\n",
      "epoch:2, nombre batch:3801 loss:14.008 precision=36.218%\n",
      "epoch:2, nombre batch:3901 loss:13.976 precision=36.486%\n",
      "epoch:2, nombre batch:4001 loss:13.932 precision=36.026%\n",
      "epoch:2, nombre batch:4101 loss:14.097 precision=35.825%\n",
      "epoch:2, nombre batch:4201 loss:13.879 precision=36.015%\n",
      "epoch:2, nombre batch:4301 loss:13.895 precision=36.291%\n",
      "epoch:2, nombre batch:4401 loss:13.800 precision=36.677%\n",
      "epoch:2, nombre batch:4501 loss:13.905 precision=36.244%\n",
      "epoch:2, nombre batch:4601 loss:14.179 precision=35.698%\n",
      "epoch:2, nombre batch:4701 loss:14.300 precision=34.668%\n",
      "epoch:2, nombre batch:4801 loss:14.118 precision=35.762%\n",
      "epoch:2, nombre batch:4901 loss:13.785 precision=36.748%\n",
      "epoch:2, nombre batch:5001 loss:14.081 precision=34.553%\n",
      "epoch:2, nombre batch:5101 loss:13.890 precision=36.832%\n",
      "epoch:2, nombre batch:5201 loss:13.986 precision=36.105%\n",
      "epoch:2, nombre batch:5301 loss:14.227 precision=35.138%\n",
      "epoch:2, nombre batch:5401 loss:14.013 precision=35.497%\n",
      "epoch:2, nombre batch:5501 loss:13.767 precision=37.146%\n",
      "epoch:2, nombre batch:5601 loss:13.688 precision=37.048%\n",
      "epoch:2, nombre batch:5701 loss:13.644 precision=37.401%\n",
      "epoch:2, nombre batch:5801 loss:13.669 precision=36.929%\n",
      "epoch:2, nombre batch:5901 loss:13.692 precision=36.198%\n",
      "epoch:2, nombre batch:6001 loss:14.169 precision=35.889%\n",
      "epoch:2, nombre batch:6101 loss:14.046 precision=35.664%\n",
      "epoch:2, nombre batch:6201 loss:13.848 precision=37.472%\n",
      "epoch:2, nombre batch:6301 loss:13.854 precision=37.074%\n",
      "epoch:2, nombre batch:6401 loss:13.643 precision=37.143%\n",
      "epoch:2, nombre batch:6501 loss:13.546 precision=37.632%\n",
      "epoch:2, nombre batch:6601 loss:14.009 precision=35.741%\n",
      "epoch:2, nombre batch:6701 loss:14.077 precision=35.401%\n",
      "epoch:2, nombre batch:6801 loss:14.089 precision=35.922%\n",
      "epoch:2, nombre batch:6901 loss:13.683 precision=36.836%\n",
      "epoch:2, nombre batch:7001 loss:14.313 precision=34.019%\n",
      "epoch:2, nombre batch:7101 loss:13.697 precision=36.818%\n",
      "epoch:2, nombre batch:7201 loss:14.073 precision=35.488%\n",
      "epoch:2, nombre batch:7301 loss:14.133 precision=35.754%\n",
      "epoch:2, nombre batch:7401 loss:13.812 precision=37.022%\n",
      "epoch:2, nombre batch:7501 loss:14.314 precision=35.303%\n",
      "epoch:2, nombre batch:7601 loss:13.901 precision=37.102%\n",
      "epoch:2, nombre batch:7701 loss:13.853 precision=36.874%\n",
      "epoch:2, nombre batch:7801 loss:14.092 precision=35.487%\n",
      "epoch:2, nombre batch:7901 loss:13.458 precision=36.409%\n",
      "epoch:2, nombre batch:8001 loss:14.391 precision=35.630%\n",
      "epoch:2, nombre batch:8101 loss:14.094 precision=34.796%\n",
      "epoch:2, nombre batch:8201 loss:13.488 precision=37.298%\n",
      "epoch:2, nombre batch:8301 loss:13.973 precision=36.100%\n",
      "epoch:2, nombre batch:8401 loss:14.429 precision=35.454%\n",
      "epoch:2, nombre batch:8501 loss:14.357 precision=35.318%\n",
      "epoch:2, nombre batch:8601 loss:14.085 precision=35.535%\n",
      "epoch:2, nombre batch:8701 loss:13.893 precision=36.044%\n",
      "epoch:2, nombre batch:8801 loss:14.086 precision=36.064%\n",
      "epoch:2, nombre batch:8901 loss:13.905 precision=35.871%\n",
      "epoch:2, nombre batch:9001 loss:14.013 precision=35.350%\n",
      "epoch:2, nombre batch:9101 loss:13.741 precision=37.041%\n",
      "epoch:2, nombre batch:9201 loss:14.346 precision=35.277%\n",
      "epoch:2, nombre batch:9301 loss:13.674 precision=36.930%\n",
      "epoch:2, nombre batch:9401 loss:14.376 precision=35.348%\n",
      "epoch:2, nombre batch:9501 loss:13.889 precision=36.928%\n",
      "epoch:2, nombre batch:9601 loss:14.060 precision=35.997%\n",
      "epoch:2, nombre batch:9701 loss:13.921 precision=36.832%\n",
      "epoch:2, nombre batch:9801 loss:13.565 precision=37.641%\n",
      "epoch:2, nombre batch:9901 loss:13.284 precision=37.906%\n",
      "epoch:2, nombre batch:10001 loss:13.508 precision=39.113%\n",
      "epoch:2, nombre batch:10101 loss:14.002 precision=36.623%\n",
      "epoch:2, nombre batch:10201 loss:13.608 precision=36.558%\n",
      "epoch:2, nombre batch:10301 loss:13.920 precision=35.803%\n",
      "epoch:2, nombre batch:10401 loss:13.914 precision=37.564%\n",
      "epoch:2, nombre batch:10501 loss:13.606 precision=37.608%\n",
      "epoch:2, nombre batch:10601 loss:14.003 precision=35.932%\n",
      "epoch:2, nombre batch:10701 loss:14.115 precision=35.993%\n",
      "epoch:2, nombre batch:10801 loss:13.658 precision=36.952%\n",
      "epoch:2, nombre batch:10901 loss:14.075 precision=36.033%\n",
      "epoch:2, nombre batch:11001 loss:13.429 precision=38.126%\n",
      "epoch:2, nombre batch:11101 loss:14.002 precision=35.500%\n",
      "epoch:2, nombre batch:11201 loss:13.573 precision=38.244%\n",
      "epoch:2, nombre batch:11301 loss:13.984 precision=36.513%\n",
      "epoch:2, nombre batch:11401 loss:13.437 precision=37.109%\n",
      "epoch:2, nombre batch:11501 loss:13.365 precision=37.495%\n",
      "epoch:2, nombre batch:11601 loss:13.910 precision=36.230%\n",
      "epoch:2, nombre batch:11701 loss:13.813 precision=37.012%\n",
      "epoch:2, nombre batch:11801 loss:13.670 precision=37.439%\n",
      "epoch:2, nombre batch:11901 loss:13.649 precision=37.284%\n",
      "epoch:2, nombre batch:12001 loss:14.348 precision=34.640%\n",
      "epoch:2, nombre batch:12101 loss:13.986 precision=35.865%\n",
      "epoch:2, nombre batch:12201 loss:14.134 precision=36.580%\n",
      "epoch:2, nombre batch:12301 loss:13.746 precision=37.268%\n",
      "epoch:2, nombre batch:12401 loss:13.686 precision=36.860%\n",
      "epoch:2, nombre batch:12501 loss:14.080 precision=36.875%\n",
      "epoch:2, nombre batch:12601 loss:14.279 precision=35.314%\n",
      "epoch:2, nombre batch:12701 loss:13.985 precision=36.169%\n",
      "epoch:2, nombre batch:12801 loss:13.768 precision=37.133%\n",
      "epoch:2, nombre batch:12901 loss:13.274 precision=37.162%\n",
      "epoch:2, nombre batch:13001 loss:14.084 precision=35.913%\n",
      "epoch:2, nombre batch:13101 loss:13.382 precision=38.164%\n",
      "epoch:2, nombre batch:13201 loss:14.180 precision=35.701%\n",
      "epoch:2, nombre batch:13301 loss:13.519 precision=38.211%\n",
      "epoch:2, nombre batch:13401 loss:14.481 precision=34.862%\n",
      "epoch:2, nombre batch:13501 loss:13.791 precision=35.490%\n",
      "epoch:2, nombre batch:13601 loss:13.889 precision=36.150%\n",
      "epoch:2, nombre batch:13701 loss:13.859 precision=36.765%\n",
      "epoch:2, nombre batch:13801 loss:14.402 precision=34.711%\n",
      "epoch:2, nombre batch:13901 loss:13.516 precision=36.688%\n",
      "epoch:2, nombre batch:14001 loss:14.137 precision=34.992%\n",
      "epoch:2, nombre batch:14101 loss:13.741 precision=37.462%\n",
      "epoch:2, nombre batch:14201 loss:13.948 precision=35.242%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, nombre batch:14301 loss:13.662 precision=37.015%\n",
      "epoch:2, nombre batch:14401 loss:13.781 precision=36.270%\n",
      "epoch:2, nombre batch:14501 loss:13.404 precision=38.528%\n",
      "epoch:2, nombre batch:14601 loss:13.873 precision=37.023%\n",
      "epoch:2, nombre batch:14701 loss:14.215 precision=34.794%\n",
      "epoch:2, nombre batch:14801 loss:14.110 precision=35.105%\n",
      "epoch:2, nombre batch:14901 loss:13.975 precision=36.016%\n",
      "epoch:2, nombre batch:15001 loss:14.213 precision=34.375%\n",
      "epoch:2, nombre batch:15101 loss:14.010 precision=36.115%\n",
      "epoch:2, nombre batch:15201 loss:13.883 precision=37.255%\n",
      "epoch:2, nombre batch:15301 loss:13.604 precision=36.458%\n",
      "epoch:2, nombre batch:15401 loss:14.476 precision=35.106%\n",
      "epoch:2, nombre batch:15501 loss:14.074 precision=36.032%\n",
      "epoch:2, nombre batch:15601 loss:13.933 precision=37.181%\n",
      "epoch:2, nombre batch:15701 loss:13.966 precision=36.632%\n",
      "epoch:2, nombre batch:15801 loss:13.568 precision=36.593%\n",
      "epoch:2, nombre batch:15901 loss:13.905 precision=36.185%\n",
      "epoch:2, nombre batch:16001 loss:13.568 precision=38.025%\n",
      "epoch:2, nombre batch:16101 loss:14.017 precision=36.021%\n",
      "epoch:2, nombre batch:16201 loss:14.083 precision=35.940%\n",
      "epoch:2, nombre batch:16301 loss:14.099 precision=35.770%\n",
      "epoch:2, nombre batch:16401 loss:13.956 precision=35.907%\n",
      "epoch:2, nombre batch:16501 loss:13.928 precision=36.554%\n",
      "epoch:2, nombre batch:16601 loss:13.945 precision=36.788%\n",
      "epoch:2, nombre batch:16701 loss:13.896 precision=36.077%\n",
      "epoch:2, nombre batch:16801 loss:13.686 precision=37.397%\n",
      "epoch:2, nombre batch:16901 loss:14.646 precision=33.854%\n",
      "epoch:2, nombre batch:17001 loss:13.742 precision=37.103%\n",
      "epoch:2, nombre batch:17101 loss:14.207 precision=35.637%\n",
      "epoch:2, nombre batch:17201 loss:13.762 precision=36.298%\n",
      "epoch:2, nombre batch:17301 loss:14.193 precision=35.677%\n",
      "epoch:2, nombre batch:17401 loss:14.145 precision=36.246%\n",
      "epoch:2, nombre batch:17501 loss:13.811 precision=37.045%\n",
      "epoch:2, nombre batch:17601 loss:13.150 precision=38.319%\n",
      "epoch:2, nombre batch:17701 loss:13.974 precision=36.498%\n",
      "epoch:2, nombre batch:17801 loss:14.103 precision=35.128%\n",
      "epoch:2, nombre batch:17901 loss:14.250 precision=34.681%\n",
      "epoch:2, nombre batch:18001 loss:13.568 precision=36.990%\n",
      "epoch:2, nombre batch:18101 loss:13.650 precision=37.172%\n",
      "epoch:2, nombre batch:18201 loss:13.897 precision=35.780%\n",
      "epoch:2, nombre batch:18301 loss:13.932 precision=36.640%\n",
      "epoch:2, nombre batch:18401 loss:13.544 precision=37.200%\n",
      "epoch:2, nombre batch:18501 loss:13.965 precision=36.367%\n",
      "epoch:2, nombre batch:18601 loss:13.455 precision=37.807%\n",
      "epoch:2, nombre batch:18701 loss:13.881 precision=36.983%\n",
      "epoch:2, nombre batch:18801 loss:13.718 precision=36.015%\n",
      "epoch:2, nombre batch:18901 loss:13.528 precision=37.925%\n",
      "epoch:2, nombre batch:19001 loss:13.814 precision=35.625%\n",
      "epoch:2, nombre batch:19101 loss:14.027 precision=35.068%\n",
      "epoch:2, nombre batch:19201 loss:13.884 precision=36.630%\n",
      "epoch:2, nombre batch:19301 loss:14.047 precision=35.265%\n",
      "epoch:2, nombre batch:19401 loss:14.141 precision=34.968%\n",
      "epoch:2, nombre batch:19501 loss:13.651 precision=36.528%\n",
      "epoch:2, nombre batch:19601 loss:13.761 precision=35.159%\n",
      "epoch:2, nombre batch:19701 loss:13.736 precision=38.067%\n",
      "epoch:2, nombre batch:19801 loss:14.075 precision=36.192%\n",
      "epoch:2, nombre batch:19901 loss:13.701 precision=36.044%\n",
      "epoch:2, nombre batch:20001 loss:13.110 precision=39.657%\n",
      "epoch:2, nombre batch:20101 loss:13.749 precision=36.685%\n",
      "epoch:2, nombre batch:20201 loss:13.925 precision=36.669%\n",
      "epoch:2, nombre batch:20301 loss:13.676 precision=36.726%\n",
      "epoch:2, nombre batch:20401 loss:13.882 precision=36.832%\n",
      "epoch:2, nombre batch:20501 loss:13.719 precision=37.205%\n",
      "epoch:2, nombre batch:20601 loss:14.138 precision=36.484%\n",
      "epoch:2, nombre batch:20701 loss:13.619 precision=36.917%\n",
      "epoch:2, nombre batch:20801 loss:14.289 precision=34.774%\n",
      "epoch:2, nombre batch:20901 loss:14.035 precision=36.551%\n",
      "epoch:2, nombre batch:21001 loss:14.258 precision=34.866%\n",
      "epoch:2, nombre batch:21101 loss:13.822 precision=35.635%\n",
      "epoch:2, nombre batch:21201 loss:13.784 precision=36.752%\n",
      "epoch:2, nombre batch:21301 loss:13.994 precision=36.414%\n",
      "epoch:2, nombre batch:21401 loss:14.014 precision=35.321%\n",
      "epoch:2, nombre batch:21501 loss:14.316 precision=35.093%\n",
      "epoch:2, nombre batch:21601 loss:13.619 precision=37.586%\n",
      "epoch:2, nombre batch:21701 loss:13.743 precision=37.101%\n",
      "epoch:2, nombre batch:21801 loss:13.654 precision=36.637%\n",
      "epoch:2, nombre batch:21901 loss:13.780 precision=35.774%\n",
      "epoch:2, nombre batch:22001 loss:14.046 precision=35.556%\n",
      "epoch:2, nombre batch:22101 loss:14.100 precision=35.886%\n",
      "epoch:2, nombre batch:22201 loss:14.028 precision=36.121%\n",
      "epoch:2, nombre batch:22301 loss:14.145 precision=36.040%\n",
      "epoch:2, nombre batch:22401 loss:14.380 precision=35.006%\n",
      "epoch:2, nombre batch:22501 loss:13.715 precision=36.291%\n",
      "epoch:2, nombre batch:22601 loss:13.670 precision=37.284%\n",
      "epoch:2, nombre batch:22701 loss:13.848 precision=36.494%\n",
      "epoch:2, nombre batch:22801 loss:13.953 precision=36.021%\n",
      "epoch:2, nombre batch:22901 loss:13.633 precision=36.534%\n",
      "epoch:2, nombre batch:23001 loss:13.976 precision=37.345%\n",
      "Resultat Epoch 2;  loss:13.897, Accuracy=43.132%,  precision:36.365%, AMS=0.9636\n",
      "epoch:3, nombre batch:1 loss:0.148 precision=35.714%\n",
      "epoch:3, nombre batch:101 loss:14.370 precision=34.548%\n",
      "epoch:3, nombre batch:201 loss:13.901 precision=36.470%\n",
      "epoch:3, nombre batch:301 loss:14.202 precision=35.656%\n",
      "epoch:3, nombre batch:401 loss:13.961 precision=36.148%\n",
      "epoch:3, nombre batch:501 loss:14.140 precision=36.109%\n",
      "epoch:3, nombre batch:601 loss:13.871 precision=36.783%\n",
      "epoch:3, nombre batch:701 loss:13.792 precision=36.606%\n",
      "epoch:3, nombre batch:801 loss:13.879 precision=35.893%\n",
      "epoch:3, nombre batch:901 loss:13.965 precision=36.979%\n",
      "epoch:3, nombre batch:1001 loss:13.956 precision=36.716%\n",
      "epoch:3, nombre batch:1101 loss:13.592 precision=37.420%\n",
      "epoch:3, nombre batch:1201 loss:13.726 precision=37.533%\n",
      "epoch:3, nombre batch:1301 loss:13.537 precision=38.134%\n",
      "epoch:3, nombre batch:1401 loss:14.092 precision=34.977%\n",
      "epoch:3, nombre batch:1501 loss:13.801 precision=36.618%\n",
      "epoch:3, nombre batch:1601 loss:14.100 precision=35.952%\n",
      "epoch:3, nombre batch:1701 loss:14.292 precision=35.090%\n",
      "epoch:3, nombre batch:1801 loss:13.704 precision=36.674%\n",
      "epoch:3, nombre batch:1901 loss:13.599 precision=36.977%\n",
      "epoch:3, nombre batch:2001 loss:14.022 precision=36.760%\n",
      "epoch:3, nombre batch:2101 loss:14.216 precision=35.366%\n",
      "epoch:3, nombre batch:2201 loss:13.433 precision=37.299%\n",
      "epoch:3, nombre batch:2301 loss:14.118 precision=36.024%\n",
      "epoch:3, nombre batch:2401 loss:13.937 precision=35.846%\n",
      "epoch:3, nombre batch:2501 loss:14.192 precision=35.127%\n",
      "epoch:3, nombre batch:2601 loss:14.193 precision=34.634%\n",
      "epoch:3, nombre batch:2701 loss:13.531 precision=36.954%\n",
      "epoch:3, nombre batch:2801 loss:13.960 precision=36.165%\n",
      "epoch:3, nombre batch:2901 loss:13.569 precision=36.286%\n",
      "epoch:3, nombre batch:3001 loss:13.897 precision=36.265%\n",
      "epoch:3, nombre batch:3101 loss:14.096 precision=36.204%\n",
      "epoch:3, nombre batch:3201 loss:14.019 precision=36.367%\n",
      "epoch:3, nombre batch:3301 loss:14.225 precision=34.806%\n",
      "epoch:3, nombre batch:3401 loss:13.847 precision=35.667%\n",
      "epoch:3, nombre batch:3501 loss:13.729 precision=36.769%\n",
      "epoch:3, nombre batch:3601 loss:13.515 precision=37.690%\n",
      "epoch:3, nombre batch:3701 loss:13.545 precision=37.737%\n",
      "epoch:3, nombre batch:3801 loss:14.011 precision=35.760%\n",
      "epoch:3, nombre batch:3901 loss:13.607 precision=38.206%\n",
      "epoch:3, nombre batch:4001 loss:13.974 precision=36.623%\n",
      "epoch:3, nombre batch:4101 loss:14.136 precision=35.725%\n",
      "epoch:3, nombre batch:4201 loss:14.282 precision=35.582%\n",
      "epoch:3, nombre batch:4301 loss:14.202 precision=35.188%\n",
      "epoch:3, nombre batch:4401 loss:13.927 precision=35.641%\n",
      "epoch:3, nombre batch:4501 loss:14.298 precision=34.666%\n",
      "epoch:3, nombre batch:4601 loss:13.886 precision=36.693%\n",
      "epoch:3, nombre batch:4701 loss:13.887 precision=37.197%\n",
      "epoch:3, nombre batch:4801 loss:13.877 precision=36.918%\n",
      "epoch:3, nombre batch:4901 loss:13.835 precision=37.710%\n",
      "epoch:3, nombre batch:5001 loss:14.198 precision=35.281%\n",
      "epoch:3, nombre batch:5101 loss:14.021 precision=35.568%\n",
      "epoch:3, nombre batch:5201 loss:14.101 precision=35.097%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, nombre batch:5301 loss:14.103 precision=35.763%\n",
      "epoch:3, nombre batch:5401 loss:13.914 precision=35.992%\n",
      "epoch:3, nombre batch:5501 loss:13.640 precision=38.085%\n",
      "epoch:3, nombre batch:5601 loss:13.805 precision=36.815%\n",
      "epoch:3, nombre batch:5701 loss:13.648 precision=37.725%\n",
      "epoch:3, nombre batch:5801 loss:13.780 precision=36.426%\n",
      "epoch:3, nombre batch:5901 loss:13.844 precision=36.123%\n",
      "epoch:3, nombre batch:6001 loss:13.874 precision=36.136%\n",
      "epoch:3, nombre batch:6101 loss:13.821 precision=37.944%\n",
      "epoch:3, nombre batch:6201 loss:14.049 precision=36.279%\n",
      "epoch:3, nombre batch:6301 loss:14.118 precision=35.414%\n",
      "epoch:3, nombre batch:6401 loss:13.980 precision=37.596%\n",
      "epoch:3, nombre batch:6501 loss:13.799 precision=34.864%\n",
      "epoch:3, nombre batch:6601 loss:14.254 precision=35.288%\n",
      "epoch:3, nombre batch:6701 loss:13.857 precision=37.079%\n",
      "epoch:3, nombre batch:6801 loss:14.119 precision=35.947%\n",
      "epoch:3, nombre batch:6901 loss:13.796 precision=36.980%\n",
      "epoch:3, nombre batch:7001 loss:13.781 precision=36.061%\n",
      "epoch:3, nombre batch:7101 loss:13.570 precision=36.986%\n",
      "epoch:3, nombre batch:7201 loss:13.672 precision=37.222%\n",
      "epoch:3, nombre batch:7301 loss:13.725 precision=36.504%\n",
      "epoch:3, nombre batch:7401 loss:13.750 precision=38.039%\n",
      "epoch:3, nombre batch:7501 loss:13.670 precision=38.442%\n",
      "epoch:3, nombre batch:7601 loss:13.938 precision=36.613%\n",
      "epoch:3, nombre batch:7701 loss:13.744 precision=37.019%\n",
      "epoch:3, nombre batch:7801 loss:13.828 precision=37.198%\n",
      "epoch:3, nombre batch:7901 loss:13.526 precision=38.192%\n",
      "epoch:3, nombre batch:8001 loss:13.785 precision=36.866%\n",
      "epoch:3, nombre batch:8101 loss:13.774 precision=36.308%\n",
      "epoch:3, nombre batch:8201 loss:13.965 precision=36.553%\n",
      "epoch:3, nombre batch:8301 loss:13.730 precision=37.248%\n",
      "epoch:3, nombre batch:8401 loss:13.837 precision=36.360%\n",
      "epoch:3, nombre batch:8501 loss:14.123 precision=35.149%\n",
      "epoch:3, nombre batch:8601 loss:13.949 precision=37.108%\n",
      "epoch:3, nombre batch:8701 loss:14.130 precision=35.575%\n",
      "epoch:3, nombre batch:8801 loss:13.757 precision=37.444%\n",
      "epoch:3, nombre batch:8901 loss:14.115 precision=35.594%\n",
      "epoch:3, nombre batch:9001 loss:13.328 precision=37.674%\n",
      "epoch:3, nombre batch:9101 loss:14.293 precision=34.720%\n",
      "epoch:3, nombre batch:9201 loss:13.799 precision=36.877%\n",
      "epoch:3, nombre batch:9301 loss:13.741 precision=37.491%\n",
      "epoch:3, nombre batch:9401 loss:14.129 precision=36.320%\n",
      "epoch:3, nombre batch:9501 loss:13.817 precision=36.040%\n",
      "epoch:3, nombre batch:9601 loss:13.665 precision=38.556%\n",
      "epoch:3, nombre batch:9701 loss:13.966 precision=36.086%\n",
      "epoch:3, nombre batch:9801 loss:13.655 precision=36.961%\n",
      "epoch:3, nombre batch:9901 loss:14.006 precision=35.816%\n",
      "epoch:3, nombre batch:10001 loss:14.198 precision=35.655%\n",
      "epoch:3, nombre batch:10101 loss:14.067 precision=36.854%\n",
      "epoch:3, nombre batch:10201 loss:13.654 precision=37.701%\n",
      "epoch:3, nombre batch:10301 loss:14.008 precision=36.154%\n",
      "epoch:3, nombre batch:10401 loss:13.657 precision=38.251%\n",
      "epoch:3, nombre batch:10501 loss:13.785 precision=36.767%\n",
      "epoch:3, nombre batch:10601 loss:14.121 precision=35.238%\n",
      "epoch:3, nombre batch:10701 loss:14.003 precision=36.221%\n",
      "epoch:3, nombre batch:10801 loss:13.700 precision=36.242%\n",
      "epoch:3, nombre batch:10901 loss:13.867 precision=37.297%\n",
      "epoch:3, nombre batch:11001 loss:14.116 precision=34.846%\n",
      "epoch:3, nombre batch:11101 loss:13.869 precision=36.822%\n",
      "epoch:3, nombre batch:11201 loss:13.686 precision=37.476%\n",
      "epoch:3, nombre batch:11301 loss:14.026 precision=36.015%\n",
      "epoch:3, nombre batch:11401 loss:13.391 precision=38.572%\n",
      "epoch:3, nombre batch:11501 loss:13.858 precision=36.660%\n",
      "epoch:3, nombre batch:11601 loss:14.094 precision=35.599%\n",
      "epoch:3, nombre batch:11701 loss:14.025 precision=35.849%\n",
      "epoch:3, nombre batch:11801 loss:13.666 precision=37.105%\n",
      "epoch:3, nombre batch:11901 loss:13.714 precision=36.491%\n",
      "epoch:3, nombre batch:12001 loss:13.880 precision=37.223%\n",
      "epoch:3, nombre batch:12101 loss:13.804 precision=37.135%\n",
      "epoch:3, nombre batch:12201 loss:13.916 precision=36.169%\n",
      "epoch:3, nombre batch:12301 loss:13.881 precision=35.959%\n",
      "epoch:3, nombre batch:12401 loss:13.699 precision=37.717%\n",
      "epoch:3, nombre batch:12501 loss:13.988 precision=36.256%\n",
      "epoch:3, nombre batch:12601 loss:14.326 precision=34.667%\n",
      "epoch:3, nombre batch:12701 loss:13.774 precision=37.149%\n",
      "epoch:3, nombre batch:12801 loss:14.007 precision=35.565%\n",
      "epoch:3, nombre batch:12901 loss:13.556 precision=37.808%\n",
      "epoch:3, nombre batch:13001 loss:13.735 precision=37.467%\n",
      "epoch:3, nombre batch:13101 loss:13.608 precision=37.003%\n",
      "epoch:3, nombre batch:13201 loss:13.755 precision=36.522%\n",
      "epoch:3, nombre batch:13301 loss:13.915 precision=36.760%\n",
      "epoch:3, nombre batch:13401 loss:14.337 precision=35.447%\n",
      "epoch:3, nombre batch:13501 loss:13.811 precision=35.563%\n",
      "epoch:3, nombre batch:13601 loss:14.150 precision=35.577%\n",
      "epoch:3, nombre batch:13701 loss:14.161 precision=36.421%\n",
      "epoch:3, nombre batch:13801 loss:13.692 precision=36.538%\n",
      "epoch:3, nombre batch:13901 loss:13.766 precision=36.480%\n",
      "epoch:3, nombre batch:14001 loss:13.661 precision=36.482%\n",
      "epoch:3, nombre batch:14101 loss:13.821 precision=36.955%\n",
      "epoch:3, nombre batch:14201 loss:14.553 precision=33.358%\n",
      "epoch:3, nombre batch:14301 loss:14.130 precision=36.531%\n",
      "epoch:3, nombre batch:14401 loss:13.417 precision=37.433%\n",
      "epoch:3, nombre batch:14501 loss:14.060 precision=36.664%\n",
      "epoch:3, nombre batch:14601 loss:14.004 precision=35.958%\n",
      "epoch:3, nombre batch:14701 loss:14.542 precision=34.333%\n",
      "epoch:3, nombre batch:14801 loss:13.849 precision=36.195%\n",
      "epoch:3, nombre batch:14901 loss:14.218 precision=34.848%\n",
      "epoch:3, nombre batch:15001 loss:13.913 precision=35.741%\n",
      "epoch:3, nombre batch:15101 loss:13.885 precision=37.332%\n",
      "epoch:3, nombre batch:15201 loss:13.914 precision=36.194%\n",
      "epoch:3, nombre batch:15301 loss:13.539 precision=37.671%\n",
      "epoch:3, nombre batch:15401 loss:13.609 precision=38.209%\n",
      "epoch:3, nombre batch:15501 loss:13.533 precision=37.538%\n",
      "epoch:3, nombre batch:15601 loss:13.920 precision=37.822%\n",
      "epoch:3, nombre batch:15701 loss:14.235 precision=34.810%\n",
      "epoch:3, nombre batch:15801 loss:14.259 precision=34.906%\n",
      "epoch:3, nombre batch:15901 loss:14.194 precision=35.930%\n",
      "epoch:3, nombre batch:16001 loss:13.956 precision=36.921%\n",
      "epoch:3, nombre batch:16101 loss:13.720 precision=36.305%\n",
      "epoch:3, nombre batch:16201 loss:13.524 precision=37.325%\n",
      "epoch:3, nombre batch:16301 loss:14.052 precision=36.313%\n",
      "epoch:3, nombre batch:16401 loss:13.987 precision=36.825%\n",
      "epoch:3, nombre batch:16501 loss:14.132 precision=35.661%\n",
      "epoch:3, nombre batch:16601 loss:13.899 precision=36.049%\n",
      "epoch:3, nombre batch:16701 loss:14.096 precision=35.256%\n",
      "epoch:3, nombre batch:16801 loss:13.853 precision=36.797%\n",
      "epoch:3, nombre batch:16901 loss:14.031 precision=35.362%\n",
      "epoch:3, nombre batch:17001 loss:13.806 precision=36.792%\n",
      "epoch:3, nombre batch:17101 loss:13.971 precision=35.242%\n",
      "epoch:3, nombre batch:17201 loss:13.805 precision=36.106%\n",
      "epoch:3, nombre batch:17301 loss:13.948 precision=36.653%\n",
      "epoch:3, nombre batch:17401 loss:13.962 precision=36.222%\n",
      "epoch:3, nombre batch:17501 loss:13.590 precision=37.043%\n",
      "epoch:3, nombre batch:17601 loss:13.980 precision=35.941%\n",
      "epoch:3, nombre batch:17701 loss:13.756 precision=36.579%\n",
      "epoch:3, nombre batch:17801 loss:13.957 precision=35.886%\n",
      "epoch:3, nombre batch:17901 loss:13.861 precision=35.679%\n",
      "epoch:3, nombre batch:18001 loss:13.925 precision=36.224%\n",
      "epoch:3, nombre batch:18101 loss:13.700 precision=37.354%\n",
      "epoch:3, nombre batch:18201 loss:13.844 precision=36.816%\n",
      "epoch:3, nombre batch:18301 loss:13.726 precision=36.322%\n",
      "epoch:3, nombre batch:18401 loss:13.591 precision=38.301%\n",
      "epoch:3, nombre batch:18501 loss:13.837 precision=36.623%\n",
      "epoch:3, nombre batch:18601 loss:13.799 precision=36.892%\n",
      "epoch:3, nombre batch:18701 loss:14.183 precision=35.677%\n",
      "epoch:3, nombre batch:18801 loss:13.839 precision=36.828%\n",
      "epoch:3, nombre batch:18901 loss:13.950 precision=36.030%\n",
      "epoch:3, nombre batch:19001 loss:14.141 precision=35.633%\n",
      "epoch:3, nombre batch:19101 loss:14.360 precision=34.251%\n",
      "epoch:3, nombre batch:19201 loss:13.456 precision=37.921%\n",
      "epoch:3, nombre batch:19301 loss:14.243 precision=35.457%\n",
      "epoch:3, nombre batch:19401 loss:13.520 precision=37.037%\n",
      "epoch:3, nombre batch:19501 loss:13.889 precision=37.537%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, nombre batch:19601 loss:13.834 precision=36.309%\n",
      "epoch:3, nombre batch:19701 loss:13.752 precision=36.599%\n",
      "epoch:3, nombre batch:19801 loss:13.912 precision=35.968%\n",
      "epoch:3, nombre batch:19901 loss:13.856 precision=37.463%\n",
      "epoch:3, nombre batch:20001 loss:13.791 precision=37.080%\n",
      "epoch:3, nombre batch:20101 loss:13.718 precision=35.731%\n",
      "epoch:3, nombre batch:20201 loss:13.719 precision=37.765%\n",
      "epoch:3, nombre batch:20301 loss:14.333 precision=34.411%\n",
      "epoch:3, nombre batch:20401 loss:13.763 precision=37.224%\n",
      "epoch:3, nombre batch:20501 loss:13.633 precision=36.260%\n",
      "epoch:3, nombre batch:20601 loss:13.926 precision=37.272%\n",
      "epoch:3, nombre batch:20701 loss:13.219 precision=38.918%\n",
      "epoch:3, nombre batch:20801 loss:14.086 precision=36.187%\n",
      "epoch:3, nombre batch:20901 loss:13.978 precision=36.560%\n",
      "epoch:3, nombre batch:21001 loss:13.826 precision=37.001%\n",
      "epoch:3, nombre batch:21101 loss:13.949 precision=36.704%\n",
      "epoch:3, nombre batch:21201 loss:13.605 precision=37.137%\n",
      "epoch:3, nombre batch:21301 loss:14.076 precision=35.814%\n",
      "epoch:3, nombre batch:21401 loss:13.830 precision=37.658%\n",
      "epoch:3, nombre batch:21501 loss:14.061 precision=34.843%\n",
      "epoch:3, nombre batch:21601 loss:13.644 precision=36.398%\n",
      "epoch:3, nombre batch:21701 loss:13.874 precision=37.096%\n",
      "epoch:3, nombre batch:21801 loss:13.899 precision=37.211%\n",
      "epoch:3, nombre batch:21901 loss:13.721 precision=37.866%\n",
      "epoch:3, nombre batch:22001 loss:13.802 precision=37.788%\n",
      "epoch:3, nombre batch:22101 loss:14.134 precision=35.489%\n",
      "epoch:3, nombre batch:22201 loss:13.965 precision=35.054%\n",
      "epoch:3, nombre batch:22301 loss:14.240 precision=35.756%\n",
      "epoch:3, nombre batch:22401 loss:13.857 precision=37.154%\n",
      "epoch:3, nombre batch:22501 loss:13.848 precision=36.333%\n",
      "epoch:3, nombre batch:22601 loss:14.084 precision=35.973%\n",
      "epoch:3, nombre batch:22701 loss:14.114 precision=35.021%\n",
      "epoch:3, nombre batch:22801 loss:13.747 precision=37.769%\n",
      "epoch:3, nombre batch:22901 loss:13.602 precision=37.863%\n",
      "epoch:3, nombre batch:23001 loss:14.123 precision=35.682%\n",
      "Resultat Epoch 3;  loss:13.895, Accuracy=43.318%,  precision:36.466%, AMS=0.9786\n",
      "epoch:4, nombre batch:1 loss:0.131 precision=41.379%\n",
      "epoch:4, nombre batch:101 loss:13.750 precision=36.548%\n",
      "epoch:4, nombre batch:201 loss:13.993 precision=35.516%\n",
      "epoch:4, nombre batch:301 loss:13.982 precision=35.299%\n",
      "epoch:4, nombre batch:401 loss:13.921 precision=36.848%\n",
      "epoch:4, nombre batch:501 loss:13.894 precision=36.484%\n",
      "epoch:4, nombre batch:601 loss:14.216 precision=36.343%\n",
      "epoch:4, nombre batch:701 loss:13.851 precision=36.278%\n",
      "epoch:4, nombre batch:801 loss:13.842 precision=35.413%\n",
      "epoch:4, nombre batch:901 loss:13.889 precision=36.968%\n",
      "epoch:4, nombre batch:1001 loss:13.633 precision=38.054%\n",
      "epoch:4, nombre batch:1101 loss:13.999 precision=34.893%\n",
      "epoch:4, nombre batch:1201 loss:13.834 precision=35.888%\n",
      "epoch:4, nombre batch:1301 loss:13.983 precision=35.942%\n",
      "epoch:4, nombre batch:1401 loss:13.955 precision=35.133%\n",
      "epoch:4, nombre batch:1501 loss:14.148 precision=35.628%\n",
      "epoch:4, nombre batch:1601 loss:13.905 precision=37.286%\n",
      "epoch:4, nombre batch:1701 loss:13.566 precision=37.860%\n",
      "epoch:4, nombre batch:1801 loss:13.945 precision=36.680%\n",
      "epoch:4, nombre batch:1901 loss:13.902 precision=36.050%\n",
      "epoch:4, nombre batch:2001 loss:13.597 precision=37.873%\n",
      "epoch:4, nombre batch:2101 loss:13.829 precision=36.991%\n",
      "epoch:4, nombre batch:2201 loss:13.894 precision=35.332%\n",
      "epoch:4, nombre batch:2301 loss:14.209 precision=36.500%\n",
      "epoch:4, nombre batch:2401 loss:13.909 precision=36.474%\n",
      "epoch:4, nombre batch:2501 loss:13.679 precision=37.561%\n",
      "epoch:4, nombre batch:2601 loss:13.865 precision=36.350%\n",
      "epoch:4, nombre batch:2701 loss:13.988 precision=35.952%\n",
      "epoch:4, nombre batch:2801 loss:13.860 precision=36.477%\n",
      "epoch:4, nombre batch:2901 loss:13.631 precision=37.194%\n",
      "epoch:4, nombre batch:3001 loss:14.031 precision=35.198%\n",
      "epoch:4, nombre batch:3101 loss:13.898 precision=37.022%\n",
      "epoch:4, nombre batch:3201 loss:13.947 precision=36.537%\n",
      "epoch:4, nombre batch:3301 loss:13.550 precision=38.734%\n",
      "epoch:4, nombre batch:3401 loss:14.514 precision=34.483%\n",
      "epoch:4, nombre batch:3501 loss:14.136 precision=35.626%\n",
      "epoch:4, nombre batch:3601 loss:13.910 precision=38.198%\n",
      "epoch:4, nombre batch:3701 loss:14.168 precision=35.045%\n",
      "epoch:4, nombre batch:3801 loss:14.054 precision=35.644%\n",
      "epoch:4, nombre batch:3901 loss:13.871 precision=36.507%\n",
      "epoch:4, nombre batch:4001 loss:13.521 precision=38.285%\n",
      "epoch:4, nombre batch:4101 loss:13.653 precision=37.860%\n",
      "epoch:4, nombre batch:4201 loss:14.046 precision=35.002%\n",
      "epoch:4, nombre batch:4301 loss:13.841 precision=36.636%\n",
      "epoch:4, nombre batch:4401 loss:13.871 precision=36.903%\n",
      "epoch:4, nombre batch:4501 loss:14.217 precision=35.481%\n",
      "epoch:4, nombre batch:4601 loss:13.851 precision=37.173%\n",
      "epoch:4, nombre batch:4701 loss:13.256 precision=38.382%\n",
      "epoch:4, nombre batch:4801 loss:14.061 precision=35.279%\n",
      "epoch:4, nombre batch:4901 loss:13.610 precision=37.131%\n",
      "epoch:4, nombre batch:5001 loss:13.998 precision=36.528%\n",
      "epoch:4, nombre batch:5101 loss:14.303 precision=35.043%\n",
      "epoch:4, nombre batch:5201 loss:14.346 precision=34.517%\n",
      "epoch:4, nombre batch:5301 loss:14.259 precision=36.124%\n",
      "epoch:4, nombre batch:5401 loss:14.100 precision=35.985%\n",
      "epoch:4, nombre batch:5501 loss:14.015 precision=35.548%\n",
      "epoch:4, nombre batch:5601 loss:13.678 precision=37.293%\n",
      "epoch:4, nombre batch:5701 loss:13.586 precision=37.993%\n",
      "epoch:4, nombre batch:5801 loss:13.824 precision=36.056%\n",
      "epoch:4, nombre batch:5901 loss:14.011 precision=36.064%\n",
      "epoch:4, nombre batch:6001 loss:14.534 precision=35.398%\n",
      "epoch:4, nombre batch:6101 loss:13.758 precision=36.931%\n",
      "epoch:4, nombre batch:6201 loss:13.745 precision=37.127%\n",
      "epoch:4, nombre batch:6301 loss:13.899 precision=36.418%\n",
      "epoch:4, nombre batch:6401 loss:14.075 precision=36.059%\n",
      "epoch:4, nombre batch:6501 loss:13.885 precision=35.384%\n",
      "epoch:4, nombre batch:6601 loss:14.434 precision=35.511%\n",
      "epoch:4, nombre batch:6701 loss:13.803 precision=37.393%\n",
      "epoch:4, nombre batch:6801 loss:13.939 precision=36.830%\n",
      "epoch:4, nombre batch:6901 loss:13.854 precision=36.350%\n",
      "epoch:4, nombre batch:7001 loss:14.354 precision=34.776%\n",
      "epoch:4, nombre batch:7101 loss:14.002 precision=36.768%\n",
      "epoch:4, nombre batch:7201 loss:13.937 precision=36.907%\n",
      "epoch:4, nombre batch:7301 loss:13.999 precision=36.441%\n",
      "epoch:4, nombre batch:7401 loss:13.482 precision=37.717%\n",
      "epoch:4, nombre batch:7501 loss:14.271 precision=34.644%\n",
      "epoch:4, nombre batch:7601 loss:13.714 precision=37.791%\n",
      "epoch:4, nombre batch:7701 loss:13.956 precision=35.971%\n",
      "epoch:4, nombre batch:7801 loss:13.745 precision=37.743%\n",
      "epoch:4, nombre batch:7901 loss:13.882 precision=36.755%\n",
      "epoch:4, nombre batch:8001 loss:13.838 precision=37.767%\n",
      "epoch:4, nombre batch:8101 loss:14.060 precision=36.680%\n",
      "epoch:4, nombre batch:8201 loss:14.096 precision=35.693%\n",
      "epoch:4, nombre batch:8301 loss:13.670 precision=37.604%\n",
      "epoch:4, nombre batch:8401 loss:13.537 precision=38.110%\n",
      "epoch:4, nombre batch:8501 loss:13.644 precision=36.986%\n",
      "epoch:4, nombre batch:8601 loss:14.032 precision=36.907%\n",
      "epoch:4, nombre batch:8701 loss:13.546 precision=38.332%\n",
      "epoch:4, nombre batch:8801 loss:13.915 precision=36.822%\n",
      "epoch:4, nombre batch:8901 loss:13.817 precision=36.698%\n",
      "epoch:4, nombre batch:9001 loss:14.059 precision=36.387%\n",
      "epoch:4, nombre batch:9101 loss:13.892 precision=36.258%\n",
      "epoch:4, nombre batch:9201 loss:13.924 precision=36.680%\n",
      "epoch:4, nombre batch:9301 loss:13.784 precision=36.945%\n",
      "epoch:4, nombre batch:9401 loss:13.452 precision=38.045%\n",
      "epoch:4, nombre batch:9501 loss:14.062 precision=34.669%\n",
      "epoch:4, nombre batch:9601 loss:13.745 precision=36.586%\n",
      "epoch:4, nombre batch:9701 loss:13.888 precision=37.073%\n",
      "epoch:4, nombre batch:9801 loss:13.294 precision=39.488%\n",
      "epoch:4, nombre batch:9901 loss:14.074 precision=35.490%\n",
      "epoch:4, nombre batch:10001 loss:13.403 precision=37.356%\n",
      "epoch:4, nombre batch:10101 loss:13.688 precision=36.045%\n",
      "epoch:4, nombre batch:10201 loss:13.971 precision=36.614%\n",
      "epoch:4, nombre batch:10301 loss:14.167 precision=35.558%\n",
      "epoch:4, nombre batch:10401 loss:14.105 precision=36.836%\n",
      "epoch:4, nombre batch:10501 loss:14.157 precision=35.632%\n",
      "epoch:4, nombre batch:10601 loss:14.213 precision=34.731%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, nombre batch:10701 loss:14.192 precision=34.733%\n",
      "epoch:4, nombre batch:10801 loss:13.914 precision=37.005%\n",
      "epoch:4, nombre batch:10901 loss:14.270 precision=35.408%\n",
      "epoch:4, nombre batch:11001 loss:14.155 precision=36.164%\n",
      "epoch:4, nombre batch:11101 loss:14.147 precision=35.671%\n",
      "epoch:4, nombre batch:11201 loss:13.824 precision=36.870%\n",
      "epoch:4, nombre batch:11301 loss:14.085 precision=34.833%\n",
      "epoch:4, nombre batch:11401 loss:13.457 precision=39.025%\n",
      "epoch:4, nombre batch:11501 loss:13.955 precision=37.236%\n",
      "epoch:4, nombre batch:11601 loss:13.883 precision=36.309%\n",
      "epoch:4, nombre batch:11701 loss:13.751 precision=36.979%\n",
      "epoch:4, nombre batch:11801 loss:13.661 precision=37.250%\n",
      "epoch:4, nombre batch:11901 loss:13.701 precision=36.654%\n",
      "epoch:4, nombre batch:12001 loss:14.473 precision=34.987%\n",
      "epoch:4, nombre batch:12101 loss:14.227 precision=35.563%\n",
      "epoch:4, nombre batch:12201 loss:13.932 precision=36.084%\n",
      "epoch:4, nombre batch:12301 loss:14.016 precision=35.307%\n",
      "epoch:4, nombre batch:12401 loss:13.996 precision=36.156%\n",
      "epoch:4, nombre batch:12501 loss:13.990 precision=36.414%\n",
      "epoch:4, nombre batch:12601 loss:13.580 precision=37.400%\n",
      "epoch:4, nombre batch:12701 loss:13.740 precision=36.540%\n",
      "epoch:4, nombre batch:12801 loss:13.307 precision=37.769%\n",
      "epoch:4, nombre batch:12901 loss:13.870 precision=36.716%\n",
      "epoch:4, nombre batch:13001 loss:13.813 precision=36.667%\n",
      "epoch:4, nombre batch:13101 loss:13.803 precision=37.076%\n",
      "epoch:4, nombre batch:13201 loss:13.526 precision=37.913%\n",
      "epoch:4, nombre batch:13301 loss:14.045 precision=34.620%\n",
      "epoch:4, nombre batch:13401 loss:13.883 precision=36.452%\n",
      "epoch:4, nombre batch:13501 loss:13.864 precision=36.453%\n",
      "epoch:4, nombre batch:13601 loss:14.096 precision=35.316%\n",
      "epoch:4, nombre batch:13701 loss:14.232 precision=34.695%\n",
      "epoch:4, nombre batch:13801 loss:14.241 precision=36.594%\n",
      "epoch:4, nombre batch:13901 loss:13.886 precision=35.584%\n",
      "epoch:4, nombre batch:14001 loss:14.289 precision=34.640%\n",
      "epoch:4, nombre batch:14101 loss:13.751 precision=37.315%\n",
      "epoch:4, nombre batch:14201 loss:14.021 precision=36.217%\n",
      "epoch:4, nombre batch:14301 loss:13.683 precision=36.114%\n",
      "epoch:4, nombre batch:14401 loss:13.915 precision=36.223%\n",
      "epoch:4, nombre batch:14501 loss:13.543 precision=37.978%\n",
      "epoch:4, nombre batch:14601 loss:14.182 precision=36.078%\n",
      "epoch:4, nombre batch:14701 loss:13.769 precision=37.528%\n",
      "epoch:4, nombre batch:14801 loss:13.917 precision=37.001%\n",
      "epoch:4, nombre batch:14901 loss:13.945 precision=35.752%\n",
      "epoch:4, nombre batch:15001 loss:13.829 precision=36.253%\n",
      "epoch:4, nombre batch:15101 loss:13.851 precision=36.637%\n",
      "epoch:4, nombre batch:15201 loss:13.909 precision=36.438%\n",
      "epoch:4, nombre batch:15301 loss:13.888 precision=36.278%\n",
      "epoch:4, nombre batch:15401 loss:14.047 precision=35.057%\n",
      "epoch:4, nombre batch:15501 loss:13.753 precision=37.280%\n",
      "epoch:4, nombre batch:15601 loss:13.937 precision=36.852%\n",
      "epoch:4, nombre batch:15701 loss:14.139 precision=35.036%\n",
      "epoch:4, nombre batch:15801 loss:13.622 precision=36.840%\n",
      "epoch:4, nombre batch:15901 loss:14.025 precision=35.133%\n",
      "epoch:4, nombre batch:16001 loss:13.569 precision=36.995%\n",
      "epoch:4, nombre batch:16101 loss:13.622 precision=37.767%\n",
      "epoch:4, nombre batch:16201 loss:14.002 precision=36.041%\n",
      "epoch:4, nombre batch:16301 loss:14.261 precision=34.890%\n",
      "epoch:4, nombre batch:16401 loss:13.883 precision=37.141%\n",
      "epoch:4, nombre batch:16501 loss:14.028 precision=35.337%\n",
      "epoch:4, nombre batch:16601 loss:13.883 precision=36.791%\n",
      "epoch:4, nombre batch:16701 loss:14.047 precision=36.320%\n",
      "epoch:4, nombre batch:16801 loss:13.689 precision=37.150%\n",
      "epoch:4, nombre batch:16901 loss:14.055 precision=36.683%\n",
      "epoch:4, nombre batch:17001 loss:13.798 precision=36.038%\n",
      "epoch:4, nombre batch:17101 loss:13.664 precision=36.814%\n",
      "epoch:4, nombre batch:17201 loss:13.898 precision=37.310%\n",
      "epoch:4, nombre batch:17301 loss:14.248 precision=35.400%\n",
      "epoch:4, nombre batch:17401 loss:13.998 precision=36.584%\n",
      "epoch:4, nombre batch:17501 loss:13.952 precision=36.261%\n",
      "epoch:4, nombre batch:17601 loss:13.821 precision=36.640%\n",
      "epoch:4, nombre batch:17701 loss:13.766 precision=36.899%\n",
      "epoch:4, nombre batch:17801 loss:14.453 precision=34.945%\n",
      "epoch:4, nombre batch:17901 loss:14.269 precision=35.321%\n",
      "epoch:4, nombre batch:18001 loss:13.802 precision=35.977%\n",
      "epoch:4, nombre batch:18101 loss:14.059 precision=36.492%\n",
      "epoch:4, nombre batch:18201 loss:14.030 precision=36.469%\n",
      "epoch:4, nombre batch:18301 loss:13.559 precision=38.072%\n",
      "epoch:4, nombre batch:18401 loss:14.115 precision=35.712%\n",
      "epoch:4, nombre batch:18501 loss:14.056 precision=37.132%\n",
      "epoch:4, nombre batch:18601 loss:13.838 precision=36.064%\n",
      "epoch:4, nombre batch:18701 loss:13.844 precision=37.645%\n",
      "epoch:4, nombre batch:18801 loss:13.504 precision=36.881%\n",
      "epoch:4, nombre batch:18901 loss:13.952 precision=36.309%\n",
      "epoch:4, nombre batch:19001 loss:13.817 precision=36.667%\n",
      "epoch:4, nombre batch:19101 loss:13.673 precision=37.780%\n",
      "epoch:4, nombre batch:19201 loss:14.184 precision=34.936%\n",
      "epoch:4, nombre batch:19301 loss:13.672 precision=37.975%\n",
      "epoch:4, nombre batch:19401 loss:13.588 precision=38.310%\n",
      "epoch:4, nombre batch:19501 loss:13.916 precision=36.801%\n",
      "epoch:4, nombre batch:19601 loss:14.186 precision=35.475%\n",
      "epoch:4, nombre batch:19701 loss:13.085 precision=39.645%\n",
      "epoch:4, nombre batch:19801 loss:13.815 precision=36.939%\n",
      "epoch:4, nombre batch:19901 loss:13.959 precision=36.988%\n",
      "epoch:4, nombre batch:20001 loss:13.548 precision=37.296%\n",
      "epoch:4, nombre batch:20101 loss:13.635 precision=37.227%\n",
      "epoch:4, nombre batch:20201 loss:13.248 precision=38.690%\n",
      "epoch:4, nombre batch:20301 loss:13.635 precision=37.145%\n",
      "epoch:4, nombre batch:20401 loss:14.175 precision=36.620%\n",
      "epoch:4, nombre batch:20501 loss:14.145 precision=35.952%\n",
      "epoch:4, nombre batch:20601 loss:13.782 precision=36.673%\n",
      "epoch:4, nombre batch:20701 loss:13.958 precision=37.260%\n",
      "epoch:4, nombre batch:20801 loss:13.667 precision=37.537%\n",
      "epoch:4, nombre batch:20901 loss:14.154 precision=36.091%\n",
      "epoch:4, nombre batch:21001 loss:14.201 precision=35.201%\n",
      "epoch:4, nombre batch:21101 loss:14.133 precision=36.963%\n",
      "epoch:4, nombre batch:21201 loss:13.948 precision=36.110%\n",
      "epoch:4, nombre batch:21301 loss:13.960 precision=35.135%\n",
      "epoch:4, nombre batch:21401 loss:13.306 precision=38.450%\n",
      "epoch:4, nombre batch:21501 loss:13.933 precision=36.319%\n",
      "epoch:4, nombre batch:21601 loss:13.695 precision=36.238%\n",
      "epoch:4, nombre batch:21701 loss:13.970 precision=35.075%\n",
      "epoch:4, nombre batch:21801 loss:13.586 precision=37.102%\n",
      "epoch:4, nombre batch:21901 loss:13.923 precision=36.768%\n",
      "epoch:4, nombre batch:22001 loss:13.437 precision=38.616%\n",
      "epoch:4, nombre batch:22101 loss:13.890 precision=35.393%\n",
      "epoch:4, nombre batch:22201 loss:13.943 precision=35.382%\n",
      "epoch:4, nombre batch:22301 loss:13.836 precision=36.838%\n",
      "epoch:4, nombre batch:22401 loss:13.990 precision=36.032%\n",
      "epoch:4, nombre batch:22501 loss:13.912 precision=35.968%\n",
      "epoch:4, nombre batch:22601 loss:14.036 precision=36.024%\n",
      "epoch:4, nombre batch:22701 loss:13.785 precision=37.096%\n",
      "epoch:4, nombre batch:22801 loss:13.563 precision=38.633%\n",
      "epoch:4, nombre batch:22901 loss:13.550 precision=37.744%\n",
      "epoch:4, nombre batch:23001 loss:13.778 precision=36.422%\n",
      "Resultat Epoch 4;  loss:13.893, Accuracy=43.404%,  precision:36.515%, AMS=0.9969\n",
      "epoch:5, nombre batch:1 loss:0.140 precision=30.769%\n",
      "epoch:5, nombre batch:101 loss:14.308 precision=36.118%\n",
      "epoch:5, nombre batch:201 loss:13.661 precision=38.222%\n",
      "epoch:5, nombre batch:301 loss:13.767 precision=36.546%\n",
      "epoch:5, nombre batch:401 loss:14.195 precision=35.934%\n",
      "epoch:5, nombre batch:501 loss:14.046 precision=36.704%\n",
      "epoch:5, nombre batch:601 loss:13.951 precision=36.535%\n",
      "epoch:5, nombre batch:701 loss:14.284 precision=35.385%\n",
      "epoch:5, nombre batch:801 loss:13.842 precision=37.175%\n",
      "epoch:5, nombre batch:901 loss:13.706 precision=36.684%\n",
      "epoch:5, nombre batch:1001 loss:13.851 precision=35.801%\n",
      "epoch:5, nombre batch:1101 loss:13.782 precision=37.216%\n",
      "epoch:5, nombre batch:1201 loss:13.710 precision=36.461%\n",
      "epoch:5, nombre batch:1301 loss:13.443 precision=38.063%\n",
      "epoch:5, nombre batch:1401 loss:13.721 precision=36.775%\n",
      "epoch:5, nombre batch:1501 loss:14.215 precision=36.627%\n",
      "epoch:5, nombre batch:1601 loss:13.921 precision=36.295%\n",
      "epoch:5, nombre batch:1701 loss:13.849 precision=36.082%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, nombre batch:1801 loss:13.928 precision=37.090%\n",
      "epoch:5, nombre batch:1901 loss:14.243 precision=35.914%\n",
      "epoch:5, nombre batch:2001 loss:13.747 precision=37.217%\n",
      "epoch:5, nombre batch:2101 loss:13.827 precision=36.630%\n",
      "epoch:5, nombre batch:2201 loss:14.121 precision=36.015%\n",
      "epoch:5, nombre batch:2301 loss:13.641 precision=37.640%\n",
      "epoch:5, nombre batch:2401 loss:14.138 precision=35.503%\n",
      "epoch:5, nombre batch:2501 loss:14.026 precision=35.965%\n",
      "epoch:5, nombre batch:2601 loss:14.087 precision=35.792%\n",
      "epoch:5, nombre batch:2701 loss:14.290 precision=35.373%\n",
      "epoch:5, nombre batch:2801 loss:13.610 precision=37.034%\n",
      "epoch:5, nombre batch:2901 loss:14.016 precision=36.249%\n",
      "epoch:5, nombre batch:3001 loss:13.687 precision=36.792%\n",
      "epoch:5, nombre batch:3101 loss:14.056 precision=36.022%\n",
      "epoch:5, nombre batch:3201 loss:13.642 precision=37.401%\n",
      "epoch:5, nombre batch:3301 loss:13.602 precision=38.017%\n",
      "epoch:5, nombre batch:3401 loss:13.612 precision=36.471%\n",
      "epoch:5, nombre batch:3501 loss:13.608 precision=38.065%\n",
      "epoch:5, nombre batch:3601 loss:13.746 precision=37.435%\n",
      "epoch:5, nombre batch:3701 loss:13.680 precision=37.250%\n",
      "epoch:5, nombre batch:3801 loss:13.805 precision=36.771%\n",
      "epoch:5, nombre batch:3901 loss:13.867 precision=36.429%\n",
      "epoch:5, nombre batch:4001 loss:14.031 precision=35.774%\n",
      "epoch:5, nombre batch:4101 loss:13.614 precision=37.100%\n",
      "epoch:5, nombre batch:4201 loss:13.293 precision=39.174%\n",
      "epoch:5, nombre batch:4301 loss:13.864 precision=36.360%\n",
      "epoch:5, nombre batch:4401 loss:14.388 precision=35.022%\n",
      "epoch:5, nombre batch:4501 loss:13.147 precision=37.270%\n",
      "epoch:5, nombre batch:4601 loss:14.342 precision=35.320%\n",
      "epoch:5, nombre batch:4701 loss:13.916 precision=36.181%\n",
      "epoch:5, nombre batch:4801 loss:14.193 precision=35.914%\n",
      "epoch:5, nombre batch:4901 loss:14.064 precision=35.180%\n",
      "epoch:5, nombre batch:5001 loss:14.041 precision=35.784%\n",
      "epoch:5, nombre batch:5101 loss:13.830 precision=36.850%\n",
      "epoch:5, nombre batch:5201 loss:13.674 precision=36.929%\n",
      "epoch:5, nombre batch:5301 loss:13.881 precision=35.674%\n",
      "epoch:5, nombre batch:5401 loss:13.668 precision=37.142%\n",
      "epoch:5, nombre batch:5501 loss:13.999 precision=35.440%\n",
      "epoch:5, nombre batch:5601 loss:13.535 precision=37.813%\n",
      "epoch:5, nombre batch:5701 loss:14.342 precision=35.322%\n",
      "epoch:5, nombre batch:5801 loss:14.123 precision=34.939%\n",
      "epoch:5, nombre batch:5901 loss:13.640 precision=37.311%\n",
      "epoch:5, nombre batch:6001 loss:13.734 precision=37.100%\n",
      "epoch:5, nombre batch:6101 loss:13.930 precision=37.000%\n",
      "epoch:5, nombre batch:6201 loss:14.300 precision=34.887%\n",
      "epoch:5, nombre batch:6301 loss:13.709 precision=37.086%\n",
      "epoch:5, nombre batch:6401 loss:13.478 precision=37.391%\n",
      "epoch:5, nombre batch:6501 loss:13.863 precision=36.751%\n",
      "epoch:5, nombre batch:6601 loss:13.862 precision=36.343%\n",
      "epoch:5, nombre batch:6701 loss:13.409 precision=38.198%\n",
      "epoch:5, nombre batch:6801 loss:14.022 precision=36.014%\n",
      "epoch:5, nombre batch:6901 loss:14.089 precision=35.236%\n",
      "epoch:5, nombre batch:7001 loss:13.841 precision=37.123%\n",
      "epoch:5, nombre batch:7101 loss:14.252 precision=35.910%\n",
      "epoch:5, nombre batch:7201 loss:13.769 precision=36.752%\n",
      "epoch:5, nombre batch:7301 loss:13.657 precision=37.524%\n",
      "epoch:5, nombre batch:7401 loss:14.038 precision=35.792%\n",
      "epoch:5, nombre batch:7501 loss:13.766 precision=36.243%\n",
      "epoch:5, nombre batch:7601 loss:14.146 precision=36.258%\n",
      "epoch:5, nombre batch:7701 loss:13.740 precision=37.734%\n",
      "epoch:5, nombre batch:7801 loss:14.168 precision=35.287%\n",
      "epoch:5, nombre batch:7901 loss:13.775 precision=36.870%\n",
      "epoch:5, nombre batch:8001 loss:13.911 precision=35.847%\n",
      "epoch:5, nombre batch:8101 loss:13.796 precision=37.048%\n",
      "epoch:5, nombre batch:8201 loss:14.294 precision=34.878%\n",
      "epoch:5, nombre batch:8301 loss:13.839 precision=36.906%\n",
      "epoch:5, nombre batch:8401 loss:13.676 precision=36.910%\n",
      "epoch:5, nombre batch:8501 loss:14.260 precision=36.207%\n",
      "epoch:5, nombre batch:8601 loss:13.877 precision=36.360%\n",
      "epoch:5, nombre batch:8701 loss:14.008 precision=36.751%\n",
      "epoch:5, nombre batch:8801 loss:13.756 precision=37.519%\n",
      "epoch:5, nombre batch:8901 loss:13.869 precision=36.929%\n",
      "epoch:5, nombre batch:9001 loss:13.749 precision=38.132%\n",
      "epoch:5, nombre batch:9101 loss:14.171 precision=36.842%\n",
      "epoch:5, nombre batch:9201 loss:13.988 precision=36.029%\n",
      "epoch:5, nombre batch:9301 loss:13.774 precision=37.153%\n",
      "epoch:5, nombre batch:9401 loss:13.932 precision=35.624%\n",
      "epoch:5, nombre batch:9501 loss:13.971 precision=36.096%\n",
      "epoch:5, nombre batch:9601 loss:13.992 precision=35.666%\n",
      "epoch:5, nombre batch:9701 loss:13.779 precision=36.284%\n",
      "epoch:5, nombre batch:9801 loss:13.586 precision=37.254%\n",
      "epoch:5, nombre batch:9901 loss:13.769 precision=36.798%\n",
      "epoch:5, nombre batch:10001 loss:13.912 precision=36.299%\n",
      "epoch:5, nombre batch:10101 loss:13.868 precision=36.487%\n",
      "epoch:5, nombre batch:10201 loss:14.425 precision=35.674%\n",
      "epoch:5, nombre batch:10301 loss:14.356 precision=34.597%\n",
      "epoch:5, nombre batch:10401 loss:14.111 precision=35.704%\n",
      "epoch:5, nombre batch:10501 loss:14.177 precision=35.237%\n",
      "epoch:5, nombre batch:10601 loss:13.696 precision=37.491%\n",
      "epoch:5, nombre batch:10701 loss:13.888 precision=36.006%\n",
      "epoch:5, nombre batch:10801 loss:13.835 precision=36.822%\n",
      "epoch:5, nombre batch:10901 loss:13.941 precision=36.773%\n",
      "epoch:5, nombre batch:11001 loss:13.583 precision=37.130%\n",
      "epoch:5, nombre batch:11101 loss:13.648 precision=37.922%\n",
      "epoch:5, nombre batch:11201 loss:13.746 precision=36.962%\n",
      "epoch:5, nombre batch:11301 loss:13.349 precision=37.800%\n",
      "epoch:5, nombre batch:11401 loss:13.897 precision=36.140%\n",
      "epoch:5, nombre batch:11501 loss:13.985 precision=35.806%\n",
      "epoch:5, nombre batch:11601 loss:13.790 precision=37.812%\n",
      "epoch:5, nombre batch:11701 loss:13.700 precision=37.837%\n",
      "epoch:5, nombre batch:11801 loss:13.384 precision=37.952%\n",
      "epoch:5, nombre batch:11901 loss:14.051 precision=34.991%\n",
      "epoch:5, nombre batch:12001 loss:14.273 precision=35.547%\n",
      "epoch:5, nombre batch:12101 loss:13.486 precision=37.495%\n",
      "epoch:5, nombre batch:12201 loss:13.283 precision=38.506%\n",
      "epoch:5, nombre batch:12301 loss:14.595 precision=35.151%\n",
      "epoch:5, nombre batch:12401 loss:13.967 precision=36.587%\n",
      "epoch:5, nombre batch:12501 loss:14.098 precision=35.452%\n",
      "epoch:5, nombre batch:12601 loss:13.563 precision=37.618%\n",
      "epoch:5, nombre batch:12701 loss:14.249 precision=35.888%\n",
      "epoch:5, nombre batch:12801 loss:13.749 precision=37.350%\n",
      "epoch:5, nombre batch:12901 loss:13.931 precision=35.771%\n",
      "epoch:5, nombre batch:13001 loss:13.613 precision=37.733%\n",
      "epoch:5, nombre batch:13101 loss:14.205 precision=35.185%\n",
      "epoch:5, nombre batch:13201 loss:14.081 precision=36.945%\n",
      "epoch:5, nombre batch:13301 loss:14.119 precision=35.593%\n",
      "epoch:5, nombre batch:13401 loss:13.601 precision=37.196%\n",
      "epoch:5, nombre batch:13501 loss:14.112 precision=36.377%\n",
      "epoch:5, nombre batch:13601 loss:14.033 precision=35.806%\n",
      "epoch:5, nombre batch:13701 loss:13.972 precision=36.528%\n",
      "epoch:5, nombre batch:13801 loss:14.077 precision=36.629%\n",
      "epoch:5, nombre batch:13901 loss:13.399 precision=38.429%\n",
      "epoch:5, nombre batch:14001 loss:14.147 precision=36.434%\n",
      "epoch:5, nombre batch:14101 loss:13.964 precision=36.452%\n",
      "epoch:5, nombre batch:14201 loss:13.618 precision=37.151%\n",
      "epoch:5, nombre batch:14301 loss:14.198 precision=34.243%\n",
      "epoch:5, nombre batch:14401 loss:13.961 precision=35.241%\n",
      "epoch:5, nombre batch:14501 loss:13.597 precision=37.486%\n",
      "epoch:5, nombre batch:14601 loss:14.272 precision=35.413%\n",
      "epoch:5, nombre batch:14701 loss:13.700 precision=37.232%\n",
      "epoch:5, nombre batch:14801 loss:13.965 precision=36.116%\n",
      "epoch:5, nombre batch:14901 loss:13.783 precision=36.994%\n",
      "epoch:5, nombre batch:15001 loss:13.903 precision=36.312%\n",
      "epoch:5, nombre batch:15101 loss:14.304 precision=36.112%\n",
      "epoch:5, nombre batch:15201 loss:13.523 precision=37.628%\n",
      "epoch:5, nombre batch:15301 loss:14.000 precision=36.448%\n",
      "epoch:5, nombre batch:15401 loss:13.863 precision=36.763%\n",
      "epoch:5, nombre batch:15501 loss:13.637 precision=37.324%\n",
      "epoch:5, nombre batch:15601 loss:13.650 precision=37.957%\n",
      "epoch:5, nombre batch:15701 loss:14.020 precision=35.923%\n",
      "epoch:5, nombre batch:15801 loss:13.946 precision=36.617%\n",
      "epoch:5, nombre batch:15901 loss:13.695 precision=36.589%\n",
      "epoch:5, nombre batch:16001 loss:13.723 precision=37.320%\n",
      "epoch:5, nombre batch:16101 loss:14.089 precision=36.701%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, nombre batch:16201 loss:13.785 precision=36.776%\n",
      "epoch:5, nombre batch:16301 loss:13.937 precision=36.333%\n",
      "epoch:5, nombre batch:16401 loss:13.965 precision=35.576%\n",
      "epoch:5, nombre batch:16501 loss:13.741 precision=36.995%\n",
      "epoch:5, nombre batch:16601 loss:13.389 precision=38.349%\n",
      "epoch:5, nombre batch:16701 loss:13.579 precision=37.307%\n",
      "epoch:5, nombre batch:16801 loss:13.983 precision=37.259%\n",
      "epoch:5, nombre batch:16901 loss:13.848 precision=36.449%\n",
      "epoch:5, nombre batch:17001 loss:14.007 precision=36.156%\n",
      "epoch:5, nombre batch:17101 loss:13.845 precision=36.439%\n",
      "epoch:5, nombre batch:17201 loss:13.825 precision=36.878%\n",
      "epoch:5, nombre batch:17301 loss:13.537 precision=38.553%\n",
      "epoch:5, nombre batch:17401 loss:14.016 precision=37.201%\n",
      "epoch:5, nombre batch:17501 loss:13.800 precision=36.401%\n",
      "epoch:5, nombre batch:17601 loss:14.021 precision=35.953%\n",
      "epoch:5, nombre batch:17701 loss:13.789 precision=35.972%\n",
      "epoch:5, nombre batch:17801 loss:13.944 precision=36.014%\n",
      "epoch:5, nombre batch:17901 loss:13.880 precision=36.901%\n",
      "epoch:5, nombre batch:18001 loss:14.276 precision=36.051%\n",
      "epoch:5, nombre batch:18101 loss:13.640 precision=38.051%\n",
      "epoch:5, nombre batch:18201 loss:13.707 precision=36.780%\n",
      "epoch:5, nombre batch:18301 loss:13.671 precision=37.486%\n",
      "epoch:5, nombre batch:18401 loss:13.991 precision=35.654%\n",
      "epoch:5, nombre batch:18501 loss:13.987 precision=34.799%\n",
      "epoch:5, nombre batch:18601 loss:14.009 precision=36.854%\n",
      "epoch:5, nombre batch:18701 loss:13.884 precision=35.627%\n",
      "epoch:5, nombre batch:18801 loss:13.590 precision=37.894%\n",
      "epoch:5, nombre batch:18901 loss:13.928 precision=36.793%\n",
      "epoch:5, nombre batch:19001 loss:13.888 precision=35.864%\n",
      "epoch:5, nombre batch:19101 loss:13.853 precision=37.258%\n",
      "epoch:5, nombre batch:19201 loss:13.857 precision=36.734%\n",
      "epoch:5, nombre batch:19301 loss:14.220 precision=36.067%\n",
      "epoch:5, nombre batch:19401 loss:13.985 precision=36.860%\n",
      "epoch:5, nombre batch:19501 loss:13.638 precision=37.552%\n",
      "epoch:5, nombre batch:19601 loss:13.551 precision=38.256%\n",
      "epoch:5, nombre batch:19701 loss:14.551 precision=34.372%\n",
      "epoch:5, nombre batch:19801 loss:14.139 precision=36.252%\n",
      "epoch:5, nombre batch:19901 loss:13.771 precision=37.472%\n",
      "epoch:5, nombre batch:20001 loss:13.758 precision=36.584%\n",
      "epoch:5, nombre batch:20101 loss:13.956 precision=37.183%\n",
      "epoch:5, nombre batch:20201 loss:14.199 precision=35.229%\n",
      "epoch:5, nombre batch:20301 loss:13.788 precision=37.223%\n",
      "epoch:5, nombre batch:20401 loss:14.288 precision=35.158%\n",
      "epoch:5, nombre batch:20501 loss:13.793 precision=37.091%\n",
      "epoch:5, nombre batch:20601 loss:13.950 precision=35.865%\n",
      "epoch:5, nombre batch:20701 loss:13.683 precision=37.363%\n",
      "epoch:5, nombre batch:20801 loss:13.673 precision=37.323%\n",
      "epoch:5, nombre batch:20901 loss:14.287 precision=35.160%\n",
      "epoch:5, nombre batch:21001 loss:14.088 precision=35.901%\n",
      "epoch:5, nombre batch:21101 loss:14.110 precision=34.843%\n",
      "epoch:5, nombre batch:21201 loss:13.587 precision=37.263%\n",
      "epoch:5, nombre batch:21301 loss:13.996 precision=35.608%\n",
      "epoch:5, nombre batch:21401 loss:13.785 precision=38.045%\n",
      "epoch:5, nombre batch:21501 loss:13.942 precision=36.439%\n",
      "epoch:5, nombre batch:21601 loss:14.024 precision=36.788%\n",
      "epoch:5, nombre batch:21701 loss:13.888 precision=37.173%\n",
      "epoch:5, nombre batch:21801 loss:13.989 precision=36.531%\n",
      "epoch:5, nombre batch:21901 loss:14.085 precision=36.885%\n",
      "epoch:5, nombre batch:22001 loss:14.049 precision=35.712%\n",
      "epoch:5, nombre batch:22101 loss:14.095 precision=36.245%\n",
      "epoch:5, nombre batch:22201 loss:13.914 precision=36.302%\n",
      "epoch:5, nombre batch:22301 loss:13.857 precision=36.315%\n",
      "epoch:5, nombre batch:22401 loss:14.079 precision=36.533%\n",
      "epoch:5, nombre batch:22501 loss:14.067 precision=35.733%\n",
      "epoch:5, nombre batch:22601 loss:13.845 precision=36.566%\n",
      "epoch:5, nombre batch:22701 loss:14.029 precision=35.227%\n",
      "epoch:5, nombre batch:22801 loss:13.616 precision=37.326%\n",
      "epoch:5, nombre batch:22901 loss:13.701 precision=37.645%\n",
      "epoch:5, nombre batch:23001 loss:14.400 precision=34.275%\n",
      "Resultat Epoch 5;  loss:13.893, Accuracy=43.475%,  precision:36.558%, AMS=0.9698\n",
      "epoch:6, nombre batch:1 loss:0.157 precision=25.000%\n",
      "epoch:6, nombre batch:101 loss:13.529 precision=37.414%\n",
      "epoch:6, nombre batch:201 loss:13.823 precision=37.725%\n",
      "epoch:6, nombre batch:301 loss:13.950 precision=35.825%\n",
      "epoch:6, nombre batch:401 loss:13.871 precision=36.085%\n",
      "epoch:6, nombre batch:501 loss:13.917 precision=35.874%\n",
      "epoch:6, nombre batch:601 loss:13.884 precision=36.075%\n",
      "epoch:6, nombre batch:701 loss:13.967 precision=36.306%\n",
      "epoch:6, nombre batch:801 loss:13.552 precision=36.866%\n",
      "epoch:6, nombre batch:901 loss:14.019 precision=36.367%\n",
      "epoch:6, nombre batch:1001 loss:13.771 precision=38.183%\n",
      "epoch:6, nombre batch:1101 loss:13.791 precision=37.214%\n",
      "epoch:6, nombre batch:1201 loss:13.775 precision=37.080%\n",
      "epoch:6, nombre batch:1301 loss:13.986 precision=36.684%\n",
      "epoch:6, nombre batch:1401 loss:13.972 precision=34.710%\n",
      "epoch:6, nombre batch:1501 loss:13.997 precision=36.889%\n",
      "epoch:6, nombre batch:1601 loss:13.671 precision=37.556%\n",
      "epoch:6, nombre batch:1701 loss:13.901 precision=36.568%\n",
      "epoch:6, nombre batch:1801 loss:13.822 precision=37.495%\n",
      "epoch:6, nombre batch:1901 loss:13.568 precision=37.376%\n",
      "epoch:6, nombre batch:2001 loss:14.133 precision=36.809%\n",
      "epoch:6, nombre batch:2101 loss:14.167 precision=35.939%\n",
      "epoch:6, nombre batch:2201 loss:14.069 precision=36.081%\n",
      "epoch:6, nombre batch:2301 loss:14.002 precision=35.733%\n",
      "epoch:6, nombre batch:2401 loss:13.659 precision=38.464%\n",
      "epoch:6, nombre batch:2501 loss:13.963 precision=36.510%\n",
      "epoch:6, nombre batch:2601 loss:13.793 precision=37.463%\n",
      "epoch:6, nombre batch:2701 loss:13.933 precision=36.667%\n",
      "epoch:6, nombre batch:2801 loss:13.852 precision=37.026%\n",
      "epoch:6, nombre batch:2901 loss:13.515 precision=37.684%\n",
      "epoch:6, nombre batch:3001 loss:13.682 precision=36.353%\n",
      "epoch:6, nombre batch:3101 loss:14.358 precision=34.420%\n",
      "epoch:6, nombre batch:3201 loss:13.886 precision=37.270%\n",
      "epoch:6, nombre batch:3301 loss:13.735 precision=36.155%\n",
      "epoch:6, nombre batch:3401 loss:13.699 precision=37.805%\n",
      "epoch:6, nombre batch:3501 loss:14.110 precision=35.098%\n",
      "epoch:6, nombre batch:3601 loss:13.818 precision=35.813%\n",
      "epoch:6, nombre batch:3701 loss:13.718 precision=38.213%\n",
      "epoch:6, nombre batch:3801 loss:13.963 precision=36.414%\n",
      "epoch:6, nombre batch:3901 loss:14.004 precision=36.418%\n",
      "epoch:6, nombre batch:4001 loss:13.623 precision=37.257%\n",
      "epoch:6, nombre batch:4101 loss:14.133 precision=35.642%\n",
      "epoch:6, nombre batch:4201 loss:13.938 precision=36.700%\n",
      "epoch:6, nombre batch:4301 loss:13.880 precision=37.176%\n",
      "epoch:6, nombre batch:4401 loss:14.250 precision=35.345%\n",
      "epoch:6, nombre batch:4501 loss:14.043 precision=35.135%\n",
      "epoch:6, nombre batch:4601 loss:14.312 precision=35.189%\n",
      "epoch:6, nombre batch:4701 loss:13.682 precision=36.454%\n",
      "epoch:6, nombre batch:4801 loss:13.950 precision=36.541%\n",
      "epoch:6, nombre batch:4901 loss:13.962 precision=36.289%\n",
      "epoch:6, nombre batch:5001 loss:13.846 precision=36.501%\n",
      "epoch:6, nombre batch:5101 loss:13.864 precision=37.214%\n",
      "epoch:6, nombre batch:5201 loss:13.543 precision=36.856%\n",
      "epoch:6, nombre batch:5301 loss:14.313 precision=35.581%\n",
      "epoch:6, nombre batch:5401 loss:13.765 precision=37.006%\n",
      "epoch:6, nombre batch:5501 loss:14.507 precision=34.044%\n",
      "epoch:6, nombre batch:5601 loss:13.732 precision=37.453%\n",
      "epoch:6, nombre batch:5701 loss:13.733 precision=37.387%\n",
      "epoch:6, nombre batch:5801 loss:13.617 precision=37.834%\n",
      "epoch:6, nombre batch:5901 loss:13.949 precision=36.347%\n",
      "epoch:6, nombre batch:6001 loss:13.506 precision=37.600%\n",
      "epoch:6, nombre batch:6101 loss:14.177 precision=35.473%\n",
      "epoch:6, nombre batch:6201 loss:13.978 precision=35.693%\n",
      "epoch:6, nombre batch:6301 loss:14.300 precision=35.111%\n",
      "epoch:6, nombre batch:6401 loss:13.714 precision=35.624%\n",
      "epoch:6, nombre batch:6501 loss:14.092 precision=35.452%\n",
      "epoch:6, nombre batch:6601 loss:13.943 precision=35.758%\n",
      "epoch:6, nombre batch:6701 loss:14.049 precision=35.566%\n",
      "epoch:6, nombre batch:6801 loss:13.836 precision=35.870%\n",
      "epoch:6, nombre batch:6901 loss:13.820 precision=37.667%\n",
      "epoch:6, nombre batch:7001 loss:13.864 precision=35.919%\n",
      "epoch:6, nombre batch:7101 loss:14.094 precision=36.319%\n",
      "epoch:6, nombre batch:7201 loss:13.701 precision=37.537%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, nombre batch:7301 loss:13.903 precision=37.602%\n",
      "epoch:6, nombre batch:7401 loss:13.956 precision=37.398%\n",
      "epoch:6, nombre batch:7501 loss:13.702 precision=36.716%\n",
      "epoch:6, nombre batch:7601 loss:13.355 precision=38.742%\n",
      "epoch:6, nombre batch:7701 loss:13.936 precision=36.525%\n",
      "epoch:6, nombre batch:7801 loss:13.961 precision=36.500%\n",
      "epoch:6, nombre batch:7901 loss:13.866 precision=35.913%\n",
      "epoch:6, nombre batch:8001 loss:14.364 precision=35.659%\n",
      "epoch:6, nombre batch:8101 loss:13.846 precision=36.056%\n",
      "epoch:6, nombre batch:8201 loss:13.826 precision=36.574%\n",
      "epoch:6, nombre batch:8301 loss:14.170 precision=35.600%\n",
      "epoch:6, nombre batch:8401 loss:13.670 precision=36.732%\n",
      "epoch:6, nombre batch:8501 loss:14.019 precision=36.780%\n",
      "epoch:6, nombre batch:8601 loss:13.640 precision=37.097%\n",
      "epoch:6, nombre batch:8701 loss:13.723 precision=38.288%\n",
      "epoch:6, nombre batch:8801 loss:13.998 precision=35.233%\n",
      "epoch:6, nombre batch:8901 loss:13.643 precision=36.810%\n",
      "epoch:6, nombre batch:9001 loss:14.022 precision=36.039%\n",
      "epoch:6, nombre batch:9101 loss:13.715 precision=36.884%\n",
      "epoch:6, nombre batch:9201 loss:14.086 precision=36.114%\n",
      "epoch:6, nombre batch:9301 loss:13.806 precision=36.961%\n",
      "epoch:6, nombre batch:9401 loss:13.603 precision=38.462%\n",
      "epoch:6, nombre batch:9501 loss:13.425 precision=37.909%\n",
      "epoch:6, nombre batch:9601 loss:13.458 precision=38.099%\n",
      "epoch:6, nombre batch:9701 loss:14.173 precision=35.541%\n",
      "epoch:6, nombre batch:9801 loss:13.637 precision=36.081%\n",
      "epoch:6, nombre batch:9901 loss:13.802 precision=37.546%\n",
      "epoch:6, nombre batch:10001 loss:13.859 precision=37.209%\n",
      "epoch:6, nombre batch:10101 loss:13.844 precision=36.360%\n",
      "epoch:6, nombre batch:10201 loss:13.660 precision=37.867%\n",
      "epoch:6, nombre batch:10301 loss:13.968 precision=36.590%\n",
      "epoch:6, nombre batch:10401 loss:13.880 precision=36.336%\n",
      "epoch:6, nombre batch:10501 loss:13.918 precision=35.201%\n",
      "epoch:6, nombre batch:10601 loss:14.150 precision=36.673%\n",
      "epoch:6, nombre batch:10701 loss:13.713 precision=36.924%\n",
      "epoch:6, nombre batch:10801 loss:13.902 precision=36.445%\n",
      "epoch:6, nombre batch:10901 loss:14.106 precision=35.896%\n",
      "epoch:6, nombre batch:11001 loss:14.123 precision=35.714%\n",
      "epoch:6, nombre batch:11101 loss:14.250 precision=35.876%\n",
      "epoch:6, nombre batch:11201 loss:14.077 precision=36.554%\n",
      "epoch:6, nombre batch:11301 loss:13.755 precision=36.246%\n",
      "epoch:6, nombre batch:11401 loss:13.834 precision=37.175%\n",
      "epoch:6, nombre batch:11501 loss:13.657 precision=37.043%\n",
      "epoch:6, nombre batch:11601 loss:13.904 precision=36.422%\n",
      "epoch:6, nombre batch:11701 loss:14.083 precision=35.480%\n",
      "epoch:6, nombre batch:11801 loss:13.612 precision=37.192%\n",
      "epoch:6, nombre batch:11901 loss:14.031 precision=37.083%\n",
      "epoch:6, nombre batch:12001 loss:14.228 precision=35.613%\n",
      "epoch:6, nombre batch:12101 loss:13.927 precision=37.206%\n",
      "epoch:6, nombre batch:12201 loss:13.872 precision=37.253%\n",
      "epoch:6, nombre batch:12301 loss:14.080 precision=35.502%\n",
      "epoch:6, nombre batch:12401 loss:13.502 precision=38.357%\n",
      "epoch:6, nombre batch:12501 loss:13.903 precision=36.697%\n",
      "epoch:6, nombre batch:12601 loss:13.702 precision=37.391%\n",
      "epoch:6, nombre batch:12701 loss:13.946 precision=36.680%\n",
      "epoch:6, nombre batch:12801 loss:13.859 precision=36.343%\n",
      "epoch:6, nombre batch:12901 loss:13.766 precision=36.688%\n",
      "epoch:6, nombre batch:13001 loss:13.810 precision=37.018%\n",
      "epoch:6, nombre batch:13101 loss:13.862 precision=35.896%\n",
      "epoch:6, nombre batch:13201 loss:14.024 precision=36.510%\n",
      "epoch:6, nombre batch:13301 loss:13.697 precision=37.734%\n",
      "epoch:6, nombre batch:13401 loss:13.895 precision=36.578%\n",
      "epoch:6, nombre batch:13501 loss:14.054 precision=36.112%\n",
      "epoch:6, nombre batch:13601 loss:13.951 precision=36.233%\n",
      "epoch:6, nombre batch:13701 loss:13.923 precision=35.834%\n",
      "epoch:6, nombre batch:13801 loss:14.209 precision=36.947%\n",
      "epoch:6, nombre batch:13901 loss:14.304 precision=34.444%\n",
      "epoch:6, nombre batch:14001 loss:13.758 precision=37.691%\n",
      "epoch:6, nombre batch:14101 loss:13.507 precision=37.939%\n",
      "epoch:6, nombre batch:14201 loss:14.047 precision=35.806%\n",
      "epoch:6, nombre batch:14301 loss:13.911 precision=36.555%\n",
      "epoch:6, nombre batch:14401 loss:13.913 precision=36.650%\n",
      "epoch:6, nombre batch:14501 loss:14.154 precision=36.219%\n",
      "epoch:6, nombre batch:14601 loss:14.243 precision=35.124%\n",
      "epoch:6, nombre batch:14701 loss:13.996 precision=36.244%\n",
      "epoch:6, nombre batch:14801 loss:13.896 precision=36.095%\n",
      "epoch:6, nombre batch:14901 loss:13.821 precision=36.250%\n",
      "epoch:6, nombre batch:15001 loss:13.774 precision=36.689%\n",
      "epoch:6, nombre batch:15101 loss:13.934 precision=36.452%\n",
      "epoch:6, nombre batch:15201 loss:13.401 precision=37.735%\n",
      "epoch:6, nombre batch:15301 loss:13.687 precision=37.672%\n",
      "epoch:6, nombre batch:15401 loss:13.718 precision=37.585%\n",
      "epoch:6, nombre batch:15501 loss:13.688 precision=36.975%\n",
      "epoch:6, nombre batch:15601 loss:14.167 precision=36.117%\n",
      "epoch:6, nombre batch:15701 loss:14.376 precision=35.598%\n",
      "epoch:6, nombre batch:15801 loss:13.631 precision=36.764%\n",
      "epoch:6, nombre batch:15901 loss:13.798 precision=36.943%\n",
      "epoch:6, nombre batch:16001 loss:14.042 precision=36.159%\n",
      "epoch:6, nombre batch:16101 loss:14.193 precision=35.861%\n",
      "epoch:6, nombre batch:16201 loss:13.881 precision=36.154%\n",
      "epoch:6, nombre batch:16301 loss:14.169 precision=35.296%\n",
      "epoch:6, nombre batch:16401 loss:14.256 precision=35.325%\n",
      "epoch:6, nombre batch:16501 loss:14.165 precision=36.593%\n",
      "epoch:6, nombre batch:16601 loss:14.086 precision=36.292%\n",
      "epoch:6, nombre batch:16701 loss:14.102 precision=35.733%\n",
      "epoch:6, nombre batch:16801 loss:13.960 precision=36.535%\n",
      "epoch:6, nombre batch:16901 loss:13.927 precision=36.944%\n",
      "epoch:6, nombre batch:17001 loss:14.195 precision=36.077%\n",
      "epoch:6, nombre batch:17101 loss:13.907 precision=36.481%\n",
      "epoch:6, nombre batch:17201 loss:14.077 precision=35.993%\n",
      "epoch:6, nombre batch:17301 loss:13.664 precision=37.081%\n",
      "epoch:6, nombre batch:17401 loss:13.629 precision=36.961%\n",
      "epoch:6, nombre batch:17501 loss:13.840 precision=37.374%\n",
      "epoch:6, nombre batch:17601 loss:13.879 precision=36.023%\n",
      "epoch:6, nombre batch:17701 loss:13.743 precision=37.551%\n",
      "epoch:6, nombre batch:17801 loss:13.755 precision=37.317%\n",
      "epoch:6, nombre batch:17901 loss:14.100 precision=35.963%\n",
      "epoch:6, nombre batch:18001 loss:14.215 precision=36.091%\n",
      "epoch:6, nombre batch:18101 loss:13.581 precision=37.632%\n",
      "epoch:6, nombre batch:18201 loss:14.039 precision=36.011%\n",
      "epoch:6, nombre batch:18301 loss:13.928 precision=36.391%\n",
      "epoch:6, nombre batch:18401 loss:13.915 precision=36.084%\n",
      "epoch:6, nombre batch:18501 loss:13.825 precision=35.940%\n",
      "epoch:6, nombre batch:18601 loss:13.890 precision=35.936%\n",
      "epoch:6, nombre batch:18701 loss:14.019 precision=36.633%\n",
      "epoch:6, nombre batch:18801 loss:13.805 precision=37.238%\n",
      "epoch:6, nombre batch:18901 loss:14.051 precision=36.644%\n",
      "epoch:6, nombre batch:19001 loss:13.721 precision=37.161%\n",
      "epoch:6, nombre batch:19101 loss:13.943 precision=36.846%\n",
      "epoch:6, nombre batch:19201 loss:13.795 precision=36.398%\n",
      "epoch:6, nombre batch:19301 loss:13.435 precision=37.913%\n",
      "epoch:6, nombre batch:19401 loss:14.196 precision=35.915%\n",
      "epoch:6, nombre batch:19501 loss:13.462 precision=37.236%\n",
      "epoch:6, nombre batch:19601 loss:13.840 precision=36.767%\n",
      "epoch:6, nombre batch:19701 loss:14.022 precision=35.693%\n",
      "epoch:6, nombre batch:19801 loss:13.878 precision=37.565%\n",
      "epoch:6, nombre batch:19901 loss:13.807 precision=37.172%\n",
      "epoch:6, nombre batch:20001 loss:13.791 precision=37.593%\n",
      "epoch:6, nombre batch:20101 loss:13.884 precision=36.536%\n",
      "epoch:6, nombre batch:20201 loss:13.932 precision=37.477%\n",
      "epoch:6, nombre batch:20301 loss:13.529 precision=37.486%\n",
      "epoch:6, nombre batch:20401 loss:14.074 precision=35.701%\n",
      "epoch:6, nombre batch:20501 loss:13.739 precision=36.688%\n",
      "epoch:6, nombre batch:20601 loss:13.979 precision=36.432%\n",
      "epoch:6, nombre batch:20701 loss:13.830 precision=36.971%\n",
      "epoch:6, nombre batch:20801 loss:14.052 precision=36.350%\n",
      "epoch:6, nombre batch:20901 loss:13.640 precision=37.429%\n",
      "epoch:6, nombre batch:21001 loss:13.293 precision=38.224%\n",
      "epoch:6, nombre batch:21101 loss:13.794 precision=37.076%\n",
      "epoch:6, nombre batch:21201 loss:13.972 precision=37.027%\n",
      "epoch:6, nombre batch:21301 loss:13.701 precision=37.293%\n",
      "epoch:6, nombre batch:21401 loss:13.983 precision=36.418%\n",
      "epoch:6, nombre batch:21501 loss:13.970 precision=36.897%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, nombre batch:21601 loss:13.955 precision=35.750%\n",
      "epoch:6, nombre batch:21701 loss:13.943 precision=37.294%\n",
      "epoch:6, nombre batch:21801 loss:14.523 precision=34.883%\n",
      "epoch:6, nombre batch:21901 loss:13.872 precision=36.864%\n",
      "epoch:6, nombre batch:22001 loss:13.839 precision=37.533%\n",
      "epoch:6, nombre batch:22101 loss:13.859 precision=36.771%\n",
      "epoch:6, nombre batch:22201 loss:13.905 precision=36.452%\n",
      "epoch:6, nombre batch:22301 loss:13.214 precision=37.928%\n",
      "epoch:6, nombre batch:22401 loss:14.473 precision=34.139%\n",
      "epoch:6, nombre batch:22501 loss:14.057 precision=36.514%\n",
      "epoch:6, nombre batch:22601 loss:13.908 precision=36.897%\n",
      "epoch:6, nombre batch:22701 loss:13.630 precision=37.231%\n",
      "epoch:6, nombre batch:22801 loss:13.952 precision=36.174%\n",
      "epoch:6, nombre batch:22901 loss:13.788 precision=37.756%\n",
      "epoch:6, nombre batch:23001 loss:13.910 precision=36.436%\n",
      "Resultat Epoch 6;  loss:13.892, Accuracy=43.567%,  precision:36.599%, AMS=0.9894\n",
      "epoch:7, nombre batch:1 loss:0.164 precision=26.923%\n",
      "epoch:7, nombre batch:101 loss:13.753 precision=36.996%\n",
      "epoch:7, nombre batch:201 loss:13.667 precision=38.825%\n",
      "epoch:7, nombre batch:301 loss:13.699 precision=36.988%\n",
      "epoch:7, nombre batch:401 loss:13.864 precision=36.944%\n",
      "epoch:7, nombre batch:501 loss:13.609 precision=37.613%\n",
      "epoch:7, nombre batch:601 loss:14.059 precision=36.595%\n",
      "epoch:7, nombre batch:701 loss:13.717 precision=36.874%\n",
      "epoch:7, nombre batch:801 loss:13.892 precision=36.055%\n",
      "epoch:7, nombre batch:901 loss:13.831 precision=36.858%\n",
      "epoch:7, nombre batch:1001 loss:13.835 precision=37.561%\n",
      "epoch:7, nombre batch:1101 loss:13.774 precision=36.208%\n",
      "epoch:7, nombre batch:1201 loss:13.968 precision=37.091%\n",
      "epoch:7, nombre batch:1301 loss:14.276 precision=34.910%\n",
      "epoch:7, nombre batch:1401 loss:13.868 precision=37.044%\n",
      "epoch:7, nombre batch:1501 loss:13.276 precision=38.162%\n",
      "epoch:7, nombre batch:1601 loss:13.870 precision=37.174%\n",
      "epoch:7, nombre batch:1701 loss:13.699 precision=37.103%\n",
      "epoch:7, nombre batch:1801 loss:13.819 precision=36.200%\n",
      "epoch:7, nombre batch:1901 loss:13.673 precision=37.199%\n",
      "epoch:7, nombre batch:2001 loss:14.238 precision=35.338%\n",
      "epoch:7, nombre batch:2101 loss:13.665 precision=38.015%\n",
      "epoch:7, nombre batch:2201 loss:13.897 precision=36.846%\n",
      "epoch:7, nombre batch:2301 loss:14.067 precision=35.132%\n",
      "epoch:7, nombre batch:2401 loss:13.317 precision=38.529%\n",
      "epoch:7, nombre batch:2501 loss:13.449 precision=38.895%\n",
      "epoch:7, nombre batch:2601 loss:13.537 precision=37.144%\n",
      "epoch:7, nombre batch:2701 loss:14.256 precision=35.810%\n",
      "epoch:7, nombre batch:2801 loss:13.988 precision=36.466%\n",
      "epoch:7, nombre batch:2901 loss:14.202 precision=35.000%\n",
      "epoch:7, nombre batch:3001 loss:14.016 precision=36.303%\n",
      "epoch:7, nombre batch:3101 loss:13.642 precision=37.250%\n",
      "epoch:7, nombre batch:3201 loss:13.968 precision=36.136%\n",
      "epoch:7, nombre batch:3301 loss:13.781 precision=36.912%\n",
      "epoch:7, nombre batch:3401 loss:13.656 precision=36.734%\n",
      "epoch:7, nombre batch:3501 loss:13.689 precision=37.509%\n",
      "epoch:7, nombre batch:3601 loss:14.017 precision=36.411%\n",
      "epoch:7, nombre batch:3701 loss:14.041 precision=36.080%\n",
      "epoch:7, nombre batch:3801 loss:14.277 precision=36.397%\n",
      "epoch:7, nombre batch:3901 loss:14.446 precision=34.046%\n",
      "epoch:7, nombre batch:4001 loss:13.956 precision=36.112%\n",
      "epoch:7, nombre batch:4101 loss:13.501 precision=38.220%\n",
      "epoch:7, nombre batch:4201 loss:13.831 precision=37.141%\n",
      "epoch:7, nombre batch:4301 loss:13.898 precision=36.797%\n",
      "epoch:7, nombre batch:4401 loss:14.217 precision=34.936%\n",
      "epoch:7, nombre batch:4501 loss:13.881 precision=35.945%\n",
      "epoch:7, nombre batch:4601 loss:13.828 precision=37.117%\n",
      "epoch:7, nombre batch:4701 loss:13.769 precision=36.743%\n",
      "epoch:7, nombre batch:4801 loss:13.630 precision=38.373%\n",
      "epoch:7, nombre batch:4901 loss:13.956 precision=36.960%\n",
      "epoch:7, nombre batch:5001 loss:13.858 precision=36.919%\n",
      "epoch:7, nombre batch:5101 loss:13.933 precision=35.823%\n",
      "epoch:7, nombre batch:5201 loss:13.996 precision=35.964%\n",
      "epoch:7, nombre batch:5301 loss:13.933 precision=36.188%\n",
      "epoch:7, nombre batch:5401 loss:14.466 precision=34.584%\n",
      "epoch:7, nombre batch:5501 loss:14.030 precision=34.892%\n",
      "epoch:7, nombre batch:5601 loss:14.055 precision=36.012%\n",
      "epoch:7, nombre batch:5701 loss:13.527 precision=38.108%\n",
      "epoch:7, nombre batch:5801 loss:14.228 precision=36.215%\n",
      "epoch:7, nombre batch:5901 loss:13.710 precision=38.419%\n",
      "epoch:7, nombre batch:6001 loss:13.525 precision=38.499%\n",
      "epoch:7, nombre batch:6101 loss:14.047 precision=36.418%\n",
      "epoch:7, nombre batch:6201 loss:13.496 precision=38.637%\n",
      "epoch:7, nombre batch:6301 loss:14.502 precision=34.736%\n",
      "epoch:7, nombre batch:6401 loss:13.688 precision=37.590%\n",
      "epoch:7, nombre batch:6501 loss:14.170 precision=36.428%\n",
      "epoch:7, nombre batch:6601 loss:13.980 precision=35.992%\n",
      "epoch:7, nombre batch:6701 loss:13.768 precision=37.439%\n",
      "epoch:7, nombre batch:6801 loss:13.703 precision=36.192%\n",
      "epoch:7, nombre batch:6901 loss:13.929 precision=36.706%\n",
      "epoch:7, nombre batch:7001 loss:14.107 precision=36.488%\n",
      "epoch:7, nombre batch:7101 loss:14.196 precision=35.936%\n",
      "epoch:7, nombre batch:7201 loss:13.909 precision=36.234%\n",
      "epoch:7, nombre batch:7301 loss:13.824 precision=36.961%\n",
      "epoch:7, nombre batch:7401 loss:13.829 precision=36.405%\n",
      "epoch:7, nombre batch:7501 loss:13.242 precision=39.286%\n",
      "epoch:7, nombre batch:7601 loss:13.546 precision=37.163%\n",
      "epoch:7, nombre batch:7701 loss:14.024 precision=35.445%\n",
      "epoch:7, nombre batch:7801 loss:13.945 precision=35.660%\n",
      "epoch:7, nombre batch:7901 loss:14.338 precision=34.549%\n",
      "epoch:7, nombre batch:8001 loss:13.758 precision=36.604%\n",
      "epoch:7, nombre batch:8101 loss:13.897 precision=36.607%\n",
      "epoch:7, nombre batch:8201 loss:14.028 precision=35.514%\n",
      "epoch:7, nombre batch:8301 loss:13.408 precision=36.487%\n",
      "epoch:7, nombre batch:8401 loss:13.903 precision=36.121%\n",
      "epoch:7, nombre batch:8501 loss:13.656 precision=37.575%\n",
      "epoch:7, nombre batch:8601 loss:13.763 precision=37.286%\n",
      "epoch:7, nombre batch:8701 loss:13.682 precision=37.457%\n",
      "epoch:7, nombre batch:8801 loss:13.883 precision=35.693%\n",
      "epoch:7, nombre batch:8901 loss:14.190 precision=36.154%\n",
      "epoch:7, nombre batch:9001 loss:13.714 precision=36.412%\n",
      "epoch:7, nombre batch:9101 loss:13.462 precision=37.672%\n",
      "epoch:7, nombre batch:9201 loss:14.222 precision=34.799%\n",
      "epoch:7, nombre batch:9301 loss:13.951 precision=35.249%\n",
      "epoch:7, nombre batch:9401 loss:13.718 precision=37.770%\n",
      "epoch:7, nombre batch:9501 loss:13.943 precision=35.265%\n",
      "epoch:7, nombre batch:9601 loss:13.809 precision=36.239%\n",
      "epoch:7, nombre batch:9701 loss:14.065 precision=37.045%\n",
      "epoch:7, nombre batch:9801 loss:13.804 precision=36.913%\n",
      "epoch:7, nombre batch:9901 loss:13.616 precision=37.444%\n",
      "epoch:7, nombre batch:10001 loss:14.004 precision=36.184%\n",
      "epoch:7, nombre batch:10101 loss:13.903 precision=36.957%\n",
      "epoch:7, nombre batch:10201 loss:13.574 precision=36.924%\n",
      "epoch:7, nombre batch:10301 loss:13.874 precision=37.033%\n",
      "epoch:7, nombre batch:10401 loss:13.866 precision=37.681%\n",
      "epoch:7, nombre batch:10501 loss:14.298 precision=34.568%\n",
      "epoch:7, nombre batch:10601 loss:14.119 precision=36.679%\n",
      "epoch:7, nombre batch:10701 loss:13.904 precision=36.615%\n",
      "epoch:7, nombre batch:10801 loss:14.018 precision=36.374%\n",
      "epoch:7, nombre batch:10901 loss:13.847 precision=35.240%\n",
      "epoch:7, nombre batch:11001 loss:14.095 precision=36.340%\n",
      "epoch:7, nombre batch:11101 loss:13.740 precision=37.222%\n",
      "epoch:7, nombre batch:11201 loss:14.030 precision=35.860%\n",
      "epoch:7, nombre batch:11301 loss:13.531 precision=37.730%\n",
      "epoch:7, nombre batch:11401 loss:13.466 precision=38.279%\n",
      "epoch:7, nombre batch:11501 loss:13.681 precision=36.633%\n",
      "epoch:7, nombre batch:11601 loss:13.836 precision=36.339%\n",
      "epoch:7, nombre batch:11701 loss:13.525 precision=36.897%\n",
      "epoch:7, nombre batch:11801 loss:14.331 precision=34.470%\n",
      "epoch:7, nombre batch:11901 loss:13.420 precision=37.694%\n",
      "epoch:7, nombre batch:12001 loss:13.652 precision=36.970%\n",
      "epoch:7, nombre batch:12101 loss:14.063 precision=36.448%\n",
      "epoch:7, nombre batch:12201 loss:14.058 precision=35.192%\n",
      "epoch:7, nombre batch:12301 loss:13.927 precision=35.763%\n",
      "epoch:7, nombre batch:12401 loss:13.932 precision=36.887%\n",
      "epoch:7, nombre batch:12501 loss:14.183 precision=36.164%\n",
      "epoch:7, nombre batch:12601 loss:13.300 precision=38.403%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, nombre batch:12701 loss:14.034 precision=36.138%\n",
      "epoch:7, nombre batch:12801 loss:13.631 precision=37.387%\n",
      "epoch:7, nombre batch:12901 loss:14.238 precision=35.006%\n",
      "epoch:7, nombre batch:13001 loss:13.760 precision=37.243%\n",
      "epoch:7, nombre batch:13101 loss:13.852 precision=36.836%\n",
      "epoch:7, nombre batch:13201 loss:13.769 precision=38.004%\n",
      "epoch:7, nombre batch:13301 loss:14.201 precision=35.334%\n",
      "epoch:7, nombre batch:13401 loss:14.063 precision=35.920%\n",
      "epoch:7, nombre batch:13501 loss:14.098 precision=36.394%\n",
      "epoch:7, nombre batch:13601 loss:13.930 precision=36.844%\n",
      "epoch:7, nombre batch:13701 loss:13.868 precision=36.826%\n",
      "epoch:7, nombre batch:13801 loss:13.509 precision=38.178%\n",
      "epoch:7, nombre batch:13901 loss:13.580 precision=37.371%\n",
      "epoch:7, nombre batch:14001 loss:14.001 precision=36.600%\n",
      "epoch:7, nombre batch:14101 loss:13.720 precision=37.401%\n",
      "epoch:7, nombre batch:14201 loss:13.982 precision=37.121%\n",
      "epoch:7, nombre batch:14301 loss:13.769 precision=38.169%\n",
      "epoch:7, nombre batch:14401 loss:13.781 precision=38.737%\n",
      "epoch:7, nombre batch:14501 loss:13.495 precision=37.947%\n",
      "epoch:7, nombre batch:14601 loss:13.934 precision=36.628%\n",
      "epoch:7, nombre batch:14701 loss:13.591 precision=37.754%\n",
      "epoch:7, nombre batch:14801 loss:13.859 precision=37.463%\n",
      "epoch:7, nombre batch:14901 loss:14.183 precision=35.026%\n",
      "epoch:7, nombre batch:15001 loss:14.153 precision=35.089%\n",
      "epoch:7, nombre batch:15101 loss:13.981 precision=37.054%\n",
      "epoch:7, nombre batch:15201 loss:14.039 precision=35.853%\n",
      "epoch:7, nombre batch:15301 loss:14.147 precision=36.066%\n",
      "epoch:7, nombre batch:15401 loss:14.003 precision=35.991%\n",
      "epoch:7, nombre batch:15501 loss:13.774 precision=36.371%\n",
      "epoch:7, nombre batch:15601 loss:13.648 precision=37.670%\n",
      "epoch:7, nombre batch:15701 loss:13.952 precision=36.838%\n",
      "epoch:7, nombre batch:15801 loss:13.870 precision=36.722%\n",
      "epoch:7, nombre batch:15901 loss:14.195 precision=34.796%\n",
      "epoch:7, nombre batch:16001 loss:13.799 precision=37.208%\n",
      "epoch:7, nombre batch:16101 loss:13.876 precision=36.951%\n",
      "epoch:7, nombre batch:16201 loss:13.993 precision=36.384%\n",
      "epoch:7, nombre batch:16301 loss:14.193 precision=37.094%\n",
      "epoch:7, nombre batch:16401 loss:14.057 precision=35.663%\n",
      "epoch:7, nombre batch:16501 loss:13.729 precision=38.227%\n",
      "epoch:7, nombre batch:16601 loss:14.158 precision=34.312%\n",
      "epoch:7, nombre batch:16701 loss:14.190 precision=36.434%\n",
      "epoch:7, nombre batch:16801 loss:13.671 precision=36.360%\n",
      "epoch:7, nombre batch:16901 loss:13.695 precision=37.998%\n",
      "epoch:7, nombre batch:17001 loss:14.059 precision=36.218%\n",
      "epoch:7, nombre batch:17101 loss:13.713 precision=37.119%\n",
      "epoch:7, nombre batch:17201 loss:13.744 precision=37.359%\n",
      "epoch:7, nombre batch:17301 loss:13.797 precision=37.556%\n",
      "epoch:7, nombre batch:17401 loss:13.939 precision=37.156%\n",
      "epoch:7, nombre batch:17501 loss:14.138 precision=36.061%\n",
      "epoch:7, nombre batch:17601 loss:14.004 precision=36.731%\n",
      "epoch:7, nombre batch:17701 loss:14.104 precision=35.671%\n",
      "epoch:7, nombre batch:17801 loss:13.572 precision=37.934%\n",
      "epoch:7, nombre batch:17901 loss:13.902 precision=35.806%\n",
      "epoch:7, nombre batch:18001 loss:13.926 precision=36.994%\n",
      "epoch:7, nombre batch:18101 loss:13.338 precision=39.495%\n",
      "epoch:7, nombre batch:18201 loss:13.804 precision=36.333%\n",
      "epoch:7, nombre batch:18301 loss:13.715 precision=37.430%\n",
      "epoch:7, nombre batch:18401 loss:14.082 precision=35.969%\n",
      "epoch:7, nombre batch:18501 loss:14.351 precision=35.137%\n",
      "epoch:7, nombre batch:18601 loss:14.204 precision=35.783%\n",
      "epoch:7, nombre batch:18701 loss:13.592 precision=37.975%\n",
      "epoch:7, nombre batch:18801 loss:13.680 precision=36.757%\n",
      "epoch:7, nombre batch:18901 loss:13.795 precision=36.651%\n",
      "epoch:7, nombre batch:19001 loss:14.327 precision=36.070%\n",
      "epoch:7, nombre batch:19101 loss:13.917 precision=36.018%\n",
      "epoch:7, nombre batch:19201 loss:13.928 precision=36.637%\n",
      "epoch:7, nombre batch:19301 loss:13.994 precision=37.197%\n",
      "epoch:7, nombre batch:19401 loss:13.945 precision=37.556%\n",
      "epoch:7, nombre batch:19501 loss:13.560 precision=36.781%\n",
      "epoch:7, nombre batch:19601 loss:14.087 precision=35.695%\n",
      "epoch:7, nombre batch:19701 loss:13.992 precision=36.517%\n",
      "epoch:7, nombre batch:19801 loss:13.772 precision=37.350%\n",
      "epoch:7, nombre batch:19901 loss:14.130 precision=35.933%\n",
      "epoch:7, nombre batch:20001 loss:13.709 precision=37.358%\n",
      "epoch:7, nombre batch:20101 loss:13.384 precision=39.065%\n",
      "epoch:7, nombre batch:20201 loss:14.270 precision=33.992%\n",
      "epoch:7, nombre batch:20301 loss:13.893 precision=36.871%\n",
      "epoch:7, nombre batch:20401 loss:14.068 precision=37.983%\n",
      "epoch:7, nombre batch:20501 loss:13.757 precision=36.571%\n",
      "epoch:7, nombre batch:20601 loss:13.924 precision=36.244%\n",
      "epoch:7, nombre batch:20701 loss:13.931 precision=37.361%\n",
      "epoch:7, nombre batch:20801 loss:13.919 precision=36.275%\n",
      "epoch:7, nombre batch:20901 loss:14.050 precision=35.698%\n",
      "epoch:7, nombre batch:21001 loss:13.964 precision=37.136%\n",
      "epoch:7, nombre batch:21101 loss:14.196 precision=35.281%\n",
      "epoch:7, nombre batch:21201 loss:13.925 precision=36.725%\n",
      "epoch:7, nombre batch:21301 loss:13.506 precision=37.303%\n",
      "epoch:7, nombre batch:21401 loss:13.963 precision=36.907%\n",
      "epoch:7, nombre batch:21501 loss:14.555 precision=33.587%\n",
      "epoch:7, nombre batch:21601 loss:13.830 precision=36.543%\n",
      "epoch:7, nombre batch:21701 loss:13.902 precision=36.483%\n",
      "epoch:7, nombre batch:21801 loss:14.091 precision=35.712%\n",
      "epoch:7, nombre batch:21901 loss:13.632 precision=36.413%\n",
      "epoch:7, nombre batch:22001 loss:14.093 precision=36.353%\n",
      "epoch:7, nombre batch:22101 loss:13.718 precision=37.012%\n",
      "epoch:7, nombre batch:22201 loss:13.987 precision=36.086%\n",
      "epoch:7, nombre batch:22301 loss:13.926 precision=37.289%\n",
      "epoch:7, nombre batch:22401 loss:13.976 precision=36.465%\n",
      "epoch:7, nombre batch:22501 loss:13.850 precision=36.595%\n",
      "epoch:7, nombre batch:22601 loss:13.946 precision=36.113%\n",
      "epoch:7, nombre batch:22701 loss:14.214 precision=35.940%\n",
      "epoch:7, nombre batch:22801 loss:14.213 precision=34.342%\n",
      "epoch:7, nombre batch:22901 loss:14.019 precision=36.265%\n",
      "epoch:7, nombre batch:23001 loss:14.461 precision=34.944%\n",
      "Resultat Epoch 7;  loss:13.891, Accuracy=43.603%,  precision:36.628%, AMS=0.9977\n",
      "epoch:8, nombre batch:1 loss:0.113 precision=39.130%\n",
      "epoch:8, nombre batch:101 loss:13.383 precision=38.266%\n",
      "epoch:8, nombre batch:201 loss:13.331 precision=38.308%\n",
      "epoch:8, nombre batch:301 loss:14.206 precision=36.010%\n",
      "epoch:8, nombre batch:401 loss:14.002 precision=37.546%\n",
      "epoch:8, nombre batch:501 loss:14.108 precision=35.720%\n",
      "epoch:8, nombre batch:601 loss:14.229 precision=35.472%\n",
      "epoch:8, nombre batch:701 loss:14.108 precision=35.929%\n",
      "epoch:8, nombre batch:801 loss:13.940 precision=36.220%\n",
      "epoch:8, nombre batch:901 loss:13.792 precision=37.158%\n",
      "epoch:8, nombre batch:1001 loss:13.743 precision=35.955%\n",
      "epoch:8, nombre batch:1101 loss:13.776 precision=36.786%\n",
      "epoch:8, nombre batch:1201 loss:14.010 precision=35.497%\n",
      "epoch:8, nombre batch:1301 loss:13.722 precision=37.307%\n",
      "epoch:8, nombre batch:1401 loss:14.101 precision=35.410%\n",
      "epoch:8, nombre batch:1501 loss:13.380 precision=39.205%\n",
      "epoch:8, nombre batch:1601 loss:13.941 precision=36.637%\n",
      "epoch:8, nombre batch:1701 loss:13.834 precision=37.110%\n",
      "epoch:8, nombre batch:1801 loss:13.783 precision=36.270%\n",
      "epoch:8, nombre batch:1901 loss:14.017 precision=36.832%\n",
      "epoch:8, nombre batch:2001 loss:14.100 precision=35.612%\n",
      "epoch:8, nombre batch:2101 loss:14.152 precision=35.589%\n",
      "epoch:8, nombre batch:2201 loss:14.074 precision=35.461%\n",
      "epoch:8, nombre batch:2301 loss:13.653 precision=37.171%\n",
      "epoch:8, nombre batch:2401 loss:13.576 precision=37.476%\n",
      "epoch:8, nombre batch:2501 loss:13.662 precision=37.425%\n",
      "epoch:8, nombre batch:2601 loss:14.186 precision=35.374%\n",
      "epoch:8, nombre batch:2701 loss:13.879 precision=36.041%\n",
      "epoch:8, nombre batch:2801 loss:13.720 precision=37.870%\n",
      "epoch:8, nombre batch:2901 loss:13.821 precision=37.095%\n",
      "epoch:8, nombre batch:3001 loss:13.952 precision=36.823%\n",
      "epoch:8, nombre batch:3101 loss:13.898 precision=36.759%\n",
      "epoch:8, nombre batch:3201 loss:13.996 precision=36.689%\n",
      "epoch:8, nombre batch:3301 loss:13.773 precision=36.225%\n",
      "epoch:8, nombre batch:3401 loss:13.916 precision=36.357%\n",
      "epoch:8, nombre batch:3501 loss:13.749 precision=36.941%\n",
      "epoch:8, nombre batch:3601 loss:13.842 precision=36.673%\n",
      "epoch:8, nombre batch:3701 loss:13.577 precision=37.599%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, nombre batch:3801 loss:13.880 precision=36.494%\n",
      "epoch:8, nombre batch:3901 loss:13.200 precision=39.925%\n",
      "epoch:8, nombre batch:4001 loss:13.874 precision=37.657%\n",
      "epoch:8, nombre batch:4101 loss:13.870 precision=36.789%\n",
      "epoch:8, nombre batch:4201 loss:13.722 precision=37.392%\n",
      "epoch:8, nombre batch:4301 loss:13.772 precision=36.890%\n",
      "epoch:8, nombre batch:4401 loss:14.272 precision=35.786%\n",
      "epoch:8, nombre batch:4501 loss:14.272 precision=34.951%\n",
      "epoch:8, nombre batch:4601 loss:13.648 precision=37.739%\n",
      "epoch:8, nombre batch:4701 loss:13.942 precision=37.230%\n",
      "epoch:8, nombre batch:4801 loss:13.841 precision=37.048%\n",
      "epoch:8, nombre batch:4901 loss:13.608 precision=37.216%\n",
      "epoch:8, nombre batch:5001 loss:13.609 precision=37.112%\n",
      "epoch:8, nombre batch:5101 loss:14.216 precision=35.083%\n",
      "epoch:8, nombre batch:5201 loss:13.646 precision=36.141%\n",
      "epoch:8, nombre batch:5301 loss:13.751 precision=37.299%\n",
      "epoch:8, nombre batch:5401 loss:13.754 precision=37.706%\n",
      "epoch:8, nombre batch:5501 loss:14.202 precision=35.890%\n",
      "epoch:8, nombre batch:5601 loss:13.641 precision=38.283%\n",
      "epoch:8, nombre batch:5701 loss:13.680 precision=36.984%\n",
      "epoch:8, nombre batch:5801 loss:14.365 precision=33.538%\n",
      "epoch:8, nombre batch:5901 loss:14.003 precision=37.048%\n",
      "epoch:8, nombre batch:6001 loss:14.252 precision=35.627%\n",
      "epoch:8, nombre batch:6101 loss:14.570 precision=35.007%\n",
      "epoch:8, nombre batch:6201 loss:13.616 precision=37.462%\n",
      "epoch:8, nombre batch:6301 loss:14.083 precision=35.884%\n",
      "epoch:8, nombre batch:6401 loss:14.075 precision=35.474%\n",
      "epoch:8, nombre batch:6501 loss:13.486 precision=38.608%\n",
      "epoch:8, nombre batch:6601 loss:14.187 precision=35.997%\n",
      "epoch:8, nombre batch:6701 loss:13.893 precision=36.515%\n",
      "epoch:8, nombre batch:6801 loss:14.009 precision=36.336%\n",
      "epoch:8, nombre batch:6901 loss:13.883 precision=36.880%\n",
      "epoch:8, nombre batch:7001 loss:13.901 precision=37.887%\n",
      "epoch:8, nombre batch:7101 loss:14.071 precision=36.202%\n",
      "epoch:8, nombre batch:7201 loss:13.797 precision=38.032%\n",
      "epoch:8, nombre batch:7301 loss:14.138 precision=35.866%\n",
      "epoch:8, nombre batch:7401 loss:13.644 precision=37.210%\n",
      "epoch:8, nombre batch:7501 loss:13.764 precision=36.941%\n",
      "epoch:8, nombre batch:7601 loss:14.060 precision=35.881%\n",
      "epoch:8, nombre batch:7701 loss:13.971 precision=36.394%\n",
      "epoch:8, nombre batch:7801 loss:13.780 precision=37.350%\n",
      "epoch:8, nombre batch:7901 loss:13.794 precision=36.874%\n",
      "epoch:8, nombre batch:8001 loss:14.141 precision=35.733%\n",
      "epoch:8, nombre batch:8101 loss:13.990 precision=36.347%\n",
      "epoch:8, nombre batch:8201 loss:14.387 precision=34.639%\n",
      "epoch:8, nombre batch:8301 loss:13.929 precision=36.384%\n",
      "epoch:8, nombre batch:8401 loss:14.105 precision=36.387%\n",
      "epoch:8, nombre batch:8501 loss:14.119 precision=36.482%\n",
      "epoch:8, nombre batch:8601 loss:14.125 precision=36.135%\n",
      "epoch:8, nombre batch:8701 loss:13.416 precision=38.554%\n",
      "epoch:8, nombre batch:8801 loss:13.830 precision=36.935%\n",
      "epoch:8, nombre batch:8901 loss:13.951 precision=36.765%\n",
      "epoch:8, nombre batch:9001 loss:14.149 precision=36.083%\n",
      "epoch:8, nombre batch:9101 loss:14.240 precision=37.106%\n",
      "epoch:8, nombre batch:9201 loss:13.881 precision=36.509%\n",
      "epoch:8, nombre batch:9301 loss:13.968 precision=36.422%\n",
      "epoch:8, nombre batch:9401 loss:13.771 precision=37.012%\n",
      "epoch:8, nombre batch:9501 loss:13.584 precision=37.367%\n",
      "epoch:8, nombre batch:9601 loss:13.736 precision=38.226%\n",
      "epoch:8, nombre batch:9701 loss:13.811 precision=37.156%\n",
      "epoch:8, nombre batch:9801 loss:14.097 precision=36.856%\n",
      "epoch:8, nombre batch:9901 loss:13.986 precision=35.603%\n",
      "epoch:8, nombre batch:10001 loss:13.807 precision=37.222%\n",
      "epoch:8, nombre batch:10101 loss:13.761 precision=37.495%\n",
      "epoch:8, nombre batch:10201 loss:14.072 precision=36.459%\n",
      "epoch:8, nombre batch:10301 loss:13.920 precision=35.988%\n",
      "epoch:8, nombre batch:10401 loss:13.450 precision=39.140%\n",
      "epoch:8, nombre batch:10501 loss:13.743 precision=36.933%\n",
      "epoch:8, nombre batch:10601 loss:13.877 precision=36.945%\n",
      "epoch:8, nombre batch:10701 loss:13.860 precision=37.412%\n",
      "epoch:8, nombre batch:10801 loss:13.866 precision=37.519%\n",
      "epoch:8, nombre batch:10901 loss:14.104 precision=35.057%\n",
      "epoch:8, nombre batch:11001 loss:14.177 precision=35.285%\n",
      "epoch:8, nombre batch:11101 loss:13.893 precision=36.185%\n",
      "epoch:8, nombre batch:11201 loss:13.618 precision=38.370%\n",
      "epoch:8, nombre batch:11301 loss:13.889 precision=37.477%\n",
      "epoch:8, nombre batch:11401 loss:13.660 precision=37.024%\n",
      "epoch:8, nombre batch:11501 loss:13.464 precision=38.102%\n",
      "epoch:8, nombre batch:11601 loss:13.761 precision=36.874%\n",
      "epoch:8, nombre batch:11701 loss:13.825 precision=37.265%\n",
      "epoch:8, nombre batch:11801 loss:13.894 precision=36.394%\n",
      "epoch:8, nombre batch:11901 loss:13.709 precision=37.677%\n",
      "epoch:8, nombre batch:12001 loss:14.474 precision=34.765%\n",
      "epoch:8, nombre batch:12101 loss:13.961 precision=36.023%\n",
      "epoch:8, nombre batch:12201 loss:13.586 precision=37.557%\n",
      "epoch:8, nombre batch:12301 loss:13.873 precision=37.574%\n",
      "epoch:8, nombre batch:12401 loss:13.934 precision=37.081%\n",
      "epoch:8, nombre batch:12501 loss:13.844 precision=36.254%\n",
      "epoch:8, nombre batch:12601 loss:14.240 precision=34.361%\n",
      "epoch:8, nombre batch:12701 loss:14.133 precision=36.171%\n",
      "epoch:8, nombre batch:12801 loss:13.840 precision=37.044%\n",
      "epoch:8, nombre batch:12901 loss:13.845 precision=37.265%\n",
      "epoch:8, nombre batch:13001 loss:14.176 precision=35.964%\n",
      "epoch:8, nombre batch:13101 loss:14.220 precision=35.205%\n",
      "epoch:8, nombre batch:13201 loss:14.274 precision=35.343%\n",
      "epoch:8, nombre batch:13301 loss:13.791 precision=36.727%\n",
      "epoch:8, nombre batch:13401 loss:14.022 precision=36.018%\n",
      "epoch:8, nombre batch:13501 loss:13.660 precision=38.058%\n",
      "epoch:8, nombre batch:13601 loss:13.789 precision=37.262%\n",
      "epoch:8, nombre batch:13701 loss:13.960 precision=37.477%\n",
      "epoch:8, nombre batch:13801 loss:13.935 precision=36.581%\n",
      "epoch:8, nombre batch:13901 loss:13.624 precision=37.072%\n",
      "epoch:8, nombre batch:14001 loss:14.346 precision=35.680%\n",
      "epoch:8, nombre batch:14101 loss:14.098 precision=35.319%\n",
      "epoch:8, nombre batch:14201 loss:14.163 precision=34.709%\n",
      "epoch:8, nombre batch:14301 loss:13.523 precision=37.179%\n",
      "epoch:8, nombre batch:14401 loss:13.366 precision=38.251%\n",
      "epoch:8, nombre batch:14501 loss:14.514 precision=34.623%\n",
      "epoch:8, nombre batch:14601 loss:13.936 precision=36.077%\n",
      "epoch:8, nombre batch:14701 loss:14.187 precision=34.476%\n",
      "epoch:8, nombre batch:14801 loss:13.759 precision=37.560%\n",
      "epoch:8, nombre batch:14901 loss:14.417 precision=34.091%\n",
      "epoch:8, nombre batch:15001 loss:13.710 precision=37.406%\n",
      "epoch:8, nombre batch:15101 loss:13.997 precision=36.679%\n",
      "epoch:8, nombre batch:15201 loss:13.197 precision=38.794%\n",
      "epoch:8, nombre batch:15301 loss:13.696 precision=37.543%\n",
      "epoch:8, nombre batch:15401 loss:13.979 precision=35.874%\n",
      "epoch:8, nombre batch:15501 loss:13.980 precision=36.460%\n",
      "epoch:8, nombre batch:15601 loss:13.960 precision=36.895%\n",
      "epoch:8, nombre batch:15701 loss:13.790 precision=37.440%\n",
      "epoch:8, nombre batch:15801 loss:13.465 precision=37.010%\n",
      "epoch:8, nombre batch:15901 loss:14.010 precision=35.979%\n",
      "epoch:8, nombre batch:16001 loss:13.708 precision=37.253%\n",
      "epoch:8, nombre batch:16101 loss:13.717 precision=37.547%\n",
      "epoch:8, nombre batch:16201 loss:13.857 precision=36.508%\n",
      "epoch:8, nombre batch:16301 loss:14.150 precision=36.642%\n",
      "epoch:8, nombre batch:16401 loss:13.635 precision=37.753%\n",
      "epoch:8, nombre batch:16501 loss:13.729 precision=37.228%\n",
      "epoch:8, nombre batch:16601 loss:13.918 precision=36.595%\n",
      "epoch:8, nombre batch:16701 loss:13.842 precision=36.803%\n",
      "epoch:8, nombre batch:16801 loss:13.982 precision=36.822%\n",
      "epoch:8, nombre batch:16901 loss:13.656 precision=37.626%\n",
      "epoch:8, nombre batch:17001 loss:13.873 precision=35.720%\n",
      "epoch:8, nombre batch:17101 loss:14.352 precision=34.760%\n",
      "epoch:8, nombre batch:17201 loss:13.776 precision=36.707%\n",
      "epoch:8, nombre batch:17301 loss:14.120 precision=36.619%\n",
      "epoch:8, nombre batch:17401 loss:13.621 precision=36.739%\n",
      "epoch:8, nombre batch:17501 loss:14.315 precision=35.201%\n",
      "epoch:8, nombre batch:17601 loss:13.553 precision=37.291%\n",
      "epoch:8, nombre batch:17701 loss:13.615 precision=37.038%\n",
      "epoch:8, nombre batch:17801 loss:14.045 precision=35.768%\n",
      "epoch:8, nombre batch:17901 loss:13.644 precision=37.163%\n",
      "epoch:8, nombre batch:18001 loss:13.712 precision=37.368%\n",
      "epoch:8, nombre batch:18101 loss:13.895 precision=37.256%\n",
      "epoch:8, nombre batch:18201 loss:13.764 precision=36.950%\n",
      "epoch:8, nombre batch:18301 loss:13.720 precision=37.453%\n",
      "epoch:8, nombre batch:18401 loss:13.678 precision=36.742%\n",
      "epoch:8, nombre batch:18501 loss:13.765 precision=37.024%\n",
      "epoch:8, nombre batch:18601 loss:14.136 precision=35.663%\n",
      "epoch:8, nombre batch:18701 loss:13.766 precision=37.425%\n",
      "epoch:8, nombre batch:18801 loss:14.192 precision=36.027%\n",
      "epoch:8, nombre batch:18901 loss:14.224 precision=36.040%\n",
      "epoch:8, nombre batch:19001 loss:14.044 precision=35.644%\n",
      "epoch:8, nombre batch:19101 loss:13.641 precision=36.968%\n",
      "epoch:8, nombre batch:19201 loss:13.653 precision=38.081%\n",
      "epoch:8, nombre batch:19301 loss:14.039 precision=36.109%\n",
      "epoch:8, nombre batch:19401 loss:14.013 precision=35.129%\n",
      "epoch:8, nombre batch:19501 loss:13.771 precision=36.364%\n",
      "epoch:8, nombre batch:19601 loss:13.897 precision=36.206%\n",
      "epoch:8, nombre batch:19701 loss:13.412 precision=38.231%\n",
      "epoch:8, nombre batch:19801 loss:13.784 precision=36.824%\n",
      "epoch:8, nombre batch:19901 loss:13.783 precision=37.232%\n",
      "epoch:8, nombre batch:20001 loss:13.812 precision=36.604%\n",
      "epoch:8, nombre batch:20101 loss:13.517 precision=37.064%\n",
      "epoch:8, nombre batch:20201 loss:13.713 precision=36.940%\n",
      "epoch:8, nombre batch:20301 loss:13.575 precision=37.888%\n",
      "epoch:8, nombre batch:20401 loss:14.174 precision=34.892%\n",
      "epoch:8, nombre batch:20501 loss:13.859 precision=36.786%\n",
      "epoch:8, nombre batch:20601 loss:13.706 precision=37.996%\n",
      "epoch:8, nombre batch:20701 loss:13.944 precision=36.793%\n",
      "epoch:8, nombre batch:20801 loss:13.913 precision=36.929%\n",
      "epoch:8, nombre batch:20901 loss:13.920 precision=35.955%\n",
      "epoch:8, nombre batch:21001 loss:13.923 precision=36.734%\n",
      "epoch:8, nombre batch:21101 loss:13.926 precision=37.070%\n",
      "epoch:8, nombre batch:21201 loss:14.025 precision=36.354%\n",
      "epoch:8, nombre batch:21301 loss:14.190 precision=35.062%\n",
      "epoch:8, nombre batch:21401 loss:13.829 precision=36.973%\n",
      "epoch:8, nombre batch:21501 loss:13.925 precision=36.053%\n",
      "epoch:8, nombre batch:21601 loss:13.927 precision=36.138%\n",
      "epoch:8, nombre batch:21701 loss:13.875 precision=36.564%\n",
      "epoch:8, nombre batch:21801 loss:14.053 precision=37.185%\n",
      "epoch:8, nombre batch:21901 loss:14.168 precision=36.491%\n",
      "epoch:8, nombre batch:22001 loss:13.509 precision=37.510%\n",
      "epoch:8, nombre batch:22101 loss:14.069 precision=35.571%\n",
      "epoch:8, nombre batch:22201 loss:14.386 precision=34.026%\n",
      "epoch:8, nombre batch:22301 loss:13.989 precision=35.828%\n",
      "epoch:8, nombre batch:22401 loss:13.734 precision=37.217%\n",
      "epoch:8, nombre batch:22501 loss:14.210 precision=35.861%\n",
      "epoch:8, nombre batch:22601 loss:13.814 precision=36.932%\n",
      "epoch:8, nombre batch:22701 loss:13.486 precision=38.180%\n",
      "epoch:8, nombre batch:22801 loss:13.954 precision=36.917%\n",
      "epoch:8, nombre batch:22901 loss:13.903 precision=36.237%\n",
      "epoch:8, nombre batch:23001 loss:14.182 precision=35.850%\n",
      "Resultat Epoch 8;  loss:13.890, Accuracy=43.652%,  precision:36.661%, AMS=0.9678\n",
      "epoch:9, nombre batch:1 loss:0.139 precision=39.286%\n",
      "epoch:9, nombre batch:101 loss:13.759 precision=37.204%\n",
      "epoch:9, nombre batch:201 loss:14.140 precision=35.593%\n",
      "epoch:9, nombre batch:301 loss:14.158 precision=36.494%\n",
      "epoch:9, nombre batch:401 loss:13.779 precision=36.551%\n",
      "epoch:9, nombre batch:501 loss:14.045 precision=36.956%\n",
      "epoch:9, nombre batch:601 loss:13.458 precision=38.750%\n",
      "epoch:9, nombre batch:701 loss:14.041 precision=36.255%\n",
      "epoch:9, nombre batch:801 loss:13.735 precision=37.401%\n",
      "epoch:9, nombre batch:901 loss:13.929 precision=36.494%\n",
      "epoch:9, nombre batch:1001 loss:13.810 precision=37.326%\n",
      "epoch:9, nombre batch:1101 loss:14.011 precision=36.523%\n",
      "epoch:9, nombre batch:1201 loss:13.885 precision=36.082%\n",
      "epoch:9, nombre batch:1301 loss:13.689 precision=38.324%\n",
      "epoch:9, nombre batch:1401 loss:13.923 precision=36.066%\n",
      "epoch:9, nombre batch:1501 loss:14.078 precision=36.976%\n",
      "epoch:9, nombre batch:1601 loss:13.886 precision=36.652%\n",
      "epoch:9, nombre batch:1701 loss:13.529 precision=37.011%\n",
      "epoch:9, nombre batch:1801 loss:13.851 precision=36.693%\n",
      "epoch:9, nombre batch:1901 loss:13.987 precision=35.833%\n",
      "epoch:9, nombre batch:2001 loss:13.849 precision=37.192%\n",
      "epoch:9, nombre batch:2101 loss:14.144 precision=34.954%\n",
      "epoch:9, nombre batch:2201 loss:13.852 precision=36.309%\n",
      "epoch:9, nombre batch:2301 loss:13.666 precision=36.608%\n",
      "epoch:9, nombre batch:2401 loss:14.408 precision=35.762%\n",
      "epoch:9, nombre batch:2501 loss:13.912 precision=35.920%\n",
      "epoch:9, nombre batch:2601 loss:13.836 precision=37.045%\n",
      "epoch:9, nombre batch:2701 loss:14.107 precision=35.518%\n",
      "epoch:9, nombre batch:2801 loss:13.679 precision=37.211%\n",
      "epoch:9, nombre batch:2901 loss:13.801 precision=36.416%\n",
      "epoch:9, nombre batch:3001 loss:13.763 precision=38.433%\n",
      "epoch:9, nombre batch:3101 loss:13.631 precision=36.163%\n",
      "epoch:9, nombre batch:3201 loss:13.996 precision=35.554%\n",
      "epoch:9, nombre batch:3301 loss:14.342 precision=36.040%\n",
      "epoch:9, nombre batch:3401 loss:14.098 precision=35.828%\n",
      "epoch:9, nombre batch:3501 loss:14.076 precision=37.283%\n",
      "epoch:9, nombre batch:3601 loss:13.877 precision=37.011%\n",
      "epoch:9, nombre batch:3701 loss:13.572 precision=37.481%\n",
      "epoch:9, nombre batch:3801 loss:13.803 precision=36.443%\n",
      "epoch:9, nombre batch:3901 loss:14.201 precision=35.061%\n",
      "epoch:9, nombre batch:4001 loss:13.514 precision=38.792%\n",
      "epoch:9, nombre batch:4101 loss:13.718 precision=37.556%\n",
      "epoch:9, nombre batch:4201 loss:13.819 precision=37.040%\n",
      "epoch:9, nombre batch:4301 loss:13.868 precision=37.290%\n",
      "epoch:9, nombre batch:4401 loss:13.941 precision=36.563%\n",
      "epoch:9, nombre batch:4501 loss:13.653 precision=36.042%\n",
      "epoch:9, nombre batch:4601 loss:13.641 precision=37.867%\n",
      "epoch:9, nombre batch:4701 loss:14.001 precision=35.946%\n",
      "epoch:9, nombre batch:4801 loss:14.086 precision=35.714%\n",
      "epoch:9, nombre batch:4901 loss:13.829 precision=37.071%\n",
      "epoch:9, nombre batch:5001 loss:13.613 precision=37.154%\n",
      "epoch:9, nombre batch:5101 loss:13.571 precision=37.884%\n",
      "epoch:9, nombre batch:5201 loss:14.178 precision=35.480%\n",
      "epoch:9, nombre batch:5301 loss:14.073 precision=36.116%\n",
      "epoch:9, nombre batch:5401 loss:14.306 precision=35.301%\n",
      "epoch:9, nombre batch:5501 loss:13.818 precision=37.668%\n",
      "epoch:9, nombre batch:5601 loss:13.999 precision=37.047%\n",
      "epoch:9, nombre batch:5701 loss:13.997 precision=35.850%\n",
      "epoch:9, nombre batch:5801 loss:14.364 precision=34.699%\n",
      "epoch:9, nombre batch:5901 loss:14.175 precision=34.846%\n",
      "epoch:9, nombre batch:6001 loss:13.942 precision=36.470%\n",
      "epoch:9, nombre batch:6101 loss:13.691 precision=37.443%\n",
      "epoch:9, nombre batch:6201 loss:14.216 precision=35.946%\n",
      "epoch:9, nombre batch:6301 loss:14.030 precision=36.523%\n",
      "epoch:9, nombre batch:6401 loss:13.758 precision=37.566%\n",
      "epoch:9, nombre batch:6501 loss:14.161 precision=35.609%\n",
      "epoch:9, nombre batch:6601 loss:13.927 precision=37.657%\n",
      "epoch:9, nombre batch:6701 loss:14.122 precision=35.998%\n",
      "epoch:9, nombre batch:6801 loss:14.080 precision=36.310%\n",
      "epoch:9, nombre batch:6901 loss:13.812 precision=37.181%\n",
      "epoch:9, nombre batch:7001 loss:13.725 precision=38.055%\n",
      "epoch:9, nombre batch:7101 loss:13.619 precision=38.004%\n",
      "epoch:9, nombre batch:7201 loss:14.019 precision=36.722%\n",
      "epoch:9, nombre batch:7301 loss:13.389 precision=38.196%\n",
      "epoch:9, nombre batch:7401 loss:13.395 precision=38.223%\n",
      "epoch:9, nombre batch:7501 loss:14.310 precision=34.788%\n",
      "epoch:9, nombre batch:7601 loss:13.431 precision=37.415%\n",
      "epoch:9, nombre batch:7701 loss:14.279 precision=34.937%\n",
      "epoch:9, nombre batch:7801 loss:13.655 precision=38.040%\n",
      "epoch:9, nombre batch:7901 loss:13.571 precision=37.320%\n",
      "epoch:9, nombre batch:8001 loss:13.951 precision=37.162%\n",
      "epoch:9, nombre batch:8101 loss:13.791 precision=37.252%\n",
      "epoch:9, nombre batch:8201 loss:14.042 precision=35.861%\n",
      "epoch:9, nombre batch:8301 loss:14.057 precision=35.853%\n",
      "epoch:9, nombre batch:8401 loss:14.042 precision=36.654%\n",
      "epoch:9, nombre batch:8501 loss:14.212 precision=34.774%\n",
      "epoch:9, nombre batch:8601 loss:13.869 precision=35.913%\n",
      "epoch:9, nombre batch:8701 loss:13.983 precision=36.577%\n",
      "epoch:9, nombre batch:8801 loss:13.977 precision=36.582%\n",
      "epoch:9, nombre batch:8901 loss:13.487 precision=37.867%\n",
      "epoch:9, nombre batch:9001 loss:14.271 precision=35.086%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, nombre batch:9101 loss:13.784 precision=37.158%\n",
      "epoch:9, nombre batch:9201 loss:14.286 precision=36.829%\n",
      "epoch:9, nombre batch:9301 loss:13.737 precision=38.058%\n",
      "epoch:9, nombre batch:9401 loss:14.527 precision=34.471%\n",
      "epoch:9, nombre batch:9501 loss:13.522 precision=38.088%\n",
      "epoch:9, nombre batch:9601 loss:13.691 precision=36.888%\n",
      "epoch:9, nombre batch:9701 loss:13.712 precision=36.953%\n",
      "epoch:9, nombre batch:9801 loss:14.032 precision=35.501%\n",
      "epoch:9, nombre batch:9901 loss:14.040 precision=35.564%\n",
      "epoch:9, nombre batch:10001 loss:14.598 precision=34.538%\n",
      "epoch:9, nombre batch:10101 loss:13.998 precision=35.814%\n",
      "epoch:9, nombre batch:10201 loss:13.869 precision=35.965%\n",
      "epoch:9, nombre batch:10301 loss:13.591 precision=37.353%\n",
      "epoch:9, nombre batch:10401 loss:14.131 precision=36.539%\n",
      "epoch:9, nombre batch:10501 loss:14.027 precision=35.447%\n",
      "epoch:9, nombre batch:10601 loss:13.747 precision=36.796%\n",
      "epoch:9, nombre batch:10701 loss:14.152 precision=35.356%\n",
      "epoch:9, nombre batch:10801 loss:13.573 precision=37.547%\n",
      "epoch:9, nombre batch:10901 loss:13.976 precision=37.523%\n",
      "epoch:9, nombre batch:11001 loss:13.867 precision=36.732%\n",
      "epoch:9, nombre batch:11101 loss:13.271 precision=37.946%\n",
      "epoch:9, nombre batch:11201 loss:13.646 precision=37.481%\n",
      "epoch:9, nombre batch:11301 loss:14.220 precision=34.862%\n",
      "epoch:9, nombre batch:11401 loss:13.995 precision=36.140%\n",
      "epoch:9, nombre batch:11501 loss:13.667 precision=38.035%\n",
      "epoch:9, nombre batch:11601 loss:14.015 precision=37.166%\n",
      "epoch:9, nombre batch:11701 loss:14.191 precision=35.250%\n",
      "epoch:9, nombre batch:11801 loss:13.763 precision=36.903%\n",
      "epoch:9, nombre batch:11901 loss:13.967 precision=36.557%\n",
      "epoch:9, nombre batch:12001 loss:13.931 precision=36.459%\n",
      "epoch:9, nombre batch:12101 loss:13.680 precision=37.233%\n",
      "epoch:9, nombre batch:12201 loss:13.893 precision=35.933%\n",
      "epoch:9, nombre batch:12301 loss:14.032 precision=36.932%\n",
      "epoch:9, nombre batch:12401 loss:14.310 precision=34.760%\n",
      "epoch:9, nombre batch:12501 loss:13.865 precision=36.918%\n",
      "epoch:9, nombre batch:12601 loss:13.395 precision=37.892%\n",
      "epoch:9, nombre batch:12701 loss:13.813 precision=36.874%\n",
      "epoch:9, nombre batch:12801 loss:14.207 precision=35.650%\n",
      "epoch:9, nombre batch:12901 loss:14.063 precision=35.220%\n",
      "epoch:9, nombre batch:13001 loss:14.066 precision=36.536%\n",
      "epoch:9, nombre batch:13101 loss:13.977 precision=36.614%\n",
      "epoch:9, nombre batch:13201 loss:13.388 precision=38.380%\n",
      "epoch:9, nombre batch:13301 loss:13.688 precision=36.322%\n",
      "epoch:9, nombre batch:13401 loss:14.034 precision=36.699%\n",
      "epoch:9, nombre batch:13501 loss:13.726 precision=37.552%\n",
      "epoch:9, nombre batch:13601 loss:13.849 precision=36.765%\n",
      "epoch:9, nombre batch:13701 loss:13.915 precision=36.633%\n",
      "epoch:9, nombre batch:13801 loss:14.299 precision=34.682%\n",
      "epoch:9, nombre batch:13901 loss:13.799 precision=36.570%\n",
      "epoch:9, nombre batch:14001 loss:13.531 precision=38.220%\n",
      "epoch:9, nombre batch:14101 loss:13.682 precision=37.631%\n",
      "epoch:9, nombre batch:14201 loss:13.717 precision=36.648%\n",
      "epoch:9, nombre batch:14301 loss:13.971 precision=36.083%\n",
      "epoch:9, nombre batch:14401 loss:13.563 precision=37.826%\n",
      "epoch:9, nombre batch:14501 loss:14.287 precision=35.383%\n",
      "epoch:9, nombre batch:14601 loss:14.045 precision=36.758%\n",
      "epoch:9, nombre batch:14701 loss:13.681 precision=36.880%\n",
      "epoch:9, nombre batch:14801 loss:14.005 precision=35.836%\n",
      "epoch:9, nombre batch:14901 loss:14.143 precision=36.357%\n",
      "epoch:9, nombre batch:15001 loss:13.775 precision=37.809%\n",
      "epoch:9, nombre batch:15101 loss:13.922 precision=36.156%\n",
      "epoch:9, nombre batch:15201 loss:13.927 precision=35.617%\n",
      "epoch:9, nombre batch:15301 loss:13.337 precision=39.716%\n",
      "epoch:9, nombre batch:15401 loss:13.907 precision=36.452%\n",
      "epoch:9, nombre batch:15501 loss:13.716 precision=37.968%\n",
      "epoch:9, nombre batch:15601 loss:13.496 precision=38.430%\n",
      "epoch:9, nombre batch:15701 loss:13.802 precision=36.336%\n",
      "epoch:9, nombre batch:15801 loss:13.457 precision=38.424%\n",
      "epoch:9, nombre batch:15901 loss:14.112 precision=36.130%\n",
      "epoch:9, nombre batch:16001 loss:13.489 precision=37.873%\n",
      "epoch:9, nombre batch:16101 loss:13.104 precision=40.038%\n",
      "epoch:9, nombre batch:16201 loss:14.062 precision=35.633%\n",
      "epoch:9, nombre batch:16301 loss:13.688 precision=37.102%\n",
      "epoch:9, nombre batch:16401 loss:13.935 precision=36.659%\n",
      "epoch:9, nombre batch:16501 loss:13.999 precision=36.415%\n",
      "epoch:9, nombre batch:16601 loss:13.789 precision=37.024%\n",
      "epoch:9, nombre batch:16701 loss:14.001 precision=36.742%\n",
      "epoch:9, nombre batch:16801 loss:13.951 precision=35.540%\n",
      "epoch:9, nombre batch:16901 loss:14.009 precision=36.442%\n",
      "epoch:9, nombre batch:17001 loss:14.182 precision=35.776%\n",
      "epoch:9, nombre batch:17101 loss:13.965 precision=36.415%\n",
      "epoch:9, nombre batch:17201 loss:13.933 precision=36.014%\n",
      "epoch:9, nombre batch:17301 loss:13.639 precision=37.433%\n",
      "epoch:9, nombre batch:17401 loss:13.735 precision=36.292%\n",
      "epoch:9, nombre batch:17501 loss:14.197 precision=36.310%\n",
      "epoch:9, nombre batch:17601 loss:14.089 precision=35.971%\n",
      "epoch:9, nombre batch:17701 loss:14.163 precision=36.400%\n",
      "epoch:9, nombre batch:17801 loss:14.237 precision=35.589%\n",
      "epoch:9, nombre batch:17901 loss:14.252 precision=35.853%\n",
      "epoch:9, nombre batch:18001 loss:13.702 precision=36.667%\n",
      "epoch:9, nombre batch:18101 loss:14.294 precision=35.509%\n",
      "epoch:9, nombre batch:18201 loss:14.290 precision=35.080%\n",
      "epoch:9, nombre batch:18301 loss:13.819 precision=36.597%\n",
      "epoch:9, nombre batch:18401 loss:13.826 precision=37.580%\n",
      "epoch:9, nombre batch:18501 loss:13.740 precision=36.780%\n",
      "epoch:9, nombre batch:18601 loss:14.030 precision=35.927%\n",
      "epoch:9, nombre batch:18701 loss:14.136 precision=36.039%\n",
      "epoch:9, nombre batch:18801 loss:13.832 precision=37.155%\n",
      "epoch:9, nombre batch:18901 loss:14.058 precision=36.245%\n",
      "epoch:9, nombre batch:19001 loss:14.322 precision=35.483%\n",
      "epoch:9, nombre batch:19101 loss:14.198 precision=35.997%\n",
      "epoch:9, nombre batch:19201 loss:13.946 precision=36.535%\n",
      "epoch:9, nombre batch:19301 loss:13.833 precision=36.642%\n",
      "epoch:9, nombre batch:19401 loss:14.182 precision=35.551%\n",
      "epoch:9, nombre batch:19501 loss:13.667 precision=36.872%\n",
      "epoch:9, nombre batch:19601 loss:13.719 precision=36.828%\n",
      "epoch:9, nombre batch:19701 loss:14.287 precision=35.071%\n",
      "epoch:9, nombre batch:19801 loss:13.490 precision=38.102%\n",
      "epoch:9, nombre batch:19901 loss:13.669 precision=36.360%\n",
      "epoch:9, nombre batch:20001 loss:13.774 precision=37.058%\n",
      "epoch:9, nombre batch:20101 loss:13.744 precision=37.088%\n",
      "epoch:9, nombre batch:20201 loss:13.686 precision=36.946%\n",
      "epoch:9, nombre batch:20301 loss:13.708 precision=37.255%\n",
      "epoch:9, nombre batch:20401 loss:13.718 precision=36.308%\n",
      "epoch:9, nombre batch:20501 loss:13.994 precision=35.543%\n",
      "epoch:9, nombre batch:20601 loss:13.571 precision=38.077%\n",
      "epoch:9, nombre batch:20701 loss:13.627 precision=37.329%\n",
      "epoch:9, nombre batch:20801 loss:13.914 precision=36.608%\n",
      "epoch:9, nombre batch:20901 loss:13.655 precision=37.491%\n",
      "epoch:9, nombre batch:21001 loss:14.128 precision=35.738%\n",
      "epoch:9, nombre batch:21101 loss:14.159 precision=35.787%\n",
      "epoch:9, nombre batch:21201 loss:13.660 precision=37.533%\n",
      "epoch:9, nombre batch:21301 loss:13.329 precision=38.621%\n",
      "epoch:9, nombre batch:21401 loss:14.136 precision=35.661%\n",
      "epoch:9, nombre batch:21501 loss:13.444 precision=38.708%\n",
      "epoch:9, nombre batch:21601 loss:13.428 precision=37.826%\n",
      "epoch:9, nombre batch:21701 loss:14.292 precision=34.874%\n",
      "epoch:9, nombre batch:21801 loss:13.868 precision=37.108%\n",
      "epoch:9, nombre batch:21901 loss:13.719 precision=37.360%\n",
      "epoch:9, nombre batch:22001 loss:14.130 precision=35.883%\n",
      "epoch:9, nombre batch:22101 loss:13.793 precision=36.769%\n",
      "epoch:9, nombre batch:22201 loss:13.384 precision=38.153%\n",
      "epoch:9, nombre batch:22301 loss:13.994 precision=36.695%\n",
      "epoch:9, nombre batch:22401 loss:13.849 precision=36.131%\n",
      "epoch:9, nombre batch:22501 loss:14.511 precision=35.285%\n",
      "epoch:9, nombre batch:22601 loss:13.471 precision=37.928%\n",
      "epoch:9, nombre batch:22701 loss:13.952 precision=37.172%\n",
      "epoch:9, nombre batch:22801 loss:14.041 precision=35.917%\n",
      "epoch:9, nombre batch:22901 loss:13.864 precision=36.758%\n",
      "epoch:9, nombre batch:23001 loss:13.846 precision=36.888%\n",
      "Resultat Epoch 9;  loss:13.890, Accuracy=43.637%,  precision:36.650%, AMS=0.9803\n",
      "epoch:10, nombre batch:1 loss:0.106 precision=42.308%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, nombre batch:101 loss:13.699 precision=37.837%\n",
      "epoch:10, nombre batch:201 loss:13.740 precision=35.778%\n",
      "epoch:10, nombre batch:301 loss:13.846 precision=36.259%\n",
      "epoch:10, nombre batch:401 loss:13.702 precision=37.826%\n",
      "epoch:10, nombre batch:501 loss:13.519 precision=37.895%\n",
      "epoch:10, nombre batch:601 loss:13.931 precision=36.500%\n",
      "epoch:10, nombre batch:701 loss:14.056 precision=35.440%\n",
      "epoch:10, nombre batch:801 loss:13.482 precision=38.616%\n",
      "epoch:10, nombre batch:901 loss:13.903 precision=37.328%\n",
      "epoch:10, nombre batch:1001 loss:13.531 precision=37.732%\n",
      "epoch:10, nombre batch:1101 loss:13.746 precision=37.686%\n",
      "epoch:10, nombre batch:1201 loss:14.314 precision=35.443%\n",
      "epoch:10, nombre batch:1301 loss:13.502 precision=37.403%\n",
      "epoch:10, nombre batch:1401 loss:14.154 precision=35.036%\n",
      "epoch:10, nombre batch:1501 loss:13.847 precision=37.402%\n",
      "epoch:10, nombre batch:1601 loss:13.629 precision=37.148%\n",
      "epoch:10, nombre batch:1701 loss:13.621 precision=37.684%\n",
      "epoch:10, nombre batch:1801 loss:13.618 precision=37.879%\n",
      "epoch:10, nombre batch:1901 loss:13.713 precision=37.158%\n",
      "epoch:10, nombre batch:2001 loss:14.037 precision=36.018%\n",
      "epoch:10, nombre batch:2101 loss:13.764 precision=37.142%\n",
      "epoch:10, nombre batch:2201 loss:14.111 precision=35.930%\n",
      "epoch:10, nombre batch:2301 loss:13.910 precision=36.685%\n",
      "epoch:10, nombre batch:2401 loss:14.091 precision=36.401%\n",
      "epoch:10, nombre batch:2501 loss:13.563 precision=37.386%\n",
      "epoch:10, nombre batch:2601 loss:14.166 precision=35.579%\n",
      "epoch:10, nombre batch:2701 loss:13.710 precision=37.773%\n",
      "epoch:10, nombre batch:2801 loss:14.009 precision=35.939%\n",
      "epoch:10, nombre batch:2901 loss:13.975 precision=36.905%\n",
      "epoch:10, nombre batch:3001 loss:13.660 precision=38.316%\n",
      "epoch:10, nombre batch:3101 loss:13.929 precision=36.852%\n",
      "epoch:10, nombre batch:3201 loss:13.681 precision=37.410%\n",
      "epoch:10, nombre batch:3301 loss:13.976 precision=36.694%\n",
      "epoch:10, nombre batch:3401 loss:14.065 precision=37.014%\n",
      "epoch:10, nombre batch:3501 loss:13.811 precision=36.536%\n",
      "epoch:10, nombre batch:3601 loss:13.734 precision=37.893%\n",
      "epoch:10, nombre batch:3701 loss:13.712 precision=37.682%\n",
      "epoch:10, nombre batch:3801 loss:13.910 precision=36.278%\n",
      "epoch:10, nombre batch:3901 loss:13.860 precision=36.614%\n",
      "epoch:10, nombre batch:4001 loss:13.729 precision=36.621%\n",
      "epoch:10, nombre batch:4101 loss:14.063 precision=36.117%\n",
      "epoch:10, nombre batch:4201 loss:13.791 precision=37.584%\n",
      "epoch:10, nombre batch:4301 loss:13.463 precision=38.502%\n",
      "epoch:10, nombre batch:4401 loss:13.293 precision=38.291%\n",
      "epoch:10, nombre batch:4501 loss:13.651 precision=37.181%\n",
      "epoch:10, nombre batch:4601 loss:14.360 precision=35.136%\n",
      "epoch:10, nombre batch:4701 loss:13.615 precision=38.124%\n",
      "epoch:10, nombre batch:4801 loss:14.338 precision=35.095%\n",
      "epoch:10, nombre batch:4901 loss:13.660 precision=36.230%\n",
      "epoch:10, nombre batch:5001 loss:13.910 precision=36.723%\n",
      "epoch:10, nombre batch:5101 loss:13.710 precision=36.461%\n",
      "epoch:10, nombre batch:5201 loss:14.149 precision=36.326%\n",
      "epoch:10, nombre batch:5301 loss:13.811 precision=37.256%\n",
      "epoch:10, nombre batch:5401 loss:13.843 precision=36.737%\n",
      "epoch:10, nombre batch:5501 loss:13.733 precision=37.631%\n",
      "epoch:10, nombre batch:5601 loss:13.727 precision=38.146%\n",
      "epoch:10, nombre batch:5701 loss:13.980 precision=35.923%\n",
      "epoch:10, nombre batch:5801 loss:13.804 precision=37.123%\n",
      "epoch:10, nombre batch:5901 loss:13.662 precision=37.102%\n",
      "epoch:10, nombre batch:6001 loss:13.575 precision=37.306%\n",
      "epoch:10, nombre batch:6101 loss:13.873 precision=36.905%\n",
      "epoch:10, nombre batch:6201 loss:13.914 precision=36.520%\n",
      "epoch:10, nombre batch:6301 loss:13.830 precision=37.220%\n",
      "epoch:10, nombre batch:6401 loss:13.280 precision=38.532%\n",
      "epoch:10, nombre batch:6501 loss:14.472 precision=34.123%\n",
      "epoch:10, nombre batch:6601 loss:13.881 precision=37.616%\n",
      "epoch:10, nombre batch:6701 loss:13.980 precision=35.565%\n",
      "epoch:10, nombre batch:6801 loss:13.634 precision=37.748%\n",
      "epoch:10, nombre batch:6901 loss:13.859 precision=35.556%\n",
      "epoch:10, nombre batch:7001 loss:13.840 precision=36.964%\n",
      "epoch:10, nombre batch:7101 loss:14.104 precision=35.792%\n",
      "epoch:10, nombre batch:7201 loss:13.790 precision=36.690%\n",
      "epoch:10, nombre batch:7301 loss:14.077 precision=35.192%\n",
      "epoch:10, nombre batch:7401 loss:13.899 precision=36.506%\n",
      "epoch:10, nombre batch:7501 loss:13.809 precision=36.460%\n",
      "epoch:10, nombre batch:7601 loss:13.754 precision=36.561%\n",
      "epoch:10, nombre batch:7701 loss:13.316 precision=38.418%\n",
      "epoch:10, nombre batch:7801 loss:13.894 precision=37.144%\n",
      "epoch:10, nombre batch:7901 loss:14.065 precision=36.035%\n",
      "epoch:10, nombre batch:8001 loss:13.640 precision=39.071%\n",
      "epoch:10, nombre batch:8101 loss:14.244 precision=35.876%\n",
      "epoch:10, nombre batch:8201 loss:13.794 precision=37.243%\n",
      "epoch:10, nombre batch:8301 loss:14.006 precision=36.787%\n",
      "epoch:10, nombre batch:8401 loss:13.688 precision=37.909%\n",
      "epoch:10, nombre batch:8501 loss:13.964 precision=37.212%\n",
      "epoch:10, nombre batch:8601 loss:13.809 precision=36.260%\n",
      "epoch:10, nombre batch:8701 loss:13.684 precision=36.709%\n",
      "epoch:10, nombre batch:8801 loss:13.996 precision=36.219%\n",
      "epoch:10, nombre batch:8901 loss:13.742 precision=35.932%\n",
      "epoch:10, nombre batch:9001 loss:14.164 precision=36.579%\n",
      "epoch:10, nombre batch:9101 loss:14.537 precision=34.439%\n",
      "epoch:10, nombre batch:9201 loss:13.645 precision=37.316%\n",
      "epoch:10, nombre batch:9301 loss:13.506 precision=38.211%\n",
      "epoch:10, nombre batch:9401 loss:13.719 precision=36.780%\n",
      "epoch:10, nombre batch:9501 loss:13.706 precision=37.034%\n",
      "epoch:10, nombre batch:9601 loss:14.275 precision=34.705%\n",
      "epoch:10, nombre batch:9701 loss:13.879 precision=36.866%\n",
      "epoch:10, nombre batch:9801 loss:13.796 precision=37.341%\n",
      "epoch:10, nombre batch:9901 loss:13.798 precision=37.749%\n",
      "epoch:10, nombre batch:10001 loss:13.622 precision=37.798%\n",
      "epoch:10, nombre batch:10101 loss:13.687 precision=37.392%\n",
      "epoch:10, nombre batch:10201 loss:13.979 precision=35.151%\n",
      "epoch:10, nombre batch:10301 loss:14.169 precision=35.929%\n",
      "epoch:10, nombre batch:10401 loss:14.005 precision=36.580%\n",
      "epoch:10, nombre batch:10501 loss:14.358 precision=34.597%\n",
      "epoch:10, nombre batch:10601 loss:14.221 precision=35.002%\n",
      "epoch:10, nombre batch:10701 loss:14.376 precision=34.712%\n",
      "epoch:10, nombre batch:10801 loss:14.000 precision=37.070%\n",
      "epoch:10, nombre batch:10901 loss:13.925 precision=37.000%\n",
      "epoch:10, nombre batch:11001 loss:13.578 precision=38.819%\n",
      "epoch:10, nombre batch:11101 loss:13.525 precision=38.172%\n",
      "epoch:10, nombre batch:11201 loss:13.957 precision=35.964%\n",
      "epoch:10, nombre batch:11301 loss:13.968 precision=36.679%\n",
      "epoch:10, nombre batch:11401 loss:14.085 precision=36.132%\n",
      "epoch:10, nombre batch:11501 loss:13.635 precision=38.274%\n",
      "epoch:10, nombre batch:11601 loss:13.668 precision=37.316%\n",
      "epoch:10, nombre batch:11701 loss:13.976 precision=36.161%\n",
      "epoch:10, nombre batch:11801 loss:13.928 precision=36.256%\n",
      "epoch:10, nombre batch:11901 loss:14.124 precision=36.788%\n",
      "epoch:10, nombre batch:12001 loss:13.883 precision=36.725%\n",
      "epoch:10, nombre batch:12101 loss:13.644 precision=36.553%\n",
      "epoch:10, nombre batch:12201 loss:14.105 precision=35.639%\n",
      "epoch:10, nombre batch:12301 loss:13.759 precision=37.085%\n",
      "epoch:10, nombre batch:12401 loss:13.416 precision=39.318%\n",
      "epoch:10, nombre batch:12501 loss:13.607 precision=37.693%\n",
      "epoch:10, nombre batch:12601 loss:13.872 precision=36.950%\n",
      "epoch:10, nombre batch:12701 loss:14.017 precision=37.412%\n",
      "epoch:10, nombre batch:12801 loss:13.726 precision=37.472%\n",
      "epoch:10, nombre batch:12901 loss:14.271 precision=36.343%\n",
      "epoch:10, nombre batch:13001 loss:13.863 precision=36.161%\n",
      "epoch:10, nombre batch:13101 loss:14.391 precision=35.791%\n",
      "epoch:10, nombre batch:13201 loss:14.017 precision=35.961%\n",
      "epoch:10, nombre batch:13301 loss:13.551 precision=36.526%\n",
      "epoch:10, nombre batch:13401 loss:13.547 precision=37.419%\n",
      "epoch:10, nombre batch:13501 loss:13.952 precision=36.377%\n",
      "epoch:10, nombre batch:13601 loss:14.631 precision=34.627%\n",
      "epoch:10, nombre batch:13701 loss:13.891 precision=35.425%\n",
      "epoch:10, nombre batch:13801 loss:14.078 precision=35.010%\n",
      "epoch:10, nombre batch:13901 loss:14.054 precision=36.323%\n",
      "epoch:10, nombre batch:14001 loss:13.959 precision=35.517%\n",
      "epoch:10, nombre batch:14101 loss:14.018 precision=35.480%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, nombre batch:14201 loss:14.060 precision=36.468%\n",
      "epoch:10, nombre batch:14301 loss:14.211 precision=34.939%\n",
      "epoch:10, nombre batch:14401 loss:14.036 precision=35.274%\n",
      "epoch:10, nombre batch:14501 loss:13.931 precision=37.285%\n",
      "epoch:10, nombre batch:14601 loss:13.653 precision=37.217%\n",
      "epoch:10, nombre batch:14701 loss:13.572 precision=37.861%\n",
      "epoch:10, nombre batch:14801 loss:14.092 precision=36.242%\n",
      "epoch:10, nombre batch:14901 loss:14.356 precision=35.822%\n",
      "epoch:10, nombre batch:15001 loss:14.027 precision=36.513%\n",
      "epoch:10, nombre batch:15101 loss:13.771 precision=36.838%\n",
      "epoch:10, nombre batch:15201 loss:14.316 precision=35.627%\n",
      "epoch:10, nombre batch:15301 loss:14.236 precision=35.104%\n",
      "epoch:10, nombre batch:15401 loss:13.852 precision=37.068%\n",
      "epoch:10, nombre batch:15501 loss:14.234 precision=35.821%\n",
      "epoch:10, nombre batch:15601 loss:14.237 precision=35.610%\n",
      "epoch:10, nombre batch:15701 loss:13.671 precision=36.977%\n",
      "epoch:10, nombre batch:15801 loss:13.884 precision=36.805%\n",
      "epoch:10, nombre batch:15901 loss:13.887 precision=36.885%\n",
      "epoch:10, nombre batch:16001 loss:14.201 precision=35.800%\n",
      "epoch:10, nombre batch:16101 loss:13.977 precision=36.388%\n",
      "epoch:10, nombre batch:16201 loss:14.233 precision=36.194%\n",
      "epoch:10, nombre batch:16301 loss:13.727 precision=37.142%\n",
      "epoch:10, nombre batch:16401 loss:14.094 precision=36.286%\n",
      "epoch:10, nombre batch:16501 loss:13.942 precision=36.381%\n",
      "epoch:10, nombre batch:16601 loss:14.019 precision=36.166%\n",
      "epoch:10, nombre batch:16701 loss:13.826 precision=34.799%\n",
      "epoch:10, nombre batch:16801 loss:14.283 precision=33.706%\n",
      "epoch:10, nombre batch:16901 loss:13.728 precision=37.153%\n",
      "epoch:10, nombre batch:17001 loss:14.257 precision=35.400%\n",
      "epoch:10, nombre batch:17101 loss:13.929 precision=36.559%\n",
      "epoch:10, nombre batch:17201 loss:13.973 precision=36.364%\n",
      "epoch:10, nombre batch:17301 loss:13.813 precision=37.318%\n",
      "epoch:10, nombre batch:17401 loss:13.941 precision=37.942%\n",
      "epoch:10, nombre batch:17501 loss:14.005 precision=36.330%\n",
      "epoch:10, nombre batch:17601 loss:14.021 precision=36.415%\n",
      "epoch:10, nombre batch:17701 loss:13.488 precision=37.661%\n",
      "epoch:10, nombre batch:17801 loss:13.666 precision=37.908%\n",
      "epoch:10, nombre batch:17901 loss:13.752 precision=36.557%\n",
      "epoch:10, nombre batch:18001 loss:14.278 precision=35.448%\n",
      "epoch:10, nombre batch:18101 loss:13.703 precision=37.462%\n",
      "epoch:10, nombre batch:18201 loss:13.803 precision=36.819%\n",
      "epoch:10, nombre batch:18301 loss:13.576 precision=38.456%\n",
      "epoch:10, nombre batch:18401 loss:14.056 precision=36.607%\n",
      "epoch:10, nombre batch:18501 loss:13.959 precision=36.071%\n",
      "epoch:10, nombre batch:18601 loss:13.129 precision=39.295%\n",
      "epoch:10, nombre batch:18701 loss:14.038 precision=36.189%\n",
      "epoch:10, nombre batch:18801 loss:13.664 precision=37.579%\n",
      "epoch:10, nombre batch:18901 loss:14.037 precision=36.062%\n",
      "epoch:10, nombre batch:19001 loss:13.731 precision=37.246%\n",
      "epoch:10, nombre batch:19101 loss:14.062 precision=36.323%\n",
      "epoch:10, nombre batch:19201 loss:14.026 precision=35.882%\n",
      "epoch:10, nombre batch:19301 loss:13.365 precision=37.741%\n",
      "epoch:10, nombre batch:19401 loss:13.716 precision=37.618%\n",
      "epoch:10, nombre batch:19501 loss:14.384 precision=35.049%\n",
      "epoch:10, nombre batch:19601 loss:13.784 precision=37.322%\n",
      "epoch:10, nombre batch:19701 loss:13.769 precision=37.184%\n",
      "epoch:10, nombre batch:19801 loss:14.257 precision=35.611%\n",
      "epoch:10, nombre batch:19901 loss:13.769 precision=36.660%\n",
      "epoch:10, nombre batch:20001 loss:13.848 precision=36.922%\n",
      "epoch:10, nombre batch:20101 loss:14.477 precision=34.383%\n",
      "epoch:10, nombre batch:20201 loss:13.856 precision=37.365%\n",
      "epoch:10, nombre batch:20301 loss:14.068 precision=37.482%\n",
      "epoch:10, nombre batch:20401 loss:14.088 precision=36.442%\n",
      "epoch:10, nombre batch:20501 loss:13.984 precision=36.139%\n",
      "epoch:10, nombre batch:20601 loss:13.833 precision=36.940%\n",
      "epoch:10, nombre batch:20701 loss:13.630 precision=37.396%\n",
      "epoch:10, nombre batch:20801 loss:13.688 precision=37.919%\n",
      "epoch:10, nombre batch:20901 loss:13.741 precision=37.689%\n",
      "epoch:10, nombre batch:21001 loss:14.052 precision=35.423%\n",
      "epoch:10, nombre batch:21101 loss:13.907 precision=36.547%\n",
      "epoch:10, nombre batch:21201 loss:13.501 precision=38.390%\n",
      "epoch:10, nombre batch:21301 loss:13.912 precision=37.208%\n",
      "epoch:10, nombre batch:21401 loss:13.707 precision=36.970%\n",
      "epoch:10, nombre batch:21501 loss:13.849 precision=37.467%\n",
      "epoch:10, nombre batch:21601 loss:14.018 precision=35.947%\n",
      "epoch:10, nombre batch:21701 loss:13.990 precision=35.747%\n",
      "epoch:10, nombre batch:21801 loss:13.961 precision=36.210%\n",
      "epoch:10, nombre batch:21901 loss:14.156 precision=35.857%\n",
      "epoch:10, nombre batch:22001 loss:14.261 precision=35.860%\n",
      "epoch:10, nombre batch:22101 loss:13.578 precision=37.543%\n",
      "epoch:10, nombre batch:22201 loss:13.356 precision=39.155%\n",
      "epoch:10, nombre batch:22301 loss:14.001 precision=35.400%\n",
      "epoch:10, nombre batch:22401 loss:14.317 precision=34.552%\n",
      "epoch:10, nombre batch:22501 loss:14.079 precision=36.762%\n",
      "epoch:10, nombre batch:22601 loss:14.560 precision=34.284%\n",
      "epoch:10, nombre batch:22701 loss:13.974 precision=36.198%\n",
      "epoch:10, nombre batch:22801 loss:14.171 precision=36.595%\n",
      "epoch:10, nombre batch:22901 loss:13.649 precision=37.707%\n",
      "epoch:10, nombre batch:23001 loss:13.528 precision=37.958%\n",
      "Resultat Epoch 10;  loss:13.889, Accuracy=43.699%,  precision:36.691%, AMS=0.9709\n",
      "Résultat Entrainement: 321809 bonnes classification sur 736414 données :loss moyen/batch=13.8894, Accuracy=43.699%,  precision=36.691%, AMS=0.9709\n"
     ]
    }
   ],
   "source": [
    "#TRAIN MLP 2 couche\n",
    "\n",
    "Epoch=10\n",
    "\n",
    "log_interval=100\n",
    "predicted=np.empty(batch_size)\n",
    "#print(type(Percep.fc1.weight[0][0].item()))\n",
    "#print(type(Percep.fc1.bias[0].item()))\n",
    "for epoch in range(Epoch):\n",
    "    ams=0\n",
    "    TP=0 #True Positive\n",
    "    FP=0 # False Positive\n",
    "    S=0 #True Positive pondéré\n",
    "    B=0 # False Positive pondéré\n",
    "    Loss=0\n",
    "    correct=0\n",
    "    \n",
    "    fp=0\n",
    "    tp=0\n",
    "    total=0\n",
    "    running_loss=0\n",
    "    for batch_id, sample in enumerate(Data_Loader,0):\n",
    "            inputs, labels= sample\n",
    "            optimizeMlp2.zero_grad()\n",
    "            \n",
    "            #print(inputs)\n",
    "            #print(labels.size(0))\n",
    "            \n",
    "            \n",
    "            \n",
    "            outputs= Mlp2(inputs[:,1:31])\n",
    "            \n",
    "            #print(outputs[0]+outputs[1])\n",
    "           # print(outputs.view(-1,32))\n",
    "        \n",
    "            #predicted= torch.tensor(1 if outputs.data[j] > 0.5 else 0 for j in range(len(outputs.data)))\n",
    "            total+= labels.size(0)\n",
    "            #correct+= (predicted== labels).sum().item()\n",
    "            for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "            \n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "            \n",
    "        \n",
    "            #print(outputs)\n",
    "            #outputs=outputs.view(-1,10)\n",
    "            loss= criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizeMlp2.step()\n",
    "        \n",
    "            running_loss+= loss.item()\n",
    "            Loss+=loss.item()\n",
    "            if batch_id%log_interval==0:\n",
    "                print(\"epoch:{}, nombre batch:{} loss:{:.3f} precision={:.3f}%\".format(epoch+1, batch_id+1, running_loss/log_interval, 100*tp/(tp+fp))) \n",
    "                total=0\n",
    "                tp=0\n",
    "                fp=0\n",
    "                running_loss=0\n",
    "    print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}%,  precision:{:.3f}%, AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ))\n",
    "   \n",
    "        \n",
    "print(\"Résultat Entrainement: {} bonnes classification sur {} données :loss moyen/batch={:.4f}, Accuracy={:.3f}%,  precision={:.3f}%, AMS={:.4f}\".format(correct,len(Data_Loader.dataset), Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset) ,100*TP/(TP+FP), AMS(S,B)))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN CNN\n",
    "\n",
    "Epoch=10\n",
    "\n",
    "log_interval=100\n",
    "predicted=np.empty(batch_size)\n",
    "#print(type(Percep.fc1.weight[0][0].item()))\n",
    "#print(type(Percep.fc1.bias[0].item()))\n",
    "for epoch in range(Epoch):\n",
    "    ams=0\n",
    "    TP=0 #True Positive\n",
    "    FP=0 # False Positive\n",
    "    S=0 #True Positive pondéré\n",
    "    B=0 # False Positive pondéré\n",
    "    Loss=0\n",
    "    correct=0\n",
    "    \n",
    "    fp=0\n",
    "    tp=0\n",
    "    total=0\n",
    "    running_loss=0\n",
    "    for batch_id, sample in enumerate(Data_Loader,0):\n",
    "            inputs, labels= sample\n",
    "            optimizeCnn.zero_grad()\n",
    "            \n",
    "            #print(inputs)\n",
    "            #print(labels.size(0))\n",
    "            \n",
    "            \n",
    "            \n",
    "            outputs= Cnn(inputs[:,1:31])\n",
    "            \n",
    "            #print(outputs[0]+outputs[1])\n",
    "           # print(outputs.view(-1,32))\n",
    "        \n",
    "            #predicted= torch.tensor(1 if outputs.data[j] > 0.5 else 0 for j in range(len(outputs.data)))\n",
    "            total+= labels.size(0)\n",
    "            #correct+= (predicted== labels).sum().item()\n",
    "            for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "            \n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "            \n",
    "        \n",
    "            #print(outputs)\n",
    "            #outputs=outputs.view(-1,10)\n",
    "            loss= criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizeCnn.step()\n",
    "        \n",
    "            running_loss+= loss.item()\n",
    "            Loss+=loss.item()\n",
    "            if batch_id%log_interval==0:\n",
    "                print(\"epoch:{}, nombre batch:{} loss:{:.3f} precision={:.3f}%\".format(epoch+1, batch_id+1, running_loss/log_interval, 100*tp/(tp+fp))) \n",
    "                total=0\n",
    "                tp=0\n",
    "                fp=0\n",
    "                running_loss=0\n",
    "    print(\"Resultat Epoch {};  loss:{:.3f}, Accuracy={:.3f}%,  precision:{:.3f}%, AMS={:.4f}\".format(epoch+1, Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset), 100*TP/(TP+FP), AMS(S,B) ))\n",
    "   \n",
    "        \n",
    "print(\"Résultat Entrainement: {} bonnes classification sur {} données :loss moyen/batch={:.4f}, Accuracy={:.3f}%,  precision={:.3f}%, AMS={:.4f}\".format(correct,len(Data_Loader.dataset), Loss/(batch_id+1), 100*correct/len(Data_Loader.dataset) ,100*TP/(TP+FP), AMS(S,B)))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat test: 115013 bonnes classification sur 163650 données de test:loss moyen=0.00000158 Accuracy=70.280%, Precision=66.881%, AMS=0.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "#TEST Perceptron\n",
    "\n",
    "correct= 0\n",
    "total= 0\n",
    "test_loss = 0\n",
    "predicted=np.empty(batch_size)\n",
    "TP=0\n",
    "FP=0\n",
    "S=0\n",
    "B=0\n",
    "\n",
    "Data_Loader.dataset.train=False\n",
    "with torch.no_grad():\n",
    "    for sample in Data_Loader:\n",
    "        inputs, labels= sample\n",
    "        #images=images.view(-1,784)\n",
    "        outputs= Percep(inputs[:,1:31])\n",
    "        \n",
    "        total+= labels.size(0)\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "                if outputs.data[i]>0.5:\n",
    "                    predicted[i]=1\n",
    "            \n",
    "                    if labels[i]==1:\n",
    "                        TP+=1\n",
    "                        #tp+=1\n",
    "                        S+= inputs[i][31]\n",
    "                        correct+=1\n",
    "                    else:\n",
    "                        FP+=1\n",
    "                        #fp+=1\n",
    "                        B+=inputs[i][31]\n",
    "                else:\n",
    "                    predicted[i]=0\n",
    "                    if labels[i]==0:\n",
    "                        correct+=1\n",
    "        \n",
    "        test_loss+=criterion(outputs,labels.float())\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "test_loss/= len(Data_Loader.dataset)\n",
    "\n",
    "print(\"Résultat test: {} bonnes classification sur {} données de test:loss moyen={:.8f} Accuracy={:.3f}%, Precision={:.3f}%, AMS={:.3f}\".format(correct,total, test_loss/total,100*correct/total, 100*TP/(TP+FP), AMS(S,B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
